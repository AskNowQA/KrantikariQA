{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook would be used to run the code on reduced dataset which can then offer finer controls over the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/priyansh/new_kranti/merity\n"
     ]
    }
   ],
   "source": [
    "cd merity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model as merity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/priyansh/new_kranti\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ULMFIT vocab and vectors from disk. Sit Tight.\n",
      "Label Cache not found. Creating a new one\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/data/priyansh/new_kranti/utils/dbpedia_interface.py\", line 133, in __init__\n",
      "    self.labels = pickle.load(open('resources/labels.pickle'))\n",
      "TypeError: a bytes-like object is required, not 'str'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT IMPLEMENTED FUNCTIONALITY NOT IMPLEMENTED FUNCTIONALITY !!!\n"
     ]
    }
   ],
   "source": [
    "# Torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import  DataLoader\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_sequence \n",
    "\n",
    "# Local imports\n",
    "import data_loader as dl\n",
    "import auxiliary as aux\n",
    "import network as net\n",
    "\n",
    "# Other libs\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from configs import config_loader as cl\n",
    "import corechain as cc\n",
    "import components as com\n",
    "import utils.tensor_utils as tu\n",
    "\n",
    "sys.path.append('/data/priyansh/conda/fastai')\n",
    "from fastai.text import *\n",
    "import fastai\n",
    "from fastai import text, core, lm_rnn\n",
    "from typing import Any, AnyStr, Callable, Collection, Dict, Hashable, Iterator, List, Mapping, NewType, Optional\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT IMPLEMENTED FUNCTIONALITY NOT IMPLEMENTED FUNCTIONALITY !!!\n"
     ]
    }
   ],
   "source": [
    "#setting up device,model name and loss types.\n",
    "device = torch.device(\"cuda\")\n",
    "training_model = 'bilstm_dot_ulmfit'\n",
    "_dataset = 'lcquad'\n",
    "_train_over_validation = False\n",
    "pointwise = False\n",
    "_train_over_validation = False\n",
    "finetune = False\n",
    "bidirectional = True\n",
    "\n",
    "#Loading relations file.\n",
    "COMMON_DATA_DIR = 'data/data/common'\n",
    "_dataset_specific_data_dir = 'data/data/%(dataset)s/' % {'dataset': _dataset}\n",
    "_inv_relations = aux.load_inverse_relation(COMMON_DATA_DIR)\n",
    "_word_to_id = aux.load_word_list(COMMON_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pointwise:\n",
    "    training_config = 'pointwise'\n",
    "else:\n",
    "    training_config = 'pairwise'\n",
    "\n",
    "if training_model == 'reldet':\n",
    "    schema = 'reldet'\n",
    "elif training_model == 'slotptr' or training_model == 'slotptr_common_encoder' or training_model == 'slotptrortho':\n",
    "    schema = 'slotptr'\n",
    "elif training_model == 'bilstm_dot_multiencoder':\n",
    "    schema = 'default'\n",
    "else:\n",
    "    schema = 'default'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 25)\n",
      "(700, 25)\n",
      "(700, 1000, 25)\n",
      "(100, 25)\n",
      "(100, 25)\n",
      "(100, 1000, 25)\n",
      "(200, 25)\n",
      "(200, 25)\n",
      "(200, 1000, 25)\n",
      "(700, 25)\n"
     ]
    }
   ],
   "source": [
    "parameter_dict = cl.corechain_parameters(dataset=_dataset,training_model=training_model,\n",
    "                                             training_config=training_config,config_file='configs/macros.cfg')\n",
    "\n",
    "parameter_dict['batch_size'] = 4000\n",
    "if _dataset == 'lcquad':\n",
    "    test_every = parameter_dict['test_every']\n",
    "else:\n",
    "    test_every = False\n",
    "validate_every = parameter_dict['validate_every']\n",
    "\n",
    "\n",
    "data = aux.load_data(_dataset=_dataset, _train_over_validation = _train_over_validation,\n",
    "                     _parameter_dict=parameter_dict, _relations =  _inv_relations, _pointwise=pointwise, _device=device,k=1000)\n",
    "\n",
    "train_loader = cc.load_data(data, parameter_dict, pointwise, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy pasting the RNN core module from the fastai"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class CustomEncoder(lm_rnn.MultiBatchRNN):\n",
    "    @property\n",
    "    def layers(self):\n",
    "        return torch.nn.ModuleList([torch.nn.ModuleList([self.rnns[0], self.dropouths[0]]),\n",
    "                                    torch.nn.ModuleList([self.rnns[1], self.dropouths[1]]),\n",
    "                                    torch.nn.ModuleList([self.rnns[2], self.dropouths[2]])])\n",
    "\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "\n",
    "    # @TODO: inject comments.\n",
    "    def __init__(self, _device: torch.device, ntoken: int, dps: list, enc_wgts, _debug=False):\n",
    "        super(TextClassifier, self).__init__()\n",
    "\n",
    "        self.device = _device\n",
    "\n",
    "        # Load the pre-trained model\n",
    "        args = {'ntoken': ntoken, 'emb_sz': 400, 'n_hid': 1150,\n",
    "                'n_layers': 3, 'pad_token': 0, 'qrnn': False, 'bptt': 70, 'max_seq': 1400,\n",
    "                'dropouti': dps[0], 'wdrop': dps[1], 'dropoute': dps[2], 'dropouth': dps[3]}\n",
    "        self.encoder = CustomEncoder(**args).to(self.device)\n",
    "        self.encoder.load_state_dict(enc_wgts)\n",
    "        '''\n",
    "            Make new classifier.\n",
    "            \n",
    "            Explanation:\n",
    "                400*3 because input is [ h_T, maxpool, meanpool ]\n",
    "                50 is hidden layer dim\n",
    "                2 is n_classes\n",
    "\n",
    "                0.4, 0.1 are drops at various layers\n",
    "        '''\n",
    "#         self.linear = text.PoolingLinearClassifier(layers=[400 * 3, 50, 2], drops=[dps[4], 0.1]).to(self.device)\n",
    "        self.encoder.reset()\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        layers = [x for x in self.encoder.layers]\n",
    "#         layers += [x for x in self.linear.layers]\n",
    "        return torch.nn.ModuleList(layers)\n",
    "\n",
    "    @property\n",
    "    def layers_rev(self):\n",
    "        layers = [x for x in self.encoder.layers]\n",
    "#         layers += [x for x in self.linear.layers]\n",
    "        layers.reverse()\n",
    "        return torch.nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # inputs are S*B\n",
    "\n",
    "        # Encoding all the data\n",
    "#         op_p = self.encoder(x.transpose(1, 0))\n",
    "        # pos_batch = op_p[1][-1][-1]\n",
    "#         score = self.linear(op_p)[0]        \n",
    "#         return score\n",
    "\n",
    "        ques_batch, pos_batch, neg_batch, y_label = data['ques_batch'], data['pos_batch'], data['neg_batch'], data['y_label']\n",
    "                \n",
    "        op_q = self.encoder(ques_batch.transpose(1,0))\n",
    "        op_p = self.encoder(pos_batch.transpose(1,0))\n",
    "        op_n = self.encoder(neg_batch.transpose(1,0))\n",
    "        \n",
    "        \n",
    "            \n",
    "        ques_batch_encoded =  op_q[1][-1]\n",
    "        pos_batch_encoded =  op_p[1][-1]\n",
    "        neg_batch_encoded =  op_n[1][-1]\n",
    "        \n",
    "#         ques_batch_encoded =  op_q[-1][0][1].squeeze()\n",
    "#         pos_batch_encoded =  op_p[-1][0][1].squeeze()\n",
    "#         neg_batch_encoded =  op_n[-1][0][1].squeeze()\n",
    "        \n",
    "#         print(ques_batch_encoded.shape, pos_batch_encoded.shape, neg_batch_encoded.shape)\n",
    "#         raise IOError\n",
    "        \n",
    "        #Calculating dot score\n",
    "        pos_scores = torch.sum(ques_batch_encoded * pos_batch_encoded, -1)\n",
    "        neg_scores = torch.sum(ques_batch_encoded * neg_batch_encoded, -1)\n",
    "        \n",
    "        \n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            predicted = self.forward(x)\n",
    "            self.train()\n",
    "            return predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [
     15,
     220,
     241,
     316
    ]
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from fastai.imports import *\n",
    "from fastai.torch_imports import *\n",
    "from fastai.rnn_reg import LockedDropout,WeightDrop,EmbeddingDropout\n",
    "from fastai.model import Stepper\n",
    "from fastai.core import set_grad_enabled\n",
    "import collections\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pack_sequence\n",
    "from typing import Sequence, Tuple, TypeVar, Union\n",
    "# from .imports.core import *\n",
    "# from fastai.imports.core import *\n",
    "# IS_TORCH_04 = LooseVersion(torch.__version__) >= LooseVersion('0.4')\n",
    "\n",
    "def seq2seq_reg(output, xtra, loss, alpha=0, beta=0):\n",
    "    hs,dropped_hs = xtra\n",
    "    if alpha:  # Activation Regularization\n",
    "        loss = loss + (alpha * dropped_hs[-1].pow(2).mean()).sum()\n",
    "    if beta:   # Temporal Activation Regularization (slowness)\n",
    "        h = hs[-1]\n",
    "        if len(h)>1: loss = loss + (beta * (h[1:] - h[:-1]).pow(2).mean()).sum()\n",
    "    return loss\n",
    "\n",
    "def repackage_var(h):\n",
    "    \"\"\"Wraps h in new Variables, to detach them from their history.\"\"\"\n",
    "    if IS_TORCH_04: return h.detach() if type(h) == torch.Tensor else tuple(repackage_var(v) for v in h)\n",
    "    else: return Variable(h.data) if type(h) == Variable else tuple(repackage_var(v) for v in h)\n",
    "    \n",
    "# def dropout_mask(x,sz,p):\n",
    "#     \"Return a dropout mask of the same type as x, size sz, with probability p to cancel an element.\"\n",
    "#     return x.new(*sz).bernoulli_(1-p).div_(1-p)\n",
    "\n",
    "# class RNNDropout(nn.Module):\n",
    "#     \"Dropout that is consistent on the seq_len dimension.\"\n",
    "#     def __init__(self, p:float=0.5):\n",
    "#         super().__init__()\n",
    "#         self.p=p\n",
    "\n",
    "#     def forward(self, x:Tensor) -> Tensor:\n",
    "#         if not self.training or self.p == 0.: return x\n",
    "#         m = dropout_mask(x.data, (1, x.size(1), x.size(2)), self.p)\n",
    "#         return x * m\n",
    "\n",
    "# class WeightDropout(nn.Module):\n",
    "#     \"A module that warps another layer in which some weights will be replaced by 0 during training.\"\n",
    "\n",
    "#     def __init__(self, module, weight_p, layer_names=['weight_hh_l0']):\n",
    "#         super().__init__()\n",
    "#         self.collection = {}\n",
    "#         self.module,self.weight_p,self.layer_names = module,weight_p,layer_names\n",
    "#         for layer in self.layer_names:\n",
    "#             #Makes a copy of the weights of the selected layers.\n",
    "#             w = getattr(self.module, layer)\n",
    "#             self.register_parameter(f'{layer}_raw', nn.Parameter(w.data))\n",
    "\n",
    "#     def _setweights(self):\n",
    "#         \"Apply dropout to the raw weights.\"\n",
    "#         for layer in self.layer_names:\n",
    "#             raw_w = getattr(self, f'{layer}_raw')\n",
    "#             self.module._parameters[layer] = F.dropout(raw_w, p=self.weight_p, training=self.training)\n",
    "\n",
    "#     def forward(self, *args):\n",
    "#         self._setweights()\n",
    "#         with warnings.catch_warnings():\n",
    "#             #To avoid the warning that comes because the weights aren't flattened.\n",
    "#             warnings.simplefilter(\"ignore\")\n",
    "#             return self.module.forward(*args)\n",
    "\n",
    "#     def reset(self):\n",
    "#         for layer in self.layer_names:\n",
    "#             raw_w = getattr(self, f'{layer}_raw')\n",
    "#             self.module._parameters[layer] = F.dropout(raw_w, p=self.weight_p, training=False)\n",
    "#         if hasattr(self.module, 'reset'): self.module.reset()\n",
    "\n",
    "# class EmbeddingDropout(nn.Module):\n",
    "#     \"Apply dropout in the embedding layer by zeroing out some elements of the embedding vector.\"\n",
    "\n",
    "#     def __init__(self, emb, embed_p:float):\n",
    "#         super().__init__()\n",
    "#         self.collection = {}\n",
    "#         self.emb,self.embed_p = emb,embed_p\n",
    "#         self.pad_idx = self.emb.padding_idx\n",
    "#         if self.pad_idx is None: self.pad_idx = -1\n",
    "\n",
    "#     def forward(self, words:LongTensor, scale:Optional[float]=None) -> Tensor:\n",
    "#         if self.training and self.embed_p != 0:\n",
    "#             size = (self.emb.weight.size(0),1)\n",
    "#             mask = dropout_mask(self.emb.weight.data, size, self.embed_p)\n",
    "#             masked_embed = self.emb.weight * mask\n",
    "#         else: masked_embed = self.emb.weight\n",
    "#         if scale: masked_embed.mul_(scale)\n",
    "#         return F.embedding(words, masked_embed, self.pad_idx, self.emb.max_norm,\n",
    "#         self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)\n",
    "\n",
    "class RNN_Encoder(nn.Module):\n",
    "\n",
    "    \"\"\"A custom RNN encoder network that uses\n",
    "        - an embedding matrix to encode input,\n",
    "        - a stack of LSTM or QRNN layers to drive the network, and\n",
    "        - variational dropouts in the embedding and LSTM/QRNN layers\n",
    "        The architecture for this network was inspired by the work done in\n",
    "        \"Regularizing and Optimizing LSTM Language Models\".\n",
    "        (https://arxiv.org/pdf/1708.02182.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, ntoken, emb_sz, n_hid, n_layers, pad_token, bidir=False,\n",
    "                 dropouth=0.3, dropouti=0.65, dropoute=0.1, wdrop=0.5, qrnn=False):\n",
    "        \"\"\" Default constructor for the RNN_Encoder class\n",
    "            Args:\n",
    "                bs (int): batch size of input data\n",
    "                ntoken (int): number of vocabulary (or tokens) in the source dataset\n",
    "                emb_sz (int): the embedding size to use to encode each token\n",
    "                n_hid (int): number of hidden activation per LSTM layer\n",
    "                n_layers (int): number of LSTM layers to use in the architecture\n",
    "                pad_token (int): the int value used for padding text.\n",
    "                dropouth (float): dropout to apply to the activations going from one LSTM layer to another\n",
    "                dropouti (float): dropout to apply to the input layer.\n",
    "                dropoute (float): dropout to apply to the embedding layer.\n",
    "                wdrop (float): dropout used for a LSTM's internal (or hidden) recurrent weights.\n",
    "            Returns:\n",
    "                None\n",
    "          \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.ndir = 2 if bidir else 1\n",
    "        self.bs, self.qrnn = 1, qrnn\n",
    "        self.pad_token =pad_token\n",
    "        self.encoder = nn.Embedding(ntoken, emb_sz, padding_idx=pad_token)\n",
    "        self.encoder_with_dropout = EmbeddingDropout(self.encoder)\n",
    "        if self.qrnn:\n",
    "            #Using QRNN requires cupy: https://github.com/cupy/cupy\n",
    "            from .torchqrnn.qrnn import QRNNLayer\n",
    "            self.rnns = [QRNNLayer(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.ndir,\n",
    "                save_prev_x=True, zoneout=0, window=2 if l == 0 else 1, output_gate=True) for l in range(n_layers)]\n",
    "            if wdrop:\n",
    "                for rnn in self.rnns:\n",
    "                    rnn.linear = WeightDrop(rnn.linear, wdrop, weights=['weight'])\n",
    "        else:\n",
    "            self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.ndir,\n",
    "                1, bidirectional=bidir) for l in range(n_layers)]\n",
    "            if wdrop: self.rnns = [WeightDrop(rnn, wdrop) for rnn in self.rnns]\n",
    "        self.rnns = torch.nn.ModuleList(self.rnns)\n",
    "        self.encoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "\n",
    "        self.emb_sz,self.n_hid,self.n_layers,self.dropoute = emb_sz,n_hid,n_layers,dropoute\n",
    "        self.dropouti = LockedDropout(dropouti)\n",
    "        self.dropouths = nn.ModuleList([LockedDropout(dropouth) for l in range(n_layers)])\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\" Invoked during the forward propagation of the RNN_Encoder module.\n",
    "        Args:\n",
    "            input (Tensor): input of shape (sentence length x batch_size)\n",
    "        Returns:\n",
    "            raw_outputs (tuple(list (Tensor), list(Tensor)): list of tensors evaluated from each RNN layer without using\n",
    "            dropouth, list of tensors evaluated from each RNN layer using dropouth,\n",
    "        \"\"\"\n",
    "        sl,bs = input.size()\n",
    "        if bs!=self.bs:\n",
    "            self.bs=bs\n",
    "            self.reset()\n",
    "        with set_grad_enabled(self.training):\n",
    "            mask = tu.compute_mask(input.transpose(1, 0))\n",
    "            \n",
    "            emb = self.encoder_with_dropout(input, dropout=self.dropoute if self.training else 0)\n",
    "#             emb = self.encoder_with_dropout(input)\n",
    "            emb = self.dropouti(emb)\n",
    "#             print(\"emb shape is \", emb.shape)\n",
    "\n",
    "                    \n",
    "            lengths = mask.eq(1).long().sum(1)                                    # bs\n",
    "            lengths_sort, idx_sort = torch.sort(lengths, dim=0, descending=True)  # bs\n",
    "            _, idx_unsort = torch.sort(idx_sort, dim=0)                           # bs\n",
    "\n",
    "            emb_sort = emb.index_select(1, idx_sort)    # sl * bs * ninp\n",
    "            hid_sort = [(h[0].index_select(1, idx_sort), h[1].index_select(1, idx_sort)) for h in self.hidden]\n",
    "            emb_sort = torch.nn.utils.rnn.pack_padded_sequence(emb_sort, lengths_sort)\n",
    "\n",
    "            \n",
    "#             raw_output = emb\n",
    "            new_hidden,raw_outputs,outputs,raw_outputs_sorted = [],[],[],[]\n",
    "    \n",
    "            for l, (rnn,drop) in enumerate(zip(self.rnns, self.dropouths)):\n",
    "                current_input = emb_sort\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "#                     print(\"shape of raw output is\", raw_output.shape)\n",
    "                    raw_output, new_h = rnn(emb_sort, hid_sort[l])\n",
    "\n",
    "                emb_sort, _ = torch.nn.utils.rnn.pad_packed_sequence(emb_sort)\n",
    "                new_hidden.append(new_h)\n",
    "                raw_outputs.append(emb_sort)\n",
    "                if l != self.n_layers - 1: emb_sort = drop(emb_sort)\n",
    "                outputs.append(emb_sort)\n",
    "\n",
    "#             self.hidden = repackage_var(new_hidden)\n",
    "        raw_outputs = [raw_output.index_select(1, idx_unsort) for raw_output in raw_outputs]\n",
    "        outputs = [output.index_select(1, idx_unsort) for output in outputs]\n",
    "#         new_hidden = [(h_sort[0].index_select(1, idx_unsort), h_sort[1].index_select(1, idx_unsort)) for h_sort in new_hidden]\n",
    "#         output = self.lockdrop(emb_sort, self.dropout)\n",
    "#         outputs.append(output)\n",
    "        \n",
    "        return raw_outputs, outputs\n",
    "\n",
    "    def one_hidden(self, l):\n",
    "#         print(self.bs)\n",
    "        nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz)//self.ndir\n",
    "        if IS_TORCH_04: return Variable(self.weights.new(self.ndir, self.bs, nh).zero_())\n",
    "        else: return Variable(self.weights.new(self.ndir, self.bs, nh).zero_(), volatile=not self.training)\n",
    "\n",
    "    def reset(self):\n",
    "        if self.qrnn: [r.reset() for r in self.rnns]\n",
    "        self.weights = next(self.parameters()).data\n",
    "        if self.qrnn: self.hidden = [self.one_hidden(l) for l in range(self.n_layers)]\n",
    "        else: self.hidden = [(self.one_hidden(l), self.one_hidden(l)) for l in range(self.n_layers)]\n",
    "    \n",
    "    def reset_hidden(self):\n",
    "        self.hidden = [(self.one_hidden(l), self.one_hidden(l)) for l in range(self.n_layers)]\n",
    "\n",
    "class PoolingLinearClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, layers, drops):\n",
    "        super().__init__()\n",
    "        print(\"done\")\n",
    "#         self.layers = nn.ModuleList([\n",
    "#             LinearBlock(layers[i], layers[i + 1], drops[i]) for i in range(len(layers) - 1)])\n",
    "\n",
    "    def pool(self, x, bs, is_max):\n",
    "        f = F.adaptive_max_pool1d if is_max else F.adaptive_avg_pool1d\n",
    "        return f(x.permute(1,2,0), (1,)).view(bs,-1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs = input\n",
    "        output = outputs[-1]\n",
    "        sl,bs,_ = output.size()\n",
    "        avgpool = self.pool(output, bs, False)\n",
    "        mxpool = self.pool(output, bs, True)\n",
    "        x = torch.cat([output[-1], mxpool, avgpool], 1)\n",
    "        return [output[-1]]\n",
    "    \n",
    "class RNN_Encoder_Masks(nn.Module):\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, vocab_sz:int, emb_sz:int, n_hid:int, n_layers:int, pad_token:int, bidir:bool=False,\n",
    "                 hidden_p:float=0.2, input_p:float=0.6, embed_p:float=0.1, weight_p:float=0.5, qrnn:bool=False):\n",
    "\n",
    "        super().__init__()\n",
    "        self.collection = {}\n",
    "        self.bs,self.qrnn,self.ndir = 1, qrnn,(2 if bidir else 1)\n",
    "        self.emb_sz,self.n_hid,self.n_layers,self.pad_token = emb_sz,n_hid,n_layers,pad_token\n",
    "        self.encoder = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n",
    "        self.encoder_dp = EmbeddingDropout(self.encoder, embed_p)\n",
    "        if self.qrnn:\n",
    "            #Using QRNN requires cupy: https://github.com/cupy/cupy\n",
    "            from .qrnn.qrnn import QRNNLayer\n",
    "            self.rnns = [QRNNLayer(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.ndir,\n",
    "                                   save_prev_x=True, zoneout=0, window=2 if l == 0 else 1, output_gate=True,\n",
    "                                   use_cuda=torch.cuda.is_available()) for l in range(n_layers)]\n",
    "            if weight_p != 0.:\n",
    "                for rnn in self.rnns:\n",
    "                    rnn.linear = WeightDropout(rnn.linear, weight_p, layer_names=['weight'])\n",
    "        else:\n",
    "            self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.ndir,\n",
    "                1, bidirectional=bidir) for l in range(n_layers)]\n",
    "            if weight_p != 0.: self.rnns = [WeightDropout(rnn, weight_p) for rnn in self.rnns]\n",
    "        self.rnns = torch.nn.ModuleList(self.rnns)\n",
    "        self.encoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.input_dp = RNNDropout(input_p)\n",
    "        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n",
    "\n",
    "    def forward(self, input:LongTensor):\n",
    "        if is_listy(input):\n",
    "            input, lengths = input\n",
    "        else:\n",
    "            input, lengths = pad_packed_sequence(pack_sequence(input))\n",
    "\n",
    "\n",
    "        # pack_padded can not handle zero lengths texts\n",
    "        empty_inputs = np.argwhere(lengths <= 0)\n",
    "        lengths[empty_inputs] = 1\n",
    "\n",
    "        sl, bs = input.size()\n",
    "        if bs != self.bs:\n",
    "            self.bs = bs\n",
    "            self.reset()\n",
    "\n",
    "        raw_output = self.input_dp(self.encoder_dp(input))\n",
    "        new_hidden, raw_outputs, outputs = [], [], []\n",
    "        for l, (rnn, hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
    "\n",
    "            packed_rnn_inp = pack_padded_sequence(raw_output, lengths)\n",
    "            rnn_output, new_h = rnn(packed_rnn_inp, self.hidden[l])\n",
    "            raw_output, _ = pad_packed_sequence(rnn_output)\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)\n",
    "            outputs.append(raw_output)\n",
    "#         self.hidden = _repackage_var(new_hidden)\n",
    "        return raw_outputs, outputs, lengths\n",
    "\n",
    "    def _one_hidden(self, l:int) -> Tensor:\n",
    "        \"Return one hidden state.\"\n",
    "        nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz)//self.ndir\n",
    "        return self.weights.new(self.ndir, self.bs, nh).zero_()\n",
    "\n",
    "    def reset(self):\n",
    "        \"Reset the hidden states.\"\n",
    "        [r.reset() for r in self.rnns if hasattr(r, 'reset')]\n",
    "        self.weights = next(self.parameters()).data\n",
    "        if self.qrnn: self.hidden = [self._one_hidden(l) for l in range(self.n_layers)]\n",
    "        else: self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]\n",
    "    \n",
    "    def reset_hidden(self):\n",
    "        self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]\n",
    "            \n",
    "class PoolingLinearClassifierMask(nn.Module):\n",
    "    \"Create a linear classifier with pooling.\"\n",
    "\n",
    "    def __init__(self, layers:Collection[int], drops:Collection[float]):\n",
    "        super().__init__()\n",
    "        mod_layers = []\n",
    "#         activs = [nn.ReLU(inplace=True)] * (len(layers) - 2) + [None]\n",
    "#         for n_in,n_out,p,actn in zip(layers[:-1],layers[1:], drops, activs):\n",
    "#             mod_layers += bn_drop_lin(n_in, n_out, p=p, actn=actn)\n",
    "#         self.layers = nn.Sequential(*mod_layers)\n",
    "\n",
    "    def avg_pool(self, x:Tensor, lengths:LongTensor):\n",
    "        diffs_from_max = lengths[0] - lengths\n",
    "        avg_lengths = np.minimum(x.shape[0] - diffs_from_max, lengths)\n",
    "        return torch.div(torch.sum(x, dim=0).permute(1, 0), avg_lengths.float()).permute(1, 0)\n",
    "\n",
    "    def max_pool(self, x:Tensor, bs:int):\n",
    "        \"Pool the tensor along the seq_len dimension.\"\n",
    "        return F.adaptive_max_pool1d(x.permute(1,2,0), (1,)).view(bs,-1)\n",
    "\n",
    "    def last_output(self, outputs:Tensor, lengths:Collection[float]):\n",
    "        diffs_from_max = lengths[0] - lengths\n",
    "        batch_lengths = outputs.shape[0] - diffs_from_max\n",
    "        return outputs[batch_lengths - 1, np.arange(outputs.shape[1]), :]\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor, LongTensor]) -> Tuple[Tensor,Tensor,Tensor]:\n",
    "        raw_outputs, outputs, lengths = input\n",
    "        output = outputs[-1]\n",
    "        sl,bs,_ = output.size()\n",
    "#         avgpool = self.avg_pool(output, lengths)\n",
    "#         mxpool = self.max_pool(output, bs)\n",
    "        return [self.last_output(outputs[-1].transpose(1,0), lengths)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = RNN_Encoder_Masks(vocab_sz=1000, emb_sz=400, weight_p=0.0,\n",
    "#                                                  n_hid=256, n_layers=2, pad_token=0, qrnn=False).to(device)\n",
    "# dropout = LockedDropout(0.1)\n",
    "# classifier = PoolingLinearClassifierMask(layers=[400*3, 50, 300], drops=[0.4, 0.1]).to(device)\n",
    "# s = 8\n",
    "# b = 11\n",
    "# x = torch.randint(0,1000,(b,s))\n",
    "# x = torch.tensor(x.transpose(1,0),dtype=torch.long,device=device)\n",
    "# output = classifier(encoder(x))\n",
    "# output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \"\"\"\n",
    "        Boilerplate class which helps others have some common functionality.\n",
    "        These are made with some debugging/loading and with corechains in mind\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def prepare_save(self):\n",
    "        pass\n",
    "\n",
    "    def load_from(self, location):\n",
    "        # Pull the data from disk\n",
    "        model_dump = torch.load(location)\n",
    "\n",
    "        # Load parameters\n",
    "        for key in self.prepare_save():\n",
    "            key[1].load_state_dict(model_dump[key[0]])\n",
    "\n",
    "    def get_parameter_sum(self):\n",
    "\n",
    "        sum = 0\n",
    "        for model in self.prepare_save():\n",
    "\n",
    "            model_sum = 0\n",
    "            for x in list(model[1].parameters()):\n",
    "\n",
    "                model_sum += np.sum(x.data.cpu().numpy().flatten())\n",
    "\n",
    "            sum += model_sum\n",
    "\n",
    "        return sum\n",
    "\n",
    "    def freeze_layer(self,layer):\n",
    "        for params in layer.parameters():\n",
    "            params.requires_grad = False\n",
    "\n",
    "    def unfreeze_layer(self,layer):\n",
    "        for params in layer.parameters():\n",
    "            params.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     73
    ]
   },
   "outputs": [],
   "source": [
    "class BiLstmDot_ulmfit(Model):\n",
    "\n",
    "    def __init__(self, _parameter_dict, _word_to_id, _device, _pointwise=False, _debug=False):\n",
    "\n",
    "        self.debug = _debug\n",
    "        self.parameter_dict = _parameter_dict\n",
    "        self.device = _device\n",
    "        self.pointwise = _pointwise\n",
    "        self.word_to_id = _word_to_id\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"Init Models\")\n",
    "\n",
    "        # Load the pre-trained model\n",
    "#         pretrained_weights = torch.load('./ulmfit/wt103/fwd_wt103_enc.h5', map_location= lambda storage, loc: storage)\n",
    "#         new_vectors = self.parameter_dict['vectors']\n",
    "#         pretrained_weights['encoder.weight'] = T(new_vectors)\n",
    "#         pretrained_weights['encoder_with_dropout.embed.weight'] = T(np.copy(new_vectors))\n",
    "\n",
    "        # self.encoder = fastai.old.lm_rnn.RNN_Encoder(ntoken=new_vectors.shape[0], emb_sz=400, n_hid=1150, n_layers=1, pad_token=0,qrnn=False).to(self.device)\n",
    "        # self.encoder.load_state_dict(pretrained_weights)\n",
    "        # fastai.RNNTrainer\n",
    "        # self.encoder = com.NotSuchABetterEncoder(\n",
    "        #     number_of_layer=self.parameter_dict['number_of_layer'],\n",
    "        #     bidirectional=self.parameter_dict['bidirectional'],\n",
    "        #     embedding_dim=self.parameter_dict['embedding_dim'],\n",
    "        #     max_length = self.parameter_dict['max_length'],\n",
    "        #     hidden_dim=self.parameter_dict['hidden_size'],\n",
    "        #     vocab_size=self.parameter_dict['vocab_size'],\n",
    "        #     dropout=self.parameter_dict['dropout'],\n",
    "        #     vectors=self.parameter_dict['vectors'],\n",
    "        #     enable_layer_norm=False,\n",
    "        #     mode = 'LSTM',\n",
    "        #     debug = self.debug).to(self.device)\n",
    "\n",
    "#         self.encoder = RNN_Encoder(ntoken=self.parameter_dict['vectors'].shape[0], emb_sz=400,\n",
    "#                                                  n_hid=256, n_layers=1, pad_token=0, qrnn=False)\n",
    "        \n",
    "        self.encoder = merity.RNNModel(rnn_type='LSTM', \n",
    "                ntoken=self.parameter_dict['vectors'].shape[0],\n",
    "                ninp=400,\n",
    "                nhid=256,\n",
    "                nlayers=1,\n",
    "                dropout=0.1,\n",
    "                dropouth=0.5,\n",
    "                dropouti=0.5,\n",
    "                dropoute=0.5,\n",
    "                wdrop=0,\n",
    "                tie_weights=False)\n",
    "        self.encoder.encoder.padding_idx = 0\n",
    "        \n",
    "        self.encoder.encoder = nn.Embedding.from_pretrained(torch.FloatTensor(self.parameter_dict['vectors']))\n",
    "        self.encoder = self.encoder.to(self.device)\n",
    "         \n",
    "        \n",
    "#         self.encoder = RNN_Encoder_Masks(vocab_sz=self.parameter_dict['vectors'].shape[0], emb_sz=400, weight_p=0,\n",
    "#                                                  n_hid=256, n_layers=1, pad_token=0, qrnn=False).to(device)\n",
    "#         self.encoder.load_state_dict(pretrained_weights)\n",
    "#         self.encoder.encoder.weight = torch.tensor(self.parameter_dict['vectors'],device=self.device)\n",
    "#         self.encoder.encoder = nn.Embedding.from_pretrained(torch.FloatTensor(self.parameter_dict['vectors']))\n",
    "        \n",
    "#         self.classifier = PoolingLinearClassifier(layers=[400*3, 50, 300], drops=[0.4, 0.1]).to(device)\n",
    "#         self.classifier = self.classifier.to(device)\n",
    "        \n",
    "#         self.encoder = self.encoder.to(self.device)\n",
    "        \n",
    "#         self.encoder.reset()\n",
    "    \n",
    "        # self.linear = torch.nn.Linear(1150,256).to(self.device)\n",
    "\n",
    "    def train(self, data, optimizer, loss_fn, device):\n",
    "        if self.pointwise:\n",
    "            return self._train_pointwise_(data, optimizer, loss_fn, device)\n",
    "        else:\n",
    "            return self._train_pairwise_(data, optimizer, loss_fn, device)\n",
    "\n",
    "    def _train_pointwise_(self, data, optimizer, loss_fn, device):\n",
    "        self.encoder.train()\n",
    "        self.classifier.train()        \n",
    "        optimizer.zero_grad()\n",
    "        self.encoder.reset_hidden()\n",
    "        \n",
    "        ques_batch, path_batch, y_label = data['ques_batch'], data['path_batch'], data['y_label']\n",
    "        self.encoder.reset_hidden()\n",
    "        op_q = self.encoder(ques_batch.transpose(1,0))\n",
    "        self.encoder.reset_hidden()\n",
    "        op_p = self.encoder(path_batch.transpose(1,0))\n",
    "        \n",
    "        ques_batch_encoded =  op_q\n",
    "        path_batch_encoded =  op_p\n",
    "        \n",
    "         #Calculating dot score\n",
    "        scores = torch.sum(ques_batch_encoded * path_batch_encoded, -1)\n",
    "\n",
    "        try:\n",
    "            loss = loss_fn(scores, y_label)\n",
    "        except RuntimeError:\n",
    "            traceback.print_exc()\n",
    "            print(pos_scores.shape, neg_scores.shape, y_label.shape,  ques_batch.shape, pos_batch.shape, neg_batch.shape)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss\n",
    "        \n",
    "        \n",
    "    def _train_pairwise_(self, data, optimizer, loss_fn, device):\n",
    "        '''\n",
    "            Given data, passes it through model, inited in constructor, returns loss and updates the weight\n",
    "            :params data: {batch of question, pos paths, neg paths and dummy y labels}\n",
    "            :params optimizer: torch.optim object\n",
    "            :params loss fn: torch.nn loss object\n",
    "            :params device: torch.device object\n",
    "\n",
    "            returns loss\n",
    "        '''\n",
    "        self.encoder.train()\n",
    "        # Unpacking the data and model from args\n",
    "        ques_batch, pos_batch, neg_batch, y_label = data['ques_batch'], data['pos_batch'], data['neg_batch'], data['y_label']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        #Encoding all the data\n",
    "\n",
    "#         print(f\"Before: \\t {ques_batch.shape[1]}, {pos_batch.shape[1]}, {neg_batch.shape[1]}\\n\"\n",
    "#         f\"After: \\t\\t {tu.trim(ques_batch).shape[1]}, {tu.trim(pos_batch).shape[1]}, {tu.trim(neg_batch).shape[1]}\")\n",
    "        \n",
    "        ques_batch = tu.trim(ques_batch)\n",
    "        pos_batch = tu.trim(pos_batch)\n",
    "        neg_batch = tu.trim(neg_batch)\n",
    "        \n",
    "#         print(pos_batch)\n",
    "#         print(neg_batch)\n",
    "\n",
    "        h = self.encoder.init_hidden(ques_batch.shape[0])\n",
    "        op_q = self.encoder(ques_batch.transpose(1,0), h)[1][-1][0].squeeze()\n",
    "        op_p = self.encoder(pos_batch.transpose(1,0), h)[1][-1][0].squeeze()\n",
    "        op_n = self.encoder(neg_batch.transpose(1,0), h)[1][-1][0].squeeze()\n",
    "        \n",
    "#         print(op_q.shape, op_p.shape, op_n.shape)\n",
    "#         raise IOError\n",
    "            \n",
    "        ques_batch_encoded =  op_q\n",
    "        pos_batch_encoded =  op_p\n",
    "        neg_batch_encoded =  op_n\n",
    "        \n",
    "#         ques_batch_encoded =  op_q[-1][0][1].squeeze()\n",
    "#         pos_batch_encoded =  op_p[-1][0][1].squeeze()\n",
    "#         neg_batch_encoded =  op_n[-1][0][1].squeeze()\n",
    "        \n",
    "#         print(ques_batch_encoded.shape, pos_batch_encoded.shape, neg_batch_encoded.shape)\n",
    "#         raise IOError\n",
    "        \n",
    "        #Calculating dot score\n",
    "        pos_scores = torch.sum(ques_batch_encoded * pos_batch_encoded, -1)\n",
    "        neg_scores = torch.sum(ques_batch_encoded * neg_batch_encoded, -1)\n",
    "#         print(pos_scores, neg_scores)\n",
    "        \n",
    "#         raise IOError\n",
    "        \n",
    "#         if True:\n",
    "#             print(\"ques_batch shape is \", ques_batch.shape)\n",
    "#             print(\"pos_batch shape is \", pos_batch.shape)\n",
    "#             print(\"neg_batch shape is \", neg_batch.shape)\n",
    "#             print(\"transposed ques bathc is \", ques_batch.transpose(1,0).shape)\n",
    "#             for o in op_p[1]:\n",
    "#                 print(\"o shape is \", o.shape)\n",
    "#             print(\"encoded pos batch shape is \", op_p[1][-1][-1].shape)\n",
    "            \n",
    "#             print(\"pos_score is\", pos_scores.shape)\n",
    "        '''\n",
    "            If `y == 1` then it assumed the first input should be ranked higher\n",
    "            (have a larger value) than the second input, and vice-versa for `y == -1`\n",
    "        '''\n",
    "#         raise ValueError\n",
    "        try:\n",
    "            loss = loss_fn(pos_scores, neg_scores, y_label)\n",
    "        except RuntimeError:\n",
    "            traceback.print_exc()\n",
    "            print(pos_scores.shape, neg_scores.shape, y_label.shape,  ques_batch.shape, pos_batch.shape, neg_batch.shape)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.encoder.parameters(), .5)\n",
    "        optimizer.step()\n",
    "        return loss\n",
    "\n",
    "    def predict(self, ques, paths, device):\n",
    "        \"\"\"\n",
    "            Same code works for both pairwise or pointwise\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "\n",
    "#             print(ques.shape, paths.shape, tu.trim(ques).shape,tu.trim(paths).shape)\n",
    "            \n",
    "#             if tu.trim(ques).shape[0] == 0:\n",
    "#                 print(ques)\n",
    "#                 print(paths)\n",
    "            \n",
    "            self.encoder.eval()\n",
    "#             h = self.encoder.reset()\n",
    "            # Encoding all the data\n",
    "            ques = tu.trim(ques)\n",
    "            paths = tu.trim(paths)\n",
    "    \n",
    "            \n",
    "            h  = self.encoder.init_hidden(ques.shape[0])\n",
    "            op_q = self.encoder(ques.transpose(1,0), h)\n",
    "            op_p = self.encoder(paths.transpose(1,0), h)\n",
    "\n",
    "#             question = op_q[1][0][-1]\n",
    "#             paths = op_p[1][0][-1]\n",
    "            question = op_q[1][-1][0].squeeze()\n",
    "            paths = op_p[1][-1][0].squeeze()\n",
    "        \n",
    "#             question = op_q[-1][0][1].squeeze()\n",
    "#             paths = op_p[-1][0][1].squeeze()\n",
    "\n",
    "            if self.pointwise:\n",
    "                # question = F.normalize(F.relu(question),p=1,dim=1)\n",
    "                # paths = F.normalize(F.relu(paths),p=1,dim=1)\n",
    "                # norm_ques_batch = torch.abs(torch.norm(question, dim=1, p=1))\n",
    "                # norm_pos_batch = torch.abs(torch.norm(paths, dim=1, p=1))\n",
    "                score = torch.sum(question * paths, -1)\n",
    "                # score = score.div(norm_ques_batch * norm_pos_batch).div_(2.0).add_(0.5)\n",
    "            else:\n",
    "                score = torch.sum(question * paths, -1)\n",
    "\n",
    "            self.encoder.train()\n",
    "            return score\n",
    "    \n",
    "    def prepare_save(self):\n",
    "        \"\"\"\n",
    "\n",
    "            This function is called when someone wants to save the underlying models.\n",
    "            Returns a tuple of key:model pairs which is to be interpreted within save model.\n",
    "\n",
    "        :return: [(key, model)]\n",
    "        \"\"\"\n",
    "        return [('encoder', self.encoder)]\n",
    "\n",
    "    def load_from(self, location):\n",
    "        # Pull the data from disk\n",
    "        if self.debug: print(\"loading Bilstmdot model from\", location)\n",
    "        self.encoder.load_state_dict(torch.load(location)['encoder'])\n",
    "        if self.debug: print(\"model loaded with weights ,\", self.get_parameter_sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     80,
     159,
     169
    ]
   },
   "outputs": [],
   "source": [
    "class BiLstmDot(Model):\n",
    "\n",
    "    def __init__(self, _parameter_dict, _word_to_id, _device, _pointwise=False, _debug=False):\n",
    "\n",
    "        self.debug = _debug\n",
    "        self.parameter_dict = _parameter_dict\n",
    "        self.device = _device\n",
    "        self.pointwise = _pointwise\n",
    "        self.word_to_id = _word_to_id\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"Init Models\")\n",
    "\n",
    "\n",
    "        self.encoder = com.Encoder(self.parameter_dict['max_length'], self.parameter_dict['hidden_size'],\n",
    "                                   self.parameter_dict['number_of_layer'], self.parameter_dict['embedding_dim'],\n",
    "                                   self.parameter_dict['vocab_size'],\n",
    "                                   bidirectional=self.parameter_dict['bidirectional'],\n",
    "                                   vectors=self.parameter_dict['vectors']).to(self.device)            \n",
    "\n",
    "#         self.encoder = com.NotSuchABetterEncoder(\n",
    "#             number_of_layer=self.parameter_dict['number_of_layer'],\n",
    "#             bidirectional=self.parameter_dict['bidirectional'],\n",
    "#             embedding_dim=self.parameter_dict['embedding_dim'],\n",
    "#             max_length = self.parameter_dict['max_length'],\n",
    "#             hidden_dim=self.parameter_dict['hidden_size'],\n",
    "#             vocab_size=self.parameter_dict['vocab_size'],\n",
    "#             dropout=self.parameter_dict['dropout'],\n",
    "#             vectors=self.parameter_dict['vectors'],\n",
    "#             enable_layer_norm=False,\n",
    "#             mode = 'LSTM',\n",
    "#             debug = self.debug).to(self.device)\n",
    "\n",
    "    def train(self, data, optimizer, loss_fn, device):\n",
    "    #\n",
    "        if self.pointwise:\n",
    "            return self._train_pointwise_(data, optimizer, loss_fn, device)\n",
    "        else:\n",
    "            return self._train_pairwise_(data, optimizer, loss_fn, device)\n",
    "\n",
    "    def _train_pairwise_(self, data, optimizer, loss_fn, device):\n",
    "        '''\n",
    "            Given data, passes it through model, inited in constructor, returns loss and updates the weight\n",
    "            :params data: {batch of question, pos paths, neg paths and dummy y labels}\n",
    "            :params optimizer: torch.optim object\n",
    "            :params loss fn: torch.nn loss object\n",
    "            :params device: torch.device object\n",
    "\n",
    "            returns loss\n",
    "        '''\n",
    "\n",
    "        # Unpacking the data and model from args\n",
    "        ques_batch, pos_batch, neg_batch, y_label = data['ques_batch'], data['pos_batch'], data['neg_batch'], data['y_label']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        #Encoding all the data\n",
    "\n",
    "        hidden = self.encoder.init_hidden(ques_batch.shape[0],self.device)\n",
    "        ques_batch_encoded, _ = self.encoder(tu.trim(ques_batch), hidden)\n",
    "        pos_batch_encoded, _ = self.encoder(tu.trim(pos_batch), hidden)\n",
    "        neg_batch_encoded, _  = self.encoder(tu.trim(neg_batch), hidden)\n",
    "\n",
    "\n",
    "\n",
    "        #Calculating dot score\n",
    "        pos_scores = torch.sum(ques_batch_encoded[-1] * pos_batch_encoded[-1], -1)\n",
    "        neg_scores = torch.sum(ques_batch_encoded[-1] * neg_batch_encoded[-1], -1)\n",
    "        '''\n",
    "            If `y == 1` then it assumed the first input should be ranked higher\n",
    "            (have a larger value) than the second input, and vice-versa for `y == -1`\n",
    "        '''\n",
    "        try:\n",
    "            loss = loss_fn(pos_scores, neg_scores, y_label)\n",
    "        except RuntimeError:\n",
    "            print(pos_scores.shape, neg_scores.shape, y_label.shape,  ques_batch.shape, pos_batch.shape, neg_batch.shape)\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(self.encoder.parameters(), .5)\n",
    "        optimizer.step()\n",
    "        return loss\n",
    "\n",
    "    def _train_pointwise_(self, data, optimizer, loss_fn, device):\n",
    "        '''\n",
    "            Given data, passes it through model, inited in constructor, returns loss and updates the weight\n",
    "            :params data: {batch of question, paths and y labels}\n",
    "            :params models list of [models]\n",
    "            :params optimizer: torch.optim object\n",
    "            :params loss fn: torch.nn loss object\n",
    "            :params device: torch.device object\n",
    "            returrns loss\n",
    "        '''\n",
    "        # Unpacking the data and model from args\n",
    "        ques_batch, path_batch, y_label = data['ques_batch'], data['path_batch'], data['y_label']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Encoding all the data\n",
    "        hidden = self.encoder.init_hidden(ques_batch.shape[0], self.device)\n",
    "        _, ques_batch, _, _ = self.encoder(tu.trim(ques_batch), hidden)\n",
    "        _, pos_batch, _, _ = self.encoder(tu.trim(path_batch), hidden)\n",
    "\n",
    "        #\n",
    "        # norm_ques_batch = torch.abs(torch.norm(ques_batch,dim=1,p=1))\n",
    "        # norm_pos_batch = torch.abs(torch.norm(pos_batch,dim=1,p=1))\n",
    "\n",
    "        # ques_batch = F.normalize(F.relu(ques_batch),p=1,dim=1)\n",
    "        # pos_batch = F.normalize(F.relu(pos_batch),p=1,dim=1)\n",
    "        # ques_batch =(F.normalize(ques_batch,p=1,dim=1)/2) + .5\n",
    "        # pos_batch =(F.normalize(pos_batch,p=1,dim=1)/2) + .5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Calculating dot score\n",
    "        score = torch.sum(ques_batch * pos_batch, -1)\n",
    "        # score = score.div(norm_ques_batch*norm_pos_batch).div_(2.0).add_(0.5)\n",
    "            # print(\"shape of score is,\", score.shape)\n",
    "            # print(\"score is , \", score)\n",
    "            #\n",
    "            #\n",
    "            # print(\"shape of y label is \", y_label.shape)\n",
    "            # print(\"value of y label is \", y_label)\n",
    "\n",
    "        # raise ValueError\n",
    "\n",
    "        '''\n",
    "            Binary Cross Entropy loss function. @TODO: Check if we can give it 1/0 labels.\n",
    "        '''\n",
    "        loss = loss_fn(score, y_label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.encoder.parameters(), .5)\n",
    "        optimizer.step()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def predict(self, ques, paths, device):\n",
    "        \"\"\"\n",
    "            Same code works for both pairwise or pointwise\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "\n",
    "            self.encoder.eval()\n",
    "            hidden = self.encoder.init_hidden(ques.shape[0], self.device)\n",
    "\n",
    "            question, _ = self.encoder(tu.trim(ques.long()), hidden)\n",
    "            paths, _ = self.encoder(tu.trim(paths.long()), hidden)\n",
    "\n",
    "            if self.pointwise:\n",
    "                # question = F.normalize(F.relu(question),p=1,dim=1)\n",
    "                # paths = F.normalize(F.relu(paths),p=1,dim=1)\n",
    "                # norm_ques_batch = torch.abs(torch.norm(question, dim=1, p=1))\n",
    "                # norm_pos_batch = torch.abs(torch.norm(paths, dim=1, p=1))\n",
    "                score = torch.sum(question[-1] * paths[-1], -1)\n",
    "                # score = score.div(norm_ques_batch * norm_pos_batch).div_(2.0).add_(0.5)\n",
    "            else:\n",
    "                score = torch.sum(question[-1] * paths[-1], -1)\n",
    "\n",
    "            self.encoder.train()\n",
    "            return score\n",
    "\n",
    "    def prepare_save(self):\n",
    "        \"\"\"\n",
    "\n",
    "            This function is called when someone wants to save the underlying models.\n",
    "            Returns a tuple of key:model pairs which is to be interpreted within save model.\n",
    "\n",
    "        :return: [(key, model)]\n",
    "        \"\"\"\n",
    "        return [('encoder', self.encoder)]\n",
    "\n",
    "    def load_from(self, location):\n",
    "        # Pull the data from disk\n",
    "        if self.debug: print(\"loading Bilstmdot model from\", location)\n",
    "        self.encoder.load_state_dict(torch.load(location)['encoder'])\n",
    "        if self.debug: print(\"model loaded with weights ,\", self.get_parameter_sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_dict['vectors'] = data['vectors']\n",
    "parameter_dict['schema'] = schema\n",
    "\n",
    "parameter_dict['bidirectional'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeler = net.BiLstmDot(_parameter_dict = parameter_dict,\n",
    "                    _word_to_id=_word_to_id,\n",
    "                    _device=device,\n",
    "                    _pointwise=pointwise,\n",
    "                    _debug=False)\n",
    "\n",
    "optimizer = optim.Adam(list(filter(lambda p: p.requires_grad, modeler.encoder.parameters())))\n",
    "#                       list(filter(lambda p: p.requires_grad, modeler.classifier.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def training_loop(training_model, parameter_dict,modeler,train_loader,\n",
    "                  optimizer,loss_func, data, dataset, device, test_every, validate_every , pointwise = False, problem='core_chain',curtail_padding_rel=True):\n",
    "\n",
    "    model_save_location = aux.save_location(problem, training_model, dataset)\n",
    "    aux_save_information = {\n",
    "        'epoch' : 0,\n",
    "        'test_accuracy':0.0,\n",
    "        'validation_accura\"\"cy':0.0,\n",
    "        'parameter_dict':parameter_dict\n",
    "    }\n",
    "    train_loss = []\n",
    "    valid_accuracy = []\n",
    "    test_accuracy = []\n",
    "    best_validation_accuracy = 0\n",
    "    best_test_accuracy = 0\n",
    "\n",
    "    if parameter_dict['schema'] == 'reldet':\n",
    "        parameter_dict['rel1_pad'] =  parameter_dict['relrd_pad']\n",
    "    elif parameter_dict['schema'] == 'slotptr':\n",
    "        parameter_dict['rel1_pad'] = parameter_dict['relsp_pad']\n",
    "\n",
    "        ###############\n",
    "    # Training Loop\n",
    "    ###############\n",
    "\n",
    "\n",
    "    #Makes test data of appropriate shape\n",
    "    print(\"the dataset is \", dataset)\n",
    "    if curtail_padding_rel and dataset == 'lcquad':\n",
    "        data = cc.curatail_padding(data, parameter_dict)\n",
    "        data['valid_neg_paths'] = np.zeros_like(data['valid_neg_paths'])\n",
    "        data['valid_pos_paths'] = np.zeros_like(data['valid_pos_paths'])\n",
    "        data['valid_questions'] = np.zeros_like(data['valid_questions'])\n",
    "\n",
    "    try:\n",
    "\n",
    "        for epoch in range(parameter_dict['epochs']):\n",
    "\n",
    "            # Epoch start print\n",
    "            print(\"Epoch: \", epoch, \"/\", parameter_dict['epochs'])\n",
    "\n",
    "            # Bookkeeping variables\n",
    "            epoch_loss = []\n",
    "            epoch_time = time.time()\n",
    "\n",
    "            # Loop for one batch\n",
    "            # tqdm_loop = tqdm(enumerate(train_loader))\n",
    "            for i_batch, sample_batched in enumerate(train_loader):\n",
    "\n",
    "                # Bookkeeping and data preparation\n",
    "                batch_time = time.time()\n",
    "\n",
    "                if not pointwise:\n",
    "                    ques_batch = torch.tensor(np.reshape(sample_batched[0][0], (-1, parameter_dict['max_length'])),\n",
    "                                              dtype=torch.long, device=device)\n",
    "                    pos_batch = torch.tensor(np.reshape(sample_batched[0][1], (-1, parameter_dict['max_length'])),\n",
    "                                             dtype=torch.long, device=device)\n",
    "                    neg_batch = torch.tensor(np.reshape(sample_batched[0][2], (-1, parameter_dict['max_length'])),\n",
    "                                             dtype=torch.long, device=device)\n",
    "\n",
    "                    data['dummy_y'] = torch.ones(ques_batch.shape[0], device=device)\n",
    "                    if parameter_dict['schema'] != 'default':\n",
    "                        pos_rel1_batch = torch.tensor(np.reshape(sample_batched[0][3], (-1, parameter_dict['max_length'])),\n",
    "                                                      dtype=torch.long, device=device)\n",
    "                        pos_rel2_batch = torch.tensor(np.reshape(sample_batched[0][4], (-1, parameter_dict['max_length'])),\n",
    "                                                      dtype=torch.long, device=device)\n",
    "                        neg_rel1_batch = torch.tensor(np.reshape(sample_batched[0][5], (-1, parameter_dict['max_length'])),\n",
    "                                                      dtype=torch.long, device=device)\n",
    "                        neg_rel2_batch = torch.tensor(np.reshape(sample_batched[0][6], (-1, parameter_dict['max_length'])),\n",
    "                                                      dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "                        data_batch = {\n",
    "                            'ques_batch': ques_batch,\n",
    "                            'pos_batch': pos_batch[:,:parameter_dict['rel_pad']],\n",
    "                            'neg_batch': neg_batch[:,:parameter_dict['rel_pad']],\n",
    "                            'y_label': data['dummy_y'],\n",
    "                            'pos_rel1_batch': pos_rel1_batch[:,:parameter_dict['rel1_pad']],\n",
    "                            'pos_rel2_batch':pos_rel2_batch[:,:parameter_dict['rel1_pad']],\n",
    "                            'neg_rel1_batch':neg_rel1_batch[:,:parameter_dict['rel1_pad']],\n",
    "                            'neg_rel2_batch' : neg_rel2_batch[:,:parameter_dict['rel1_pad']]\n",
    "                        }\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        data_batch = {\n",
    "                            'ques_batch': ques_batch,\n",
    "                            'pos_batch': pos_batch[:,:parameter_dict['rel_pad']],\n",
    "                            'neg_batch': neg_batch[:,:parameter_dict['rel_pad']],\n",
    "                            'y_label': data['dummy_y']}\n",
    "\n",
    "                else:\n",
    "                    ques_batch = torch.tensor(np.reshape(sample_batched[0][0], (-1, parameter_dict['max_length'])),\n",
    "                                              dtype=torch.long, device=device)\n",
    "                    path_batch = torch.tensor(np.reshape(sample_batched[0][1], (-1, parameter_dict['max_length'])),\n",
    "                                             dtype=torch.long, device=device)\n",
    "                    y = torch.tensor(sample_batched[1],dtype = torch.float,device=device).view(-1)\n",
    "\n",
    "\n",
    "                    if parameter_dict['schema'] != 'default':\n",
    "                        path_rel1_batch = torch.tensor(np.reshape(sample_batched[0][2], (-1, parameter_dict['max_length'])),\n",
    "                                                      dtype=torch.long, device=device)\n",
    "                        path_rel2_batch = torch.tensor(np.reshape(sample_batched[0][3], (-1, parameter_dict['max_length'])),\n",
    "                                                      dtype=torch.long, device=device)\n",
    "\n",
    "                        data_batch = {\n",
    "                            'ques_batch': ques_batch,\n",
    "                            'path_batch': path_batch[:,:parameter_dict['rel_pad']],\n",
    "                            'y_label': y,\n",
    "                            'path_rel1_batch': path_rel1_batch[:,:parameter_dict['rel1_pad']],\n",
    "                            'path_rel2_batch': path_rel2_batch[:,:parameter_dict['rel1_pad']]\n",
    "                        }\n",
    "                    else:\n",
    "                        data_batch = {\n",
    "                            'ques_batch': ques_batch,\n",
    "                            'path_batch': path_batch[:,:parameter_dict['rel_pad']],\n",
    "                            'y_label': y\n",
    "                        }\n",
    "\n",
    "\n",
    "                return data_batch\n",
    "                loss = modeler.train(data=data_batch,\n",
    "                                  optimizer=optimizer,\n",
    "                                  loss_fn=loss_func,\n",
    "                                  device=device)\n",
    "\n",
    "                # Bookkeep the training loss\n",
    "                epoch_loss.append(loss.item())\n",
    "\n",
    "                # tqdm_loop.desc(\"#\"+str(i_batch)+\"\\tLoss:\" + str(loss.item())[:min(5, len(str(loss.item())))])\n",
    "\n",
    "                print(\"Batch:\\t%d\" % i_batch, \"/%d\\t: \" % (parameter_dict['batch_size']),\n",
    "                      \"%s\" % (time.time() - batch_time),\n",
    "                      \"\\t%s\" % (time.time() - epoch_time),\n",
    "                      \"\\t%s\" % (str(loss.item())),\n",
    "                      end=None if i_batch + 1 == int(int(i_batch) / parameter_dict['batch_size']) else \"\\n\")\n",
    "\n",
    "            # EPOCH LEVEL\n",
    "\n",
    "            # Track training loss\n",
    "            train_loss.append(sum(epoch_loss))\n",
    "\n",
    "            # test_every = False\n",
    "            if test_every:\n",
    "                # Run on test set\n",
    "                if epoch%test_every == 0:\n",
    "                    if parameter_dict['schema'] != 'default':\n",
    "                        if parameter_dict['schema']  == 'slotptr':\n",
    "                            test_accuracy.append(aux.validation_accuracy(data['test_questions'], data['test_pos_paths'],\n",
    "                                                             data['test_neg_paths'],modeler, device,data['test_pos_paths_rel1_sp'],data['test_pos_paths_rel2_sp'],\n",
    "                                                                 data['test_neg_paths_rel1_sp'],data['test_neg_paths_rel2_sp']))\n",
    "                        else:\n",
    "                            test_accuracy.append(aux.validation_accuracy(data['test_questions'], data['test_pos_paths'],\n",
    "                                                                         data['test_neg_paths'], modeler, device,\n",
    "                                                                         data['test_pos_paths_rel1_rd'],\n",
    "                                                                         data['test_pos_paths_rel2_rd'],\n",
    "                                                                         data['test_neg_paths_rel1_rd'],\n",
    "                                                                         data['test_neg_paths_rel2_rd']))\n",
    "                    else:\n",
    "                        test_accuracy.append(aux.validation_accuracy(data['test_questions'], data['test_pos_paths'],\n",
    "                                                                     data['test_neg_paths'], modeler, device))\n",
    "                    if test_accuracy[-1] >= best_test_accuracy:\n",
    "                        best_test_accuracy = test_accuracy[-1]\n",
    "                        aux_save_information['test_accuracy'] = best_test_accuracy\n",
    "            else:\n",
    "                test_accuracy.append(0)\n",
    "                best_test_accuracy = 0\n",
    "\n",
    "            # Run on validation set\n",
    "            if validate_every:\n",
    "                if epoch%validate_every == 0:\n",
    "                    if parameter_dict['schema'] != 'default':\n",
    "                        if parameter_dict['schema'] == 'slotptr':\n",
    "                            valid_accuracy.append(aux.validation_accuracy(data['valid_questions'], data['valid_pos_paths'],\n",
    "                                                              data['valid_neg_paths'],  modeler, device, data['valid_pos_paths_rel1_sp'],data['valid_pos_paths_rel2_sp'],\n",
    "                                                                 data['valid_neg_paths_rel1_sp'],data['valid_neg_paths_rel2_sp']))\n",
    "                        else:\n",
    "                            valid_accuracy.append(aux.validation_accuracy(data['valid_questions'][:-1], data['valid_pos_paths'][:-1],\n",
    "                                                                          data['valid_neg_paths'][:-1], modeler, device,\n",
    "                                                                          data['valid_pos_paths_rel1_rd'][:-1],\n",
    "                                                                          data['valid_pos_paths_rel2_rd'][:-1],\n",
    "                                                                          data['valid_neg_paths_rel1_rd'][:-1],\n",
    "                                                                          data['valid_neg_paths_rel2_rd'][:-1]))\n",
    "                    else:\n",
    "                        valid_accuracy.append(aux.validation_accuracy(data['valid_questions'], data['valid_pos_paths'],\n",
    "                                                                      data['valid_neg_paths'], modeler, device))\n",
    "                    if valid_accuracy[-1] > best_validation_accuracy:\n",
    "                        print(\"MODEL WEIGHTS RIGHT NOW: \", modeler.get_parameter_sum())\n",
    "                        best_validation_accuracy = valid_accuracy[-1]\n",
    "                        aux_save_information['epoch'] = epoch\n",
    "                        aux_save_information['validation_accuracy'] = best_validation_accuracy\n",
    "                        aux.save_model(model_save_location, modeler, model_name='model.torch'\n",
    "                                   , epochs=epoch, optimizer=optimizer, accuracy=best_validation_accuracy, aux_save_information=aux_save_information)\n",
    "\n",
    "            # Resample new negative paths per epoch and shuffle all data\n",
    "            train_loader.dataset.shuffle()\n",
    "\n",
    "            # Epoch level prints\n",
    "            print(\"Time: %s\\t\" % (time.time() - epoch_time),\n",
    "                  \"Loss: %s\\t\" % (sum(epoch_loss)),\n",
    "                  \"Valdacc: %s\\t\" % (valid_accuracy[-1]),\n",
    "                    \"Testacc: %s\\n\" % (test_accuracy[-1]),\n",
    "                  \"BestValidAcc: %s\\n\" % (best_validation_accuracy),\n",
    "                  \"BestTestAcc: %s\\n\" % (best_test_accuracy))\n",
    "\n",
    "        return train_loss, modeler, valid_accuracy, test_accuracy\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('-' * 89)\n",
    "        return train_loss, modeler, valid_accuracy, test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/priyansh/conda/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dataset is  lcquad\n",
      "Epoch:  0 / 200\n",
      "Batch:\t0 /4000\t:  0.18439483642578125 \t0.18633723258972168 \t3747.1337890625\n",
      "Batch:\t1 /4000\t:  0.17785239219665527 \t0.3659210205078125 \t2612.345458984375\n",
      "Batch:\t2 /4000\t:  0.16532492637634277 \t0.5328128337860107 \t2171.2763671875\n",
      "Batch:\t3 /4000\t:  0.16448497772216797 \t0.6990041732788086 \t2168.224365234375\n",
      "Batch:\t4 /4000\t:  0.16237759590148926 \t0.8632485866546631 \t1932.858154296875\n",
      "Batch:\t5 /4000\t:  0.19405031204223633 \t1.0590980052947998 \t1758.6558837890625\n",
      "Batch:\t6 /4000\t:  0.1609654426574707 \t1.2268600463867188 \t1687.7353515625\n",
      "Batch:\t7 /4000\t:  0.16631865501403809 \t1.3949918746948242 \t1502.944580078125\n",
      "Batch:\t8 /4000\t:  0.16706061363220215 \t1.5639498233795166 \t1383.369873046875\n",
      "Batch:\t9 /4000\t:  0.1625051498413086 \t1.728454351425171 \t1358.6357421875\n",
      "Batch:\t10 /4000\t:  0.17899346351623535 \t1.9093446731567383 \t1297.380615234375\n",
      "Batch:\t11 /4000\t:  0.15916848182678223 \t2.070383071899414 \t1276.4354248046875\n",
      "Batch:\t12 /4000\t:  0.16166234016418457 \t2.2338521480560303 \t1147.6739501953125\n",
      "Batch:\t13 /4000\t:  0.16569256782531738 \t2.401773691177368 \t1133.7332763671875\n",
      "Batch:\t14 /4000\t:  0.16818857192993164 \t2.5719339847564697 \t1069.5726318359375\n",
      "Batch:\t15 /4000\t:  0.15986156463623047 \t2.7332756519317627 \t1087.2767333984375\n",
      "Batch:\t16 /4000\t:  0.16315770149230957 \t2.8983545303344727 \t1042.9871826171875\n",
      "Batch:\t17 /4000\t:  0.10080313682556152 \t3.000267267227173 \t491.4232482910156\n",
      "MODEL WEIGHTS RIGHT NOW:  16084.48236322403\n",
      "model with accuracy  0.29 stored at data/models/core_chain/bilstm_dot_ulmfit/lcquad/93/model.torch\n",
      "Time: 6.569644451141357\t Loss: 28869.662628173828\t Valdacc: 0.29\t Testacc: 0.24\n",
      " BestValidAcc: 0.29\n",
      " BestTestAcc: 0.24\n",
      "\n",
      "Epoch:  1 / 200\n",
      "Batch:\t0 /4000\t:  0.19001555442810059 \t0.19276905059814453 \t932.8788452148438\n",
      "Batch:\t1 /4000\t:  0.16463613510131836 \t0.35985732078552246 \t854.171630859375\n",
      "Batch:\t2 /4000\t:  0.168961763381958 \t0.5303657054901123 \t847.3291015625\n",
      "Batch:\t3 /4000\t:  0.15897011756896973 \t0.6910810470581055 \t768.516357421875\n",
      "Batch:\t4 /4000\t:  0.17895293235778809 \t0.8714706897735596 \t788.9854736328125\n",
      "Batch:\t5 /4000\t:  0.1650857925415039 \t1.038459300994873 \t695.5888061523438\n",
      "Batch:\t6 /4000\t:  0.18755888938903809 \t1.2284762859344482 \t739.6298217773438\n",
      "Batch:\t7 /4000\t:  0.16450977325439453 \t1.3947672843933105 \t670.4751586914062\n",
      "Batch:\t8 /4000\t:  0.15999794006347656 \t1.5569570064544678 \t640.6141967773438\n",
      "Batch:\t9 /4000\t:  0.15961933135986328 \t1.718336820602417 \t599.7169799804688\n",
      "Batch:\t10 /4000\t:  0.15983867645263672 \t1.8798060417175293 \t603.6930541992188\n",
      "Batch:\t11 /4000\t:  0.1600942611694336 \t2.0416831970214844 \t543.9678955078125\n",
      "Batch:\t12 /4000\t:  0.1611480712890625 \t2.204688549041748 \t559.2657470703125\n",
      "Batch:\t13 /4000\t:  0.16256403923034668 \t2.369826555252075 \t485.08734130859375\n",
      "Batch:\t14 /4000\t:  0.18144512176513672 \t2.5533955097198486 \t500.8731689453125\n",
      "Batch:\t15 /4000\t:  0.17451977729797363 \t2.7302844524383545 \t484.0772705078125\n",
      "Batch:\t16 /4000\t:  0.17917299270629883 \t2.9111168384552 \t474.31475830078125\n",
      "Batch:\t17 /4000\t:  0.11258840560913086 \t3.025555372238159 \t227.7908935546875\n",
      "MODEL WEIGHTS RIGHT NOW:  16401.59499359131\n",
      "model with accuracy  0.33 stored at data/models/core_chain/bilstm_dot_ulmfit/lcquad/93/model.torch\n",
      "in model save, no vectors were found.\n",
      "Time: 7.559242010116577\t Loss: 11416.976501464844\t Valdacc: 0.33\t Testacc: 0.305\n",
      " BestValidAcc: 0.33\n",
      " BestTestAcc: 0.305\n",
      "\n",
      "Epoch:  2 / 200\n",
      "Batch:\t0 /4000\t:  0.18634486198425293 \t0.18894624710083008 \t445.6165771484375\n",
      "Batch:\t1 /4000\t:  0.1647782325744629 \t0.3560020923614502 \t354.5161437988281\n",
      "Batch:\t2 /4000\t:  0.16448593139648438 \t0.5218918323516846 \t385.156982421875\n",
      "Batch:\t3 /4000\t:  0.16598796844482422 \t0.6896514892578125 \t364.872314453125\n",
      "Batch:\t4 /4000\t:  0.17044377326965332 \t0.8614964485168457 \t333.2517395019531\n",
      "Batch:\t5 /4000\t:  0.16293716430664062 \t1.0262670516967773 \t345.3009948730469\n",
      "Batch:\t6 /4000\t:  0.164567232131958 \t1.1922636032104492 \t337.8519592285156\n",
      "Batch:\t7 /4000\t:  0.18374347686767578 \t1.3784842491149902 \t336.22039794921875\n",
      "Batch:\t8 /4000\t:  0.1682288646697998 \t1.5480337142944336 \t287.2255554199219\n",
      "Batch:\t9 /4000\t:  0.17154669761657715 \t1.7211661338806152 \t283.1978759765625\n",
      "Batch:\t10 /4000\t:  0.1769733428955078 \t1.8995511531829834 \t279.8343505859375\n",
      "Batch:\t11 /4000\t:  0.16622138023376465 \t2.067063808441162 \t224.48863220214844\n",
      "Batch:\t12 /4000\t:  0.1611478328704834 \t2.2319586277008057 \t266.9981384277344\n",
      "Batch:\t13 /4000\t:  0.1627659797668457 \t2.3961925506591797 \t270.5043640136719\n",
      "Batch:\t14 /4000\t:  0.17993927001953125 \t2.5780951976776123 \t224.12950134277344\n",
      "Batch:\t15 /4000\t:  0.18028974533081055 \t2.7598671913146973 \t239.49594116210938\n",
      "Batch:\t16 /4000\t:  0.16421294212341309 \t2.9260847568511963 \t224.113037109375\n",
      "Batch:\t17 /4000\t:  0.11120295524597168 \t3.0391275882720947 \t110.7588119506836\n",
      "MODEL WEIGHTS RIGHT NOW:  16527.22186279297\n",
      "model with accuracy  0.37 stored at data/models/core_chain/bilstm_dot_ulmfit/lcquad/93/model.torch\n",
      "in model save, no vectors were found.\n",
      "Time: 7.561246871948242\t Loss: 5313.533317565918\t Valdacc: 0.37\t Testacc: 0.35\n",
      " BestValidAcc: 0.37\n",
      " BestTestAcc: 0.35\n",
      "\n",
      "Epoch:  3 / 200\n",
      "Batch:\t0 /4000\t:  0.1852860450744629 \t0.18822693824768066 \t196.18707275390625\n",
      "Batch:\t1 /4000\t:  0.18120217323303223 \t0.3789329528808594 \t216.3721160888672\n",
      "Batch:\t2 /4000\t:  0.16255426406860352 \t0.542884349822998 \t201.52133178710938\n",
      "Batch:\t3 /4000\t:  0.18299388885498047 \t0.7278928756713867 \t198.23387145996094\n",
      "Batch:\t4 /4000\t:  0.1633620262145996 \t0.8937828540802002 \t171.65672302246094\n",
      "Batch:\t5 /4000\t:  0.16686654090881348 \t1.0629761219024658 \t170.42587280273438\n",
      "Batch:\t6 /4000\t:  0.18800568580627441 \t1.252990484237671 \t175.28463745117188\n",
      "Batch:\t7 /4000\t:  0.16771268844604492 \t1.4222803115844727 \t162.217529296875\n",
      "Batch:\t8 /4000\t:  0.16689491271972656 \t1.5909464359283447 \t138.24676513671875\n",
      "Batch:\t9 /4000\t:  0.1670541763305664 \t1.759504795074463 \t146.09634399414062\n",
      "Batch:\t10 /4000\t:  0.16607975959777832 \t1.9269671440124512 \t151.1893310546875\n",
      "Batch:\t11 /4000\t:  0.1667804718017578 \t2.0956151485443115 \t147.45376586914062\n",
      "Batch:\t12 /4000\t:  0.16080474853515625 \t2.2586123943328857 \t134.78643798828125\n",
      "Batch:\t13 /4000\t:  0.16396617889404297 \t2.4244635105133057 \t138.12481689453125\n",
      "Batch:\t14 /4000\t:  0.1633772850036621 \t2.5895957946777344 \t139.92120361328125\n",
      "Batch:\t15 /4000\t:  0.1864025592803955 \t2.7777626514434814 \t138.14967346191406\n",
      "Batch:\t16 /4000\t:  0.15990638732910156 \t2.9395852088928223 \t134.56494140625\n",
      "Batch:\t17 /4000\t:  0.1042640209197998 \t3.0459909439086914 \t64.57878875732422\n",
      "Time: 5.009634971618652\t Loss: 2825.0112228393555\t Valdacc: 0.36\t Testacc: 0.33\n",
      " BestValidAcc: 0.37\n",
      " BestTestAcc: 0.35\n",
      "\n",
      "Epoch:  4 / 200\n",
      "Batch:\t0 /4000\t:  0.17040467262268066 \t0.17148470878601074 \t111.0094985961914\n",
      "Batch:\t1 /4000\t:  0.1594388484954834 \t0.33255672454833984 \t106.88459777832031\n",
      "Batch:\t2 /4000\t:  0.16380047798156738 \t0.4983220100402832 \t104.8114013671875\n",
      "Batch:\t3 /4000\t:  0.1946711540222168 \t0.6957757472991943 \t114.84588623046875\n",
      "Batch:\t4 /4000\t:  0.17309331893920898 \t0.870354175567627 \t122.3988037109375\n",
      "Batch:\t5 /4000\t:  0.160888671875 \t1.0333540439605713 \t76.78102111816406\n",
      "Batch:\t6 /4000\t:  0.165360689163208 \t1.2004830837249756 \t77.99543762207031\n",
      "Batch:\t7 /4000\t:  0.16062164306640625 \t1.3627674579620361 \t90.00981140136719\n",
      "Batch:\t8 /4000\t:  0.16686296463012695 \t1.535585641860962 \t89.9073715209961\n",
      "Batch:\t9 /4000\t:  0.16010618209838867 \t1.6973915100097656 \t107.03863525390625\n",
      "Batch:\t10 /4000\t:  0.16069340705871582 \t1.8599181175231934 \t96.81657409667969\n",
      "Batch:\t11 /4000\t:  0.16634321212768555 \t2.028311014175415 \t73.89811706542969\n",
      "Batch:\t12 /4000\t:  0.16242218017578125 \t2.197136878967285 \t79.9829330444336\n",
      "Batch:\t13 /4000\t:  0.15973258018493652 \t2.3586108684539795 \t105.26091003417969\n",
      "Batch:\t14 /4000\t:  0.16397547721862793 \t2.5243608951568604 \t85.98202514648438\n",
      "Batch:\t15 /4000\t:  0.15997838973999023 \t2.686671733856201 \t81.16724395751953\n",
      "Batch:\t16 /4000\t:  0.16875076293945312 \t2.8570690155029297 \t62.48111343383789\n",
      "Batch:\t17 /4000\t:  0.13736581802368164 \t2.9958250522613525 \t34.164329528808594\n",
      "MODEL WEIGHTS RIGHT NOW:  16564.237617492676\n",
      "model with accuracy  0.38 stored at data/models/core_chain/bilstm_dot_ulmfit/lcquad/93/model.torch\n",
      "in model save, no vectors were found.\n",
      "Time: 7.6124162673950195\t Loss: 1621.4357109069824\t Valdacc: 0.38\t Testacc: 0.365\n",
      " BestValidAcc: 0.38\n",
      " BestTestAcc: 0.365\n",
      "\n",
      "Epoch:  5 / 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t0 /4000\t:  0.19789719581604004 \t0.2032303810119629 \t69.92471313476562\n",
      "Batch:\t1 /4000\t:  0.16103863716125488 \t0.36578798294067383 \t81.08175659179688\n",
      "Batch:\t2 /4000\t:  0.17596030235290527 \t0.5439639091491699 \t65.96009063720703\n",
      "Batch:\t3 /4000\t:  0.16574311256408691 \t0.7116081714630127 \t79.96665954589844\n",
      "Batch:\t4 /4000\t:  0.2276449203491211 \t0.9411015510559082 \t63.22256851196289\n",
      "Batch:\t5 /4000\t:  0.19546842575073242 \t1.1380248069763184 \t65.76836395263672\n",
      "Batch:\t6 /4000\t:  0.16994905471801758 \t1.3091816902160645 \t63.24906539916992\n",
      "Batch:\t7 /4000\t:  0.16528677940368652 \t1.4772021770477295 \t68.17379760742188\n",
      "Batch:\t8 /4000\t:  0.16509079933166504 \t1.6481328010559082 \t73.97216033935547\n",
      "Batch:\t9 /4000\t:  0.18063998222351074 \t1.830613374710083 \t64.40385437011719\n",
      "Batch:\t10 /4000\t:  0.17105364799499512 \t2.0036957263946533 \t59.83440399169922\n",
      "Batch:\t11 /4000\t:  0.16057252883911133 \t2.165783643722534 \t56.92182922363281\n",
      "Batch:\t12 /4000\t:  0.1600499153137207 \t2.327173948287964 \t61.52204132080078\n",
      "Batch:\t13 /4000\t:  0.1749556064605713 \t2.503805160522461 \t52.1954345703125\n",
      "Batch:\t14 /4000\t:  0.1682431697845459 \t2.6736419200897217 \t62.17387008666992\n",
      "Batch:\t15 /4000\t:  0.16845035552978516 \t2.843515396118164 \t58.83683776855469\n",
      "Batch:\t16 /4000\t:  0.16933441162109375 \t3.014080286026001 \t64.83303833007812\n",
      "Batch:\t17 /4000\t:  0.10492944717407227 \t3.1204946041107178 \t22.39632797241211\n",
      "MODEL WEIGHTS RIGHT NOW:  16557.334032058716\n",
      "model with accuracy  0.4 stored at data/models/core_chain/bilstm_dot_ulmfit/lcquad/93/model.torch\n",
      "in model save, no vectors were found.\n",
      "Time: 7.624034404754639\t Loss: 1134.4368133544922\t Valdacc: 0.4\t Testacc: 0.375\n",
      " BestValidAcc: 0.4\n",
      " BestTestAcc: 0.375\n",
      "\n",
      "Epoch:  6 / 200\n",
      "Batch:\t0 /4000\t:  0.19192218780517578 \t0.19442129135131836 \t55.89021301269531\n",
      "Batch:\t1 /4000\t:  0.16353201866149902 \t0.36014366149902344 \t48.18987274169922\n",
      "Batch:\t2 /4000\t:  0.16302132606506348 \t0.5244498252868652 \t53.22035598754883\n",
      "Batch:\t3 /4000\t:  0.16524004936218262 \t0.6910226345062256 \t49.45424270629883\n",
      "Batch:\t4 /4000\t:  0.17448854446411133 \t0.8668665885925293 \t53.11500549316406\n",
      "Batch:\t5 /4000\t:  0.16250038146972656 \t1.0306084156036377 \t51.053836822509766\n",
      "Batch:\t6 /4000\t:  0.1603846549987793 \t1.192925214767456 \t55.08033752441406\n",
      "Batch:\t7 /4000\t:  0.16688227653503418 \t1.361846923828125 \t48.55628204345703\n",
      "Batch:\t8 /4000\t:  0.16249871253967285 \t1.525709629058838 \t60.837623596191406\n",
      "Batch:\t9 /4000\t:  0.16437196731567383 \t1.6918199062347412 \t59.509178161621094\n",
      "Batch:\t10 /4000\t:  0.1780226230621338 \t1.8767781257629395 \t45.80859375\n",
      "Batch:\t11 /4000\t:  0.16689848899841309 \t2.0454368591308594 \t60.190704345703125\n",
      "Batch:\t12 /4000\t:  0.17055916786193848 \t2.2178661823272705 \t37.91810607910156\n",
      "Batch:\t13 /4000\t:  0.16428112983703613 \t2.383708953857422 \t51.62488555908203\n",
      "Batch:\t14 /4000\t:  0.16688776016235352 \t2.552391767501831 \t43.522422790527344\n",
      "Batch:\t15 /4000\t:  0.19478535652160645 \t2.7489781379699707 \t32.805233001708984\n",
      "Batch:\t16 /4000\t:  0.16626644134521484 \t2.9166111946105957 \t45.93678283691406\n",
      "Batch:\t17 /4000\t:  0.10454893112182617 \t3.022655725479126 \t27.762407302856445\n",
      "Time: 4.933002710342407\t Loss: 880.4760837554932\t Valdacc: 0.39\t Testacc: 0.34\n",
      " BestValidAcc: 0.4\n",
      " BestTestAcc: 0.375\n",
      "\n",
      "Epoch:  7 / 200\n",
      "Batch:\t0 /4000\t:  0.1600344181060791 \t0.16294169425964355 \t48.1506233215332\n",
      "Batch:\t1 /4000\t:  0.16342830657958984 \t0.3279259204864502 \t46.75122833251953\n",
      "Batch:\t2 /4000\t:  0.1628110408782959 \t0.49227309226989746 \t47.875770568847656\n",
      "Batch:\t3 /4000\t:  0.1862499713897705 \t0.6854619979858398 \t39.76355743408203\n",
      "Batch:\t4 /4000\t:  0.1643083095550537 \t0.8512253761291504 \t32.23042297363281\n",
      "Batch:\t5 /4000\t:  0.17005443572998047 \t1.0229003429412842 \t35.00236511230469\n",
      "Batch:\t6 /4000\t:  0.17580437660217285 \t1.2009100914001465 \t30.279006958007812\n",
      "Batch:\t7 /4000\t:  0.16144251823425293 \t1.3639042377471924 \t33.267372131347656\n",
      "Batch:\t8 /4000\t:  0.16639161109924316 \t1.5322327613830566 \t38.48078918457031\n",
      "Batch:\t9 /4000\t:  0.17189264297485352 \t1.7058568000793457 \t34.52275466918945\n",
      "Batch:\t10 /4000\t:  0.16405320167541504 \t1.8715624809265137 \t18.721662521362305\n",
      "Batch:\t11 /4000\t:  0.16428017616271973 \t2.0376694202423096 \t40.49750518798828\n",
      "Batch:\t12 /4000\t:  0.16936016082763672 \t2.2085392475128174 \t30.446453094482422\n",
      "Batch:\t13 /4000\t:  0.1641216278076172 \t2.374556064605713 \t31.88739013671875\n",
      "Batch:\t14 /4000\t:  0.19612455368041992 \t2.5725221633911133 \t31.949459075927734\n",
      "Batch:\t15 /4000\t:  0.1604290008544922 \t2.7345054149627686 \t31.142534255981445\n",
      "Batch:\t16 /4000\t:  0.1665487289428711 \t2.9027998447418213 \t39.02284240722656\n",
      "Batch:\t17 /4000\t:  0.11307930946350098 \t3.018538236618042 \t10.87932300567627\n",
      "Time: 4.950748443603516\t Loss: 620.8710603713989\t Valdacc: 0.4\t Testacc: 0.38\n",
      " BestValidAcc: 0.4\n",
      " BestTestAcc: 0.38\n",
      "\n",
      "Epoch:  8 / 200\n",
      "Batch:\t0 /4000\t:  0.16590046882629395 \t0.16808295249938965 \t42.963626861572266\n",
      "Batch:\t1 /4000\t:  0.17565584182739258 \t0.3451879024505615 \t34.2907829284668\n",
      "Batch:\t2 /4000\t:  0.21673846244812012 \t0.5667374134063721 \t43.88273620605469\n",
      "Batch:\t3 /4000\t:  0.16020822525024414 \t0.728691577911377 \t23.545635223388672\n",
      "Batch:\t4 /4000\t:  0.16377615928649902 \t0.8946037292480469 \t42.13214874267578\n",
      "Batch:\t5 /4000\t:  0.17011451721191406 \t1.066359043121338 \t40.85309982299805\n",
      "Batch:\t6 /4000\t:  0.16059470176696777 \t1.2282497882843018 \t28.91575050354004\n",
      "Batch:\t7 /4000\t:  0.1820385456085205 \t1.4117376804351807 \t26.192527770996094\n",
      "Batch:\t8 /4000\t:  0.1660141944885254 \t1.5791289806365967 \t30.658052444458008\n",
      "Batch:\t9 /4000\t:  0.16306328773498535 \t1.7438771724700928 \t42.98323440551758\n",
      "Batch:\t10 /4000\t:  0.19011139869689941 \t1.9376225471496582 \t23.366106033325195\n",
      "Batch:\t11 /4000\t:  0.16262149810791016 \t2.101623773574829 \t20.620994567871094\n",
      "Batch:\t12 /4000\t:  0.20401978492736816 \t2.3071463108062744 \t17.971900939941406\n",
      "Batch:\t13 /4000\t:  0.18089866638183594 \t2.491271734237671 \t28.736248016357422\n",
      "Batch:\t14 /4000\t:  0.16221237182617188 \t2.6548943519592285 \t22.733945846557617\n",
      "Batch:\t15 /4000\t:  0.16224169731140137 \t2.8185694217681885 \t25.728757858276367\n",
      "Batch:\t16 /4000\t:  0.20195651054382324 \t3.0222694873809814 \t27.58026885986328\n",
      "Batch:\t17 /4000\t:  0.10133218765258789 \t3.1253368854522705 \t16.02743911743164\n",
      "Time: 5.026971101760864\t Loss: 539.183256149292\t Valdacc: 0.4\t Testacc: 0.39\n",
      " BestValidAcc: 0.4\n",
      " BestTestAcc: 0.39\n",
      "\n",
      "Epoch:  9 / 200\n",
      "Batch:\t0 /4000\t:  0.1714024543762207 \t0.17393708229064941 \t31.724679946899414\n",
      "Batch:\t1 /4000\t:  0.16878128051757812 \t0.3441610336303711 \t38.57129669189453\n",
      "Batch:\t2 /4000\t:  0.17355108261108398 \t0.5195064544677734 \t22.310115814208984\n",
      "Batch:\t3 /4000\t:  0.16979432106018066 \t0.6914467811584473 \t34.19599151611328\n",
      "Batch:\t4 /4000\t:  0.1625690460205078 \t0.8554983139038086 \t22.807048797607422\n",
      "Batch:\t5 /4000\t:  0.22222590446472168 \t1.0799098014831543 \t19.558006286621094\n",
      "Batch:\t6 /4000\t:  0.18230628967285156 \t1.2641072273254395 \t21.2989444732666\n",
      "Batch:\t7 /4000\t:  0.16045689582824707 \t1.4258854389190674 \t36.79283905029297\n",
      "Batch:\t8 /4000\t:  0.16350746154785156 \t1.5908541679382324 \t29.216575622558594\n",
      "Batch:\t9 /4000\t:  0.18208694458007812 \t1.77439284324646 \t21.17830467224121\n",
      "Batch:\t10 /4000\t:  0.16368699073791504 \t1.9394726753234863 \t21.526573181152344\n",
      "Batch:\t11 /4000\t:  0.16707730293273926 \t2.1083500385284424 \t17.886892318725586\n",
      "Batch:\t12 /4000\t:  0.16669893264770508 \t2.2768659591674805 \t19.10936164855957\n",
      "Batch:\t13 /4000\t:  0.18073225021362305 \t2.459192991256714 \t20.82752799987793\n",
      "Batch:\t14 /4000\t:  0.16689133644104004 \t2.6275582313537598 \t25.214988708496094\n",
      "Batch:\t15 /4000\t:  0.16370463371276855 \t2.7928340435028076 \t15.66680908203125\n",
      "Batch:\t16 /4000\t:  0.16312932968139648 \t2.957923173904419 \t17.201030731201172\n",
      "Batch:\t17 /4000\t:  0.10555839538574219 \t3.065354824066162 \t8.892931938171387\n",
      "Time: 5.014250755310059\t Loss: 423.97991847991943\t Valdacc: 0.35\t Testacc: 0.38\n",
      " BestValidAcc: 0.4\n",
      " BestTestAcc: 0.39\n",
      "\n",
      "Epoch:  10 / 200\n",
      "Batch:\t0 /4000\t:  0.16309523582458496 \t0.16681933403015137 \t24.420711517333984\n",
      "Batch:\t1 /4000\t:  0.1647968292236328 \t0.33454394340515137 \t13.438520431518555\n",
      "Batch:\t2 /4000\t:  0.1642286777496338 \t0.5004260540008545 \t20.821399688720703\n",
      "Batch:\t3 /4000\t:  0.17987275123596191 \t0.6821603775024414 \t15.023843765258789\n",
      "Batch:\t4 /4000\t:  0.16071581840515137 \t0.8445577621459961 \t28.872722625732422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t5 /4000\t:  0.16511821746826172 \t1.011580467224121 \t23.806156158447266\n",
      "Batch:\t6 /4000\t:  0.1760697364807129 \t1.189446210861206 \t14.665584564208984\n",
      "Batch:\t7 /4000\t:  0.160691499710083 \t1.3515489101409912 \t23.411502838134766\n",
      "Batch:\t8 /4000\t:  0.16939425468444824 \t1.5225343704223633 \t13.823110580444336\n",
      "Batch:\t9 /4000\t:  0.17296481132507324 \t1.6969366073608398 \t20.623077392578125\n",
      "Batch:\t10 /4000\t:  0.16660356521606445 \t1.8652939796447754 \t28.173873901367188\n",
      "Batch:\t11 /4000\t:  0.16086435317993164 \t2.0276119709014893 \t14.32510757446289\n",
      "Batch:\t12 /4000\t:  0.16428446769714355 \t2.193411111831665 \t23.630111694335938\n",
      "Batch:\t13 /4000\t:  0.16173934936523438 \t2.357544183731079 \t20.1048526763916\n",
      "Batch:\t14 /4000\t:  0.16042566299438477 \t2.5196874141693115 \t17.579513549804688\n",
      "Batch:\t15 /4000\t:  0.18114614486694336 \t2.702454090118408 \t17.355131149291992\n",
      "Batch:\t16 /4000\t:  0.1635429859161377 \t2.867509126663208 \t20.11899185180664\n",
      "Batch:\t17 /4000\t:  0.10197591781616211 \t2.9707138538360596 \t4.065553188323975\n",
      "Time: 4.856733322143555\t Loss: 344.25976514816284\t Valdacc: 0.37\t Testacc: 0.38\n",
      " BestValidAcc: 0.4\n",
      " BestTestAcc: 0.39\n",
      "\n",
      "Epoch:  11 / 200\n",
      "Batch:\t0 /4000\t:  0.16307353973388672 \t0.16494059562683105 \t22.67508316040039\n",
      "Batch:\t1 /4000\t:  0.16114211082458496 \t0.3275566101074219 \t16.203441619873047\n",
      "Batch:\t2 /4000\t:  0.18302202224731445 \t0.5122742652893066 \t11.698088645935059\n",
      "Batch:\t3 /4000\t:  0.16270136833190918 \t0.6769094467163086 \t15.093229293823242\n",
      "Batch:\t4 /4000\t:  0.16695094108581543 \t0.8457372188568115 \t19.19140625\n",
      "Batch:\t5 /4000\t:  0.1624763011932373 \t1.0101161003112793 \t26.52421760559082\n",
      "Batch:\t6 /4000\t:  0.1823139190673828 \t1.1955082416534424 \t18.020748138427734\n",
      "Batch:\t7 /4000\t:  0.1637110710144043 \t1.3609790802001953 \t21.75457763671875\n",
      "Batch:\t8 /4000\t:  0.18156886100769043 \t1.5440304279327393 \t20.02285385131836\n",
      "Batch:\t9 /4000\t:  0.16072344779968262 \t1.706190586090088 \t16.0382080078125\n",
      "Batch:\t10 /4000\t:  0.16315841674804688 \t1.8711011409759521 \t20.238391876220703\n",
      "Batch:\t11 /4000\t:  0.16903138160705566 \t2.0415143966674805 \t22.336063385009766\n",
      "Batch:\t12 /4000\t:  0.16180753707885742 \t2.2051775455474854 \t20.098541259765625\n",
      "Batch:\t13 /4000\t:  0.16149353981018066 \t2.368084192276001 \t23.669912338256836\n",
      "Batch:\t14 /4000\t:  0.1838393211364746 \t2.553509473800659 \t15.789346694946289\n",
      "Batch:\t15 /4000\t:  0.1815645694732666 \t2.7366321086883545 \t21.925006866455078\n",
      "Batch:\t16 /4000\t:  0.16390347480773926 \t2.901949405670166 \t13.674337387084961\n",
      "Batch:\t17 /4000\t:  0.1048727035522461 \t3.0081357955932617 \t9.22822380065918\n",
      "Time: 5.11948561668396\t Loss: 334.18167781829834\t Valdacc: 0.4\t Testacc: 0.365\n",
      " BestValidAcc: 0.4\n",
      " BestTestAcc: 0.39\n",
      "\n",
      "Epoch:  12 / 200\n",
      "Batch:\t0 /4000\t:  0.16361308097839355 \t0.1687784194946289 \t18.664752960205078\n",
      "Batch:\t1 /4000\t:  0.16362214088439941 \t0.33374524116516113 \t10.486871719360352\n",
      "Batch:\t2 /4000\t:  0.17612099647521973 \t0.5116109848022461 \t17.143085479736328\n",
      "Batch:\t3 /4000\t:  0.18325424194335938 \t0.6969723701477051 \t16.494606018066406\n",
      "Batch:\t4 /4000\t:  0.16489005088806152 \t0.8633935451507568 \t8.816922187805176\n",
      "Batch:\t5 /4000\t:  0.20788192749023438 \t1.0729255676269531 \t15.358455657958984\n",
      "Batch:\t6 /4000\t:  0.16015267372131348 \t1.2345867156982422 \t14.981928825378418\n",
      "Batch:\t7 /4000\t:  0.16376423835754395 \t1.399918794631958 \t14.083856582641602\n",
      "Batch:\t8 /4000\t:  0.16934919357299805 \t1.5709397792816162 \t19.2564697265625\n",
      "Batch:\t9 /4000\t:  0.16300392150878906 \t1.7355930805206299 \t15.0246000289917\n",
      "Batch:\t10 /4000\t:  0.16645312309265137 \t1.903531551361084 \t23.1118221282959\n",
      "Batch:\t11 /4000\t:  0.16142988204956055 \t2.067739725112915 \t16.206256866455078\n",
      "Batch:\t12 /4000\t:  0.1818397045135498 \t2.2510335445404053 \t21.433107376098633\n",
      "Batch:\t13 /4000\t:  0.16121745109558105 \t2.4138641357421875 \t26.173030853271484\n",
      "Batch:\t14 /4000\t:  0.20611262321472168 \t2.621885299682617 \t12.9660005569458\n",
      "Batch:\t15 /4000\t:  0.16842007637023926 \t2.791635513305664 \t16.9298152923584\n",
      "Batch:\t16 /4000\t:  0.16933655738830566 \t2.9628846645355225 \t14.158407211303711\n",
      "Batch:\t17 /4000\t:  0.12435650825500488 \t3.088059902191162 \t4.197905540466309\n",
      "Time: 5.138404607772827\t Loss: 285.48789501190186\t Valdacc: 0.4\t Testacc: 0.395\n",
      " BestValidAcc: 0.4\n",
      " BestTestAcc: 0.395\n",
      "\n",
      "Epoch:  13 / 200\n",
      "Batch:\t0 /4000\t:  0.161360502243042 \t0.1632251739501953 \t13.900697708129883\n",
      "Batch:\t1 /4000\t:  0.16307401657104492 \t0.3279697895050049 \t8.5966796875\n",
      "Batch:\t2 /4000\t:  0.16493844985961914 \t0.4944651126861572 \t18.74330711364746\n",
      "Batch:\t3 /4000\t:  0.16188955307006836 \t0.6578271389007568 \t23.982933044433594\n",
      "Batch:\t4 /4000\t:  0.16170763969421387 \t0.8212170600891113 \t23.8265380859375\n",
      "Batch:\t5 /4000\t:  0.16983795166015625 \t0.9927606582641602 \t8.102409362792969\n",
      "Batch:\t6 /4000\t:  0.16583514213562012 \t1.1600236892700195 \t7.876030921936035\n",
      "Batch:\t7 /4000\t:  0.16650176048278809 \t1.3279521465301514 \t8.083183288574219\n",
      "Batch:\t8 /4000\t:  0.16408634185791016 \t1.4936845302581787 \t8.476184844970703\n",
      "Batch:\t9 /4000\t:  0.16093111038208008 \t1.6562151908874512 \t15.791719436645508\n",
      "Batch:\t10 /4000\t:  0.16084027290344238 \t1.8186163902282715 \t22.32630729675293\n",
      "Batch:\t11 /4000\t:  0.16403675079345703 \t1.9843418598175049 \t8.881359100341797\n",
      "Batch:\t12 /4000\t:  0.1673729419708252 \t2.1541173458099365 \t19.90019989013672\n",
      "Batch:\t13 /4000\t:  0.1601400375366211 \t2.315786838531494 \t16.06757926940918\n",
      "Batch:\t14 /4000\t:  0.16590261459350586 \t2.4832231998443604 \t8.896180152893066\n",
      "Batch:\t15 /4000\t:  0.19550681114196777 \t2.6805076599121094 \t12.66364860534668\n",
      "Batch:\t16 /4000\t:  0.17801141738891602 \t2.8602511882781982 \t10.047725677490234\n",
      "Batch:\t17 /4000\t:  0.10279679298400879 \t2.9644477367401123 \t7.299026012420654\n",
      "Time: 4.9163818359375\t Loss: 243.46170949935913\t Valdacc: 0.36\t Testacc: 0.355\n",
      " BestValidAcc: 0.4\n",
      " BestTestAcc: 0.395\n",
      "\n",
      "Epoch:  14 / 200\n",
      "Batch:\t0 /4000\t:  0.16475367546081543 \t0.16670727729797363 \t11.18588638305664\n",
      "Batch:\t1 /4000\t:  0.18671751022338867 \t0.3555271625518799 \t8.945418357849121\n",
      "Batch:\t2 /4000\t:  0.18114900588989258 \t0.5428140163421631 \t11.104728698730469\n",
      "Batch:\t3 /4000\t:  0.1743178367614746 \t0.7186453342437744 \t15.371551513671875\n",
      "Batch:\t4 /4000\t:  0.18608522415161133 \t0.9071614742279053 \t9.717966079711914\n",
      "Batch:\t5 /4000\t:  0.16452765464782715 \t1.0734076499938965 \t19.884201049804688\n",
      "Batch:\t6 /4000\t:  0.16297054290771484 \t1.238203525543213 \t5.937953472137451\n",
      "Batch:\t7 /4000\t:  0.19569039344787598 \t1.4361145496368408 \t14.966336250305176\n",
      "Batch:\t8 /4000\t:  0.16272544860839844 \t1.6004316806793213 \t7.457729339599609\n",
      "Batch:\t9 /4000\t:  0.16710400581359863 \t1.7691667079925537 \t20.984560012817383\n",
      "Batch:\t10 /4000\t:  0.18137788772583008 \t1.9523487091064453 \t12.772749900817871\n",
      "Batch:\t11 /4000\t:  0.16359949111938477 \t2.117349863052368 \t18.14275360107422\n",
      "Batch:\t12 /4000\t:  0.17023968696594238 \t2.2893941402435303 \t15.430126190185547\n",
      "Batch:\t13 /4000\t:  0.16149353981018066 \t2.452603578567505 \t12.05681324005127\n",
      "Batch:\t14 /4000\t:  0.1667037010192871 \t2.6213631629943848 \t7.238253116607666\n",
      "Batch:\t15 /4000\t:  0.16545414924621582 \t2.788783550262451 \t21.382448196411133\n",
      "Batch:\t16 /4000\t:  0.16785359382629395 \t2.958220958709717 \t4.242195129394531\n",
      "Batch:\t17 /4000\t:  0.10290765762329102 \t3.062885284423828 \t4.985640525817871\n",
      "MODEL WEIGHTS RIGHT NOW:  16477.066811561584\n",
      "model with accuracy  0.42 stored at data/models/core_chain/bilstm_dot_ulmfit/lcquad/93/model.torch\n",
      "in model save, no vectors were found.\n",
      "Time: 7.82021689414978\t Loss: 221.80731105804443\t Valdacc: 0.42\t Testacc: 0.4\n",
      " BestValidAcc: 0.42\n",
      " BestTestAcc: 0.4\n",
      "\n",
      "Epoch:  15 / 200\n",
      "Batch:\t0 /4000\t:  0.1839127540588379 \t0.18506455421447754 \t7.472339153289795\n",
      "Batch:\t1 /4000\t:  0.1903223991394043 \t0.37729334831237793 \t14.271238327026367\n",
      "Batch:\t2 /4000\t:  0.16970300674438477 \t0.5491023063659668 \t12.898565292358398\n",
      "Batch:\t3 /4000\t:  0.16714143753051758 \t0.717876672744751 \t9.80941390991211\n",
      "Batch:\t4 /4000\t:  0.1727585792541504 \t0.8923969268798828 \t10.210481643676758\n",
      "Batch:\t5 /4000\t:  0.16500186920166016 \t1.0595381259918213 \t11.658594131469727\n",
      "Batch:\t6 /4000\t:  0.16832637786865234 \t1.2300217151641846 \t12.799333572387695\n",
      "Batch:\t7 /4000\t:  0.16100573539733887 \t1.3925800323486328 \t13.513513565063477\n",
      "Batch:\t8 /4000\t:  0.16395902633666992 \t1.5580272674560547 \t9.041293144226074\n",
      "Batch:\t9 /4000\t:  0.16936445236206055 \t1.7289073467254639 \t7.4156270027160645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t10 /4000\t:  0.16441059112548828 \t1.8950021266937256 \t12.156009674072266\n",
      "Batch:\t11 /4000\t:  0.16631460189819336 \t2.0631721019744873 \t5.154423713684082\n",
      "Batch:\t12 /4000\t:  0.20415639877319336 \t2.2688052654266357 \t20.897302627563477\n",
      "Batch:\t13 /4000\t:  0.1696305274963379 \t2.4403867721557617 \t9.936735153198242\n",
      "Batch:\t14 /4000\t:  0.17226529121398926 \t2.614137649536133 \t10.202583312988281\n",
      "Batch:\t15 /4000\t:  0.17196440696716309 \t2.7889084815979004 \t15.168107986450195\n",
      "Batch:\t16 /4000\t:  0.16438579559326172 \t2.9554781913757324 \t9.984480857849121\n",
      "Batch:\t17 /4000\t:  0.10337448120117188 \t3.060384750366211 \t9.142034530639648\n",
      "MODEL WEIGHTS RIGHT NOW:  16475.774765491486\n",
      "model with accuracy  0.43 stored at data/models/core_chain/bilstm_dot_ulmfit/lcquad/93/model.torch\n",
      "in model save, no vectors were found.\n",
      "Time: 7.755717039108276\t Loss: 201.73207759857178\t Valdacc: 0.43\t Testacc: 0.405\n",
      " BestValidAcc: 0.43\n",
      " BestTestAcc: 0.405\n",
      "\n",
      "Epoch:  16 / 200\n",
      "Batch:\t0 /4000\t:  0.16494536399841309 \t0.16680598258972168 \t8.334527969360352\n",
      "Batch:\t1 /4000\t:  0.16864728927612305 \t0.3377838134765625 \t6.840705871582031\n",
      "Batch:\t2 /4000\t:  0.17129921913146973 \t0.5105400085449219 \t11.242711067199707\n",
      "Batch:\t3 /4000\t:  0.16182398796081543 \t0.6738424301147461 \t7.098784923553467\n",
      "Batch:\t4 /4000\t:  0.1609947681427002 \t0.8362679481506348 \t10.261970520019531\n",
      "Batch:\t5 /4000\t:  0.16146636009216309 \t0.9995348453521729 \t7.250482082366943\n",
      "Batch:\t6 /4000\t:  0.16130423545837402 \t1.1622235774993896 \t14.111505508422852\n",
      "Batch:\t7 /4000\t:  0.21048212051391602 \t1.3742647171020508 \t8.288846969604492\n",
      "Batch:\t8 /4000\t:  0.16457676887512207 \t1.540498971939087 \t12.540749549865723\n",
      "Batch:\t9 /4000\t:  0.16768121719360352 \t1.7096238136291504 \t10.511648178100586\n",
      "Batch:\t10 /4000\t:  0.16224932670593262 \t1.8733716011047363 \t3.8260269165039062\n",
      "Batch:\t11 /4000\t:  0.16118311882019043 \t2.0362672805786133 \t10.506209373474121\n",
      "Batch:\t12 /4000\t:  0.17344164848327637 \t2.2115044593811035 \t5.855660438537598\n",
      "Batch:\t13 /4000\t:  0.16808128356933594 \t2.3817107677459717 \t12.670510292053223\n",
      "Batch:\t14 /4000\t:  0.16506361961364746 \t2.5488362312316895 \t11.649463653564453\n",
      "Batch:\t15 /4000\t:  0.1652531623840332 \t2.7158713340759277 \t10.368896484375\n",
      "Batch:\t16 /4000\t:  0.166123628616333 \t2.88387131690979 \t9.996929168701172\n",
      "Batch:\t17 /4000\t:  0.10235881805419922 \t2.9874424934387207 \t9.167155265808105\n",
      "MODEL WEIGHTS RIGHT NOW:  16472.935344696045\n",
      "model with accuracy  0.44 stored at data/models/core_chain/bilstm_dot_ulmfit/lcquad/93/model.torch\n",
      "in model save, no vectors were found.\n",
      "Time: 7.7427709102630615\t Loss: 170.52278423309326\t Valdacc: 0.44\t Testacc: 0.38\n",
      " BestValidAcc: 0.44\n",
      " BestTestAcc: 0.405\n",
      "\n",
      "Epoch:  17 / 200\n",
      "Batch:\t0 /4000\t:  0.20899009704589844 \t0.21013188362121582 \t12.589159965515137\n",
      "Batch:\t1 /4000\t:  0.1610414981842041 \t0.37302327156066895 \t3.567415237426758\n",
      "Batch:\t2 /4000\t:  0.17029595375061035 \t0.5447385311126709 \t4.164064407348633\n",
      "Batch:\t3 /4000\t:  0.17600059509277344 \t0.7224125862121582 \t13.856460571289062\n",
      "Batch:\t4 /4000\t:  0.1902318000793457 \t0.9147498607635498 \t9.11393928527832\n",
      "Batch:\t5 /4000\t:  0.16420364379882812 \t1.0806772708892822 \t4.541349411010742\n",
      "Batch:\t6 /4000\t:  0.16612029075622559 \t1.2485544681549072 \t11.074403762817383\n",
      "Batch:\t7 /4000\t:  0.16492247581481934 \t1.4152956008911133 \t4.6269121170043945\n",
      "Batch:\t8 /4000\t:  0.16165661811828613 \t1.5785293579101562 \t4.351324558258057\n",
      "Batch:\t9 /4000\t:  0.16782879829406738 \t1.748147964477539 \t6.525567054748535\n",
      "Batch:\t10 /4000\t:  0.16213536262512207 \t1.912069320678711 \t8.830526351928711\n",
      "Batch:\t11 /4000\t:  0.1625227928161621 \t2.08254337310791 \t8.071113586425781\n",
      "Batch:\t12 /4000\t:  0.16657447814941406 \t2.2512147426605225 \t11.41534423828125\n",
      "Batch:\t13 /4000\t:  0.17664241790771484 \t2.434786319732666 \t9.662954330444336\n",
      "Batch:\t14 /4000\t:  0.16709518432617188 \t2.603231430053711 \t7.958561420440674\n",
      "Batch:\t15 /4000\t:  0.19385194778442383 \t2.798823356628418 \t3.825979709625244\n",
      "Batch:\t16 /4000\t:  0.16417455673217773 \t2.964733123779297 \t7.443856239318848\n",
      "Batch:\t17 /4000\t:  0.10229849815368652 \t3.0683534145355225 \t4.879031181335449\n",
      "Time: 5.0091118812561035\t Loss: 136.49796342849731\t Valdacc: 0.43\t Testacc: 0.38\n",
      " BestValidAcc: 0.44\n",
      " BestTestAcc: 0.405\n",
      "\n",
      "Epoch:  18 / 200\n",
      "Batch:\t0 /4000\t:  0.16483664512634277 \t0.1658339500427246 \t4.669532775878906\n",
      "Batch:\t1 /4000\t:  0.1639418601989746 \t0.33146214485168457 \t8.571197509765625\n",
      "Batch:\t2 /4000\t:  0.1668698787689209 \t0.5006828308105469 \t7.459970951080322\n",
      "Batch:\t3 /4000\t:  0.17911720275878906 \t0.6813786029815674 \t7.071041584014893\n",
      "Batch:\t4 /4000\t:  0.16064834594726562 \t0.8438079357147217 \t13.801332473754883\n",
      "Batch:\t5 /4000\t:  0.1842026710510254 \t1.0294578075408936 \t4.595035076141357\n",
      "Batch:\t6 /4000\t:  0.18624663352966309 \t1.233018398284912 \t8.863370895385742\n",
      "Batch:\t7 /4000\t:  0.1667628288269043 \t1.4012372493743896 \t5.335773468017578\n",
      "Batch:\t8 /4000\t:  0.16070914268493652 \t1.563704490661621 \t4.911118507385254\n",
      "Batch:\t9 /4000\t:  0.17303967475891113 \t1.7386319637298584 \t17.093286514282227\n",
      "Batch:\t10 /4000\t:  0.1672961711883545 \t1.9077138900756836 \t18.832332611083984\n",
      "Batch:\t11 /4000\t:  0.16821694374084473 \t2.077465534210205 \t10.597984313964844\n",
      "Batch:\t12 /4000\t:  0.16414523124694824 \t2.242910146713257 \t6.220095157623291\n",
      "Batch:\t13 /4000\t:  0.1681368350982666 \t2.4129631519317627 \t12.873926162719727\n",
      "Batch:\t14 /4000\t:  0.19046878814697266 \t2.604904890060425 \t9.99757194519043\n",
      "Batch:\t15 /4000\t:  0.16389775276184082 \t2.7707409858703613 \t8.655218124389648\n",
      "Batch:\t16 /4000\t:  0.16742944717407227 \t2.9395599365234375 \t5.545103073120117\n",
      "Batch:\t17 /4000\t:  0.10190606117248535 \t3.0428361892700195 \t2.540637969970703\n",
      "Time: 5.0177600383758545\t Loss: 157.63452911376953\t Valdacc: 0.4\t Testacc: 0.405\n",
      " BestValidAcc: 0.44\n",
      " BestTestAcc: 0.405\n",
      "\n",
      "Epoch:  19 / 200\n",
      "Batch:\t0 /4000\t:  0.18054986000061035 \t0.18336892127990723 \t9.971595764160156\n",
      "Batch:\t1 /4000\t:  0.16796398162841797 \t0.35294342041015625 \t5.587039947509766\n",
      "Batch:\t2 /4000\t:  0.16107726097106934 \t0.5160868167877197 \t9.125951766967773\n",
      "Batch:\t3 /4000\t:  0.18636107444763184 \t0.7039430141448975 \t7.4793195724487305\n",
      "Batch:\t4 /4000\t:  0.1694340705871582 \t0.8748276233673096 \t3.0890846252441406\n",
      "Batch:\t5 /4000\t:  0.1647176742553711 \t1.0422403812408447 \t7.851350784301758\n",
      "Batch:\t6 /4000\t:  0.2034919261932373 \t1.24778413772583 \t16.2351131439209\n",
      "Batch:\t7 /4000\t:  0.16074109077453613 \t1.4098107814788818 \t11.286041259765625\n",
      "Batch:\t8 /4000\t:  0.16631364822387695 \t1.5774908065795898 \t12.288570404052734\n",
      "Batch:\t9 /4000\t:  0.16629409790039062 \t1.7451424598693848 \t12.812744140625\n",
      "Batch:\t10 /4000\t:  0.16677045822143555 \t1.9148988723754883 \t5.645362854003906\n",
      "Batch:\t11 /4000\t:  0.18237590789794922 \t2.1010842323303223 \t3.1783952713012695\n",
      "Batch:\t12 /4000\t:  0.16742205619812012 \t2.269737958908081 \t7.612709045410156\n",
      "Batch:\t13 /4000\t:  0.16634583473205566 \t2.4373035430908203 \t8.30937671661377\n",
      "Batch:\t14 /4000\t:  0.16060638427734375 \t2.5992941856384277 \t2.8117141723632812\n",
      "Batch:\t15 /4000\t:  0.16292262077331543 \t2.764173984527588 \t4.334718227386475\n",
      "Batch:\t16 /4000\t:  0.17320895195007324 \t2.939023494720459 \t3.162423610687256\n",
      "Batch:\t17 /4000\t:  0.1228017807006836 \t3.0659115314483643 \t1.5263128280639648\n",
      "Time: 4.978410243988037\t Loss: 132.30782413482666\t Valdacc: 0.42\t Testacc: 0.36\n",
      " BestValidAcc: 0.44\n",
      " BestTestAcc: 0.405\n",
      "\n",
      "Epoch:  20 / 200\n",
      "Batch:\t0 /4000\t:  0.20665574073791504 \t0.20908880233764648 \t6.859447956085205\n",
      "Batch:\t1 /4000\t:  0.17898011207580566 \t0.3899102210998535 \t10.030898094177246\n",
      "Batch:\t2 /4000\t:  0.16872859001159668 \t0.5602421760559082 \t16.746597290039062\n",
      "Batch:\t3 /4000\t:  0.20606160163879395 \t0.771367073059082 \t5.206254482269287\n",
      "Batch:\t4 /4000\t:  0.18240118026733398 \t0.9602036476135254 \t8.39688777923584\n",
      "Batch:\t5 /4000\t:  0.16038990020751953 \t1.1220335960388184 \t2.24664306640625\n",
      "Batch:\t6 /4000\t:  0.1671922206878662 \t1.29075026512146 \t10.251435279846191\n",
      "Batch:\t7 /4000\t:  0.1707160472869873 \t1.466463565826416 \t13.143430709838867\n",
      "Batch:\t8 /4000\t:  0.1658010482788086 \t1.6332283020019531 \t4.525114059448242\n",
      "Batch:\t9 /4000\t:  0.16885852813720703 \t1.8037209510803223 \t9.220507621765137\n",
      "Batch:\t10 /4000\t:  0.16069316864013672 \t1.965801477432251 \t8.270439147949219\n",
      "Batch:\t11 /4000\t:  0.2049722671508789 \t2.172212839126587 \t10.867949485778809\n",
      "Batch:\t12 /4000\t:  0.19818806648254395 \t2.3815224170684814 \t7.396953582763672\n",
      "Batch:\t13 /4000\t:  0.16286182403564453 \t2.5456018447875977 \t4.223532676696777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t14 /4000\t:  0.16620993614196777 \t2.713484048843384 \t7.629835605621338\n",
      "Batch:\t15 /4000\t:  0.16603922843933105 \t2.8812198638916016 \t10.299169540405273\n",
      "Batch:\t16 /4000\t:  0.16015625 \t3.0426347255706787 \t7.771888732910156\n",
      "Batch:\t17 /4000\t:  0.11155581474304199 \t3.155716896057129 \t0.8810024261474609\n",
      "Time: 5.128629684448242\t Loss: 143.96798753738403\t Valdacc: 0.43\t Testacc: 0.395\n",
      " BestValidAcc: 0.44\n",
      " BestTestAcc: 0.405\n",
      "\n",
      "Epoch:  21 / 200\n",
      "Batch:\t0 /4000\t:  0.16938161849975586 \t0.17028450965881348 \t8.667390823364258\n",
      "Batch:\t1 /4000\t:  0.16019129753112793 \t0.33192014694213867 \t6.676827430725098\n",
      "Batch:\t2 /4000\t:  0.1670517921447754 \t0.5005307197570801 \t10.622867584228516\n",
      "Batch:\t3 /4000\t:  0.20328235626220703 \t0.7052996158599854 \t3.4478416442871094\n",
      "Batch:\t4 /4000\t:  0.16175222396850586 \t0.8684210777282715 \t9.62797737121582\n",
      "Batch:\t5 /4000\t:  0.16017651557922363 \t1.0302271842956543 \t2.7255420684814453\n",
      "Batch:\t6 /4000\t:  0.16608881950378418 \t1.1979703903198242 \t4.487421035766602\n",
      "Batch:\t7 /4000\t:  0.16148829460144043 \t1.3610360622406006 \t11.024428367614746\n",
      "Batch:\t8 /4000\t:  0.1812279224395752 \t1.5436549186706543 \t5.190826892852783\n",
      "Batch:\t9 /4000\t:  0.16059494018554688 \t1.7054457664489746 \t4.928551197052002\n",
      "Batch:\t10 /4000\t:  0.16777634620666504 \t1.8745172023773193 \t5.774029731750488\n",
      "Batch:\t11 /4000\t:  0.17844223976135254 \t2.0548505783081055 \t3.2961931228637695\n",
      "Batch:\t12 /4000\t:  0.16270995140075684 \t2.2189197540283203 \t5.9242377281188965\n",
      "Batch:\t13 /4000\t:  0.16781830787658691 \t2.3883564472198486 \t15.02699089050293\n",
      "Batch:\t14 /4000\t:  0.16975975036621094 \t2.5598080158233643 \t15.834725379943848\n",
      "Batch:\t15 /4000\t:  0.16383671760559082 \t2.7253260612487793 \t6.6406707763671875\n",
      "Batch:\t16 /4000\t:  0.18105602264404297 \t2.9143378734588623 \t4.082324504852295\n",
      "Batch:\t17 /4000\t:  0.1020665168762207 \t3.026293992996216 \t1.7151670455932617\n",
      "Time: 4.92628288269043\t Loss: 125.69401359558105\t Valdacc: 0.44\t Testacc: 0.39\n",
      " BestValidAcc: 0.44\n",
      " BestTestAcc: 0.405\n",
      "\n",
      "Epoch:  22 / 200\n",
      "Batch:\t0 /4000\t:  0.16601324081420898 \t0.16895580291748047 \t8.757118225097656\n",
      "Batch:\t1 /4000\t:  0.16318058967590332 \t0.3337721824645996 \t6.65362548828125\n",
      "Batch:\t2 /4000\t:  0.16010808944702148 \t0.49545955657958984 \t10.208724975585938\n",
      "Batch:\t3 /4000\t:  0.16660499572753906 \t0.6643717288970947 \t10.455900192260742\n",
      "Batch:\t4 /4000\t:  0.20386672019958496 \t0.8698194026947021 \t5.234679698944092\n",
      "Batch:\t5 /4000\t:  0.20362043380737305 \t1.0747668743133545 \t4.238717079162598\n",
      "Batch:\t6 /4000\t:  0.15776586532592773 \t1.2338628768920898 \t4.810422897338867\n",
      "Batch:\t7 /4000\t:  0.1770322322845459 \t1.4123036861419678 \t6.707606792449951\n",
      "Batch:\t8 /4000\t:  0.20651602745056152 \t1.625659465789795 \t5.426231861114502\n",
      "Batch:\t9 /4000\t:  0.16468024253845215 \t1.791985273361206 \t9.307751655578613\n",
      "Batch:\t10 /4000\t:  0.18421339988708496 \t1.980900764465332 \t8.347579956054688\n",
      "Batch:\t11 /4000\t:  0.1606595516204834 \t2.1430680751800537 \t0.9652605056762695\n",
      "Batch:\t12 /4000\t:  0.18276691436767578 \t2.3272533416748047 \t6.148905277252197\n",
      "Batch:\t13 /4000\t:  0.16044282913208008 \t2.489321231842041 \t10.473923683166504\n",
      "Batch:\t14 /4000\t:  0.16573548316955566 \t2.6563503742218018 \t10.110618591308594\n",
      "Batch:\t15 /4000\t:  0.17108893394470215 \t2.8292574882507324 \t12.883488655090332\n",
      "Batch:\t16 /4000\t:  0.18035387992858887 \t3.0109496116638184 \t3.553104877471924\n",
      "Batch:\t17 /4000\t:  0.10204625129699707 \t3.114288091659546 \t1.9103050231933594\n",
      "Time: 5.062578439712524\t Loss: 126.19396543502808\t Valdacc: 0.41\t Testacc: 0.425\n",
      " BestValidAcc: 0.44\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  23 / 200\n",
      "Batch:\t0 /4000\t:  0.1644291877746582 \t0.16697454452514648 \t4.571589469909668\n",
      "Batch:\t1 /4000\t:  0.16089987754821777 \t0.3293945789337158 \t11.670116424560547\n",
      "Batch:\t2 /4000\t:  0.20653939247131348 \t0.5374166965484619 \t8.327278137207031\n",
      "Batch:\t3 /4000\t:  0.16788578033447266 \t0.7070534229278564 \t1.0721464157104492\n",
      "Batch:\t4 /4000\t:  0.1878056526184082 \t0.8970038890838623 \t2.17258882522583\n",
      "Batch:\t5 /4000\t:  0.20473098754882812 \t1.1031644344329834 \t6.839169025421143\n",
      "Batch:\t6 /4000\t:  0.17618560791015625 \t1.2811071872711182 \t8.074344635009766\n",
      "Batch:\t7 /4000\t:  0.1612248420715332 \t1.448857069015503 \t4.460417747497559\n",
      "Batch:\t8 /4000\t:  0.16539931297302246 \t1.6157360076904297 \t12.017813682556152\n",
      "Batch:\t9 /4000\t:  0.17951393127441406 \t1.7969913482666016 \t0.8230419158935547\n",
      "Batch:\t10 /4000\t:  0.16458988189697266 \t1.9630236625671387 \t14.795543670654297\n",
      "Batch:\t11 /4000\t:  0.1638338565826416 \t2.12838077545166 \t8.211257934570312\n",
      "Batch:\t12 /4000\t:  0.1669771671295166 \t2.296755075454712 \t2.5488390922546387\n",
      "Batch:\t13 /4000\t:  0.160905122756958 \t2.45906662940979 \t9.960775375366211\n",
      "Batch:\t14 /4000\t:  0.16318345069885254 \t2.6239657402038574 \t11.53056526184082\n",
      "Batch:\t15 /4000\t:  0.16375112533569336 \t2.789255380630493 \t4.741783618927002\n",
      "Batch:\t16 /4000\t:  0.16487836837768555 \t2.955617666244507 \t5.271661758422852\n",
      "Batch:\t17 /4000\t:  0.1026153564453125 \t3.059570789337158 \t4.932705879211426\n",
      "Time: 4.974035978317261\t Loss: 122.02163887023926\t Valdacc: 0.42\t Testacc: 0.395\n",
      " BestValidAcc: 0.44\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  24 / 200\n",
      "Batch:\t0 /4000\t:  0.16388320922851562 \t0.16614079475402832 \t2.5668838024139404\n",
      "Batch:\t1 /4000\t:  0.1645519733428955 \t0.33268022537231445 \t2.6668038368225098\n",
      "Batch:\t2 /4000\t:  0.20517373085021973 \t0.5395510196685791 \t6.8633623123168945\n",
      "Batch:\t3 /4000\t:  0.1609516143798828 \t0.7020432949066162 \t3.873836040496826\n",
      "Batch:\t4 /4000\t:  0.1636199951171875 \t0.8672432899475098 \t5.319677829742432\n",
      "Batch:\t5 /4000\t:  0.16354990005493164 \t1.0325489044189453 \t6.196779727935791\n",
      "Batch:\t6 /4000\t:  0.20430588722229004 \t1.2383992671966553 \t6.244957447052002\n",
      "Batch:\t7 /4000\t:  0.1635429859161377 \t1.4034647941589355 \t6.846213340759277\n",
      "Batch:\t8 /4000\t:  0.1630237102508545 \t1.5679447650909424 \t3.188199996948242\n",
      "Batch:\t9 /4000\t:  0.16680169105529785 \t1.7366771697998047 \t4.487906455993652\n",
      "Batch:\t10 /4000\t:  0.16050052642822266 \t1.8986599445343018 \t3.7106447219848633\n",
      "Batch:\t11 /4000\t:  0.16181635856628418 \t2.062411069869995 \t5.093660354614258\n",
      "Batch:\t12 /4000\t:  0.17498540878295898 \t2.239036798477173 \t4.028931617736816\n",
      "Batch:\t13 /4000\t:  0.16632533073425293 \t2.4066638946533203 \t3.921785354614258\n",
      "Batch:\t14 /4000\t:  0.16074538230895996 \t2.569068431854248 \t3.0386128425598145\n",
      "Batch:\t15 /4000\t:  0.1752641201019287 \t2.7459332942962646 \t7.841115474700928\n",
      "Batch:\t16 /4000\t:  0.1606736183166504 \t2.9084272384643555 \t10.033483505249023\n",
      "Batch:\t17 /4000\t:  0.10142064094543457 \t3.011303663253784 \t0.9763774871826172\n",
      "MODEL WEIGHTS RIGHT NOW:  16439.897437095642\n",
      "model with accuracy  0.47 stored at data/models/core_chain/bilstm_dot_ulmfit/lcquad/93/model.torch\n",
      "in model save, no vectors were found.\n",
      "Time: 7.417960166931152\t Loss: 86.89923214912415\t Valdacc: 0.47\t Testacc: 0.41\n",
      " BestValidAcc: 0.47\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  25 / 200\n",
      "Batch:\t0 /4000\t:  0.16155624389648438 \t0.1625664234161377 \t7.244983673095703\n",
      "Batch:\t1 /4000\t:  0.1666858196258545 \t0.3306734561920166 \t7.228506088256836\n",
      "Batch:\t2 /4000\t:  0.16071319580078125 \t0.4930453300476074 \t7.7636213302612305\n",
      "Batch:\t3 /4000\t:  0.1607527732849121 \t0.6552186012268066 \t5.295358180999756\n",
      "Batch:\t4 /4000\t:  0.16064929962158203 \t0.8172578811645508 \t10.7490816116333\n",
      "Batch:\t5 /4000\t:  0.1619548797607422 \t0.981109619140625 \t2.1902050971984863\n",
      "Batch:\t6 /4000\t:  0.1624743938446045 \t1.1449415683746338 \t4.743729114532471\n",
      "Batch:\t7 /4000\t:  0.16710615158081055 \t1.3136491775512695 \t5.8360724449157715\n",
      "Batch:\t8 /4000\t:  0.16080546379089355 \t1.475841760635376 \t12.786599159240723\n",
      "Batch:\t9 /4000\t:  0.16383075714111328 \t1.6410963535308838 \t3.2136287689208984\n",
      "Batch:\t10 /4000\t:  0.16229486465454102 \t1.8050682544708252 \t3.8493599891662598\n",
      "Batch:\t11 /4000\t:  0.16065049171447754 \t1.9673161506652832 \t5.403064727783203\n",
      "Batch:\t12 /4000\t:  0.16326308250427246 \t2.1322414875030518 \t3.2698442935943604\n",
      "Batch:\t13 /4000\t:  0.16740775108337402 \t2.301069974899292 \t3.2263832092285156\n",
      "Batch:\t14 /4000\t:  0.18062090873718262 \t2.4834773540496826 \t1.5584630966186523\n",
      "Batch:\t15 /4000\t:  0.1637439727783203 \t2.648643732070923 \t5.806419372558594\n",
      "Batch:\t16 /4000\t:  0.1622469425201416 \t2.8128950595855713 \t1.6271038055419922\n",
      "Batch:\t17 /4000\t:  0.10350894927978516 \t2.9359536170959473 \t5.961482048034668\n",
      "Time: 4.867323160171509\t Loss: 97.75390601158142\t Valdacc: 0.47\t Testacc: 0.41\n",
      " BestValidAcc: 0.47\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  26 / 200\n",
      "Batch:\t0 /4000\t:  0.16394257545471191 \t0.16649174690246582 \t7.257229804992676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t1 /4000\t:  0.16749882698059082 \t0.33623242378234863 \t5.108767509460449\n",
      "Batch:\t2 /4000\t:  0.1605055332183838 \t0.49865221977233887 \t4.446056365966797\n",
      "Batch:\t3 /4000\t:  0.17279362678527832 \t0.6729695796966553 \t3.005791664123535\n",
      "Batch:\t4 /4000\t:  0.1760563850402832 \t0.8506629467010498 \t5.152884483337402\n",
      "Batch:\t5 /4000\t:  0.1840343475341797 \t1.036700963973999 \t3.676785469055176\n",
      "Batch:\t6 /4000\t:  0.16074585914611816 \t1.1986722946166992 \t9.257257461547852\n",
      "Batch:\t7 /4000\t:  0.16407299041748047 \t1.3640720844268799 \t8.914649963378906\n",
      "Batch:\t8 /4000\t:  0.18471741676330566 \t1.5510458946228027 \t10.18919563293457\n",
      "Batch:\t9 /4000\t:  0.16028833389282227 \t1.7126705646514893 \t8.247259140014648\n",
      "Batch:\t10 /4000\t:  0.16633224487304688 \t1.8803949356079102 \t2.950472831726074\n",
      "Batch:\t11 /4000\t:  0.18084192276000977 \t2.0628623962402344 \t3.617177963256836\n",
      "Batch:\t12 /4000\t:  0.16497278213500977 \t2.229327917098999 \t2.9724035263061523\n",
      "Batch:\t13 /4000\t:  0.16322040557861328 \t2.3939099311828613 \t2.157735824584961\n",
      "Batch:\t14 /4000\t:  0.1665782928466797 \t2.561861276626587 \t7.876559734344482\n",
      "Batch:\t15 /4000\t:  0.2036123275756836 \t2.7670981884002686 \t2.98250675201416\n",
      "Batch:\t16 /4000\t:  0.18072867393493652 \t2.949443817138672 \t0.7284784317016602\n",
      "Batch:\t17 /4000\t:  0.1013641357421875 \t3.0519590377807617 \t3.613448143005371\n",
      "Time: 4.929955959320068\t Loss: 92.15466070175171\t Valdacc: 0.41\t Testacc: 0.41\n",
      " BestValidAcc: 0.47\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  27 / 200\n",
      "Batch:\t0 /4000\t:  0.16077828407287598 \t0.16335391998291016 \t7.98736047744751\n",
      "Batch:\t1 /4000\t:  0.17713642120361328 \t0.34200119972229004 \t8.956016540527344\n",
      "Batch:\t2 /4000\t:  0.18029046058654785 \t0.5235536098480225 \t3.0956826210021973\n",
      "Batch:\t3 /4000\t:  0.16891026496887207 \t0.6945393085479736 \t4.207485675811768\n",
      "Batch:\t4 /4000\t:  0.16330337524414062 \t0.8595693111419678 \t2.4960784912109375\n",
      "Batch:\t5 /4000\t:  0.16063451766967773 \t1.021498441696167 \t1.35929274559021\n",
      "Batch:\t6 /4000\t:  0.16384363174438477 \t1.1868607997894287 \t7.850871562957764\n",
      "Batch:\t7 /4000\t:  0.1740860939025879 \t1.362339973449707 \t3.9097900390625\n",
      "Batch:\t8 /4000\t:  0.16065597534179688 \t1.5242984294891357 \t3.911241054534912\n",
      "Batch:\t9 /4000\t:  0.16601896286010742 \t1.691535472869873 \t0.5483770370483398\n",
      "Batch:\t10 /4000\t:  0.16036725044250488 \t1.8535587787628174 \t5.4111714363098145\n",
      "Batch:\t11 /4000\t:  0.16382288932800293 \t2.018730401992798 \t7.611098289489746\n",
      "Batch:\t12 /4000\t:  0.16768360137939453 \t2.188098907470703 \t5.36519718170166\n",
      "Batch:\t13 /4000\t:  0.16475176811218262 \t2.3546125888824463 \t6.307577133178711\n",
      "Batch:\t14 /4000\t:  0.1676955223083496 \t2.5241594314575195 \t3.6009178161621094\n",
      "Batch:\t15 /4000\t:  0.20395922660827637 \t2.7299389839172363 \t6.071613788604736\n",
      "Batch:\t16 /4000\t:  0.16373515129089355 \t2.8951797485351562 \t3.7097702026367188\n",
      "Batch:\t17 /4000\t:  0.10548663139343262 \t3.0018515586853027 \t6.153836250305176\n",
      "MODEL WEIGHTS RIGHT NOW:  16447.85601425171\n",
      "model with accuracy  0.48 stored at data/models/core_chain/bilstm_dot_ulmfit/lcquad/93/model.torch\n",
      "in model save, no vectors were found.\n",
      "Time: 7.502172470092773\t Loss: 88.55337834358215\t Valdacc: 0.48\t Testacc: 0.41\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  28 / 200\n",
      "Batch:\t0 /4000\t:  0.1678316593170166 \t0.169419527053833 \t3.2416043281555176\n",
      "Batch:\t1 /4000\t:  0.1610274314880371 \t0.3321387767791748 \t1.8147788047790527\n",
      "Batch:\t2 /4000\t:  0.1673290729522705 \t0.5008945465087891 \t1.6219773292541504\n",
      "Batch:\t3 /4000\t:  0.18323826789855957 \t0.6925108432769775 \t2.5015811920166016\n",
      "Batch:\t4 /4000\t:  0.16611528396606445 \t0.8601951599121094 \t2.601839065551758\n",
      "Batch:\t5 /4000\t:  0.16664552688598633 \t1.0285017490386963 \t6.785475730895996\n",
      "Batch:\t6 /4000\t:  0.17971158027648926 \t1.210200309753418 \t3.1434459686279297\n",
      "Batch:\t7 /4000\t:  0.1668260097503662 \t1.378455638885498 \t5.8748064041137695\n",
      "Batch:\t8 /4000\t:  0.16739392280578613 \t1.547349214553833 \t6.479074478149414\n",
      "Batch:\t9 /4000\t:  0.21669411659240723 \t1.7664060592651367 \t1.7465555667877197\n",
      "Batch:\t10 /4000\t:  0.1607344150543213 \t1.9286816120147705 \t5.982419490814209\n",
      "Batch:\t11 /4000\t:  0.16365957260131836 \t2.0937225818634033 \t2.3859567642211914\n",
      "Batch:\t12 /4000\t:  0.16080808639526367 \t2.2562639713287354 \t4.1607666015625\n",
      "Batch:\t13 /4000\t:  0.16733908653259277 \t2.425135850906372 \t5.672313213348389\n",
      "Batch:\t14 /4000\t:  0.16712069511413574 \t2.5937392711639404 \t4.937465667724609\n",
      "Batch:\t15 /4000\t:  0.16404485702514648 \t2.759453535079956 \t2.2988195419311523\n",
      "Batch:\t16 /4000\t:  0.16499733924865723 \t2.925919532775879 \t2.005682945251465\n",
      "Batch:\t17 /4000\t:  0.1003105640411377 \t3.0277538299560547 \t2.4463367462158203\n",
      "Time: 4.949443340301514\t Loss: 65.70089983940125\t Valdacc: 0.43\t Testacc: 0.4\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  29 / 200\n",
      "Batch:\t0 /4000\t:  0.1806654930114746 \t0.18355727195739746 \t9.700971603393555\n",
      "Batch:\t1 /4000\t:  0.1612696647644043 \t0.34606146812438965 \t2.9215879440307617\n",
      "Batch:\t2 /4000\t:  0.1833198070526123 \t0.5313014984130859 \t2.1300039291381836\n",
      "Batch:\t3 /4000\t:  0.21084094047546387 \t0.7479312419891357 \t2.731903076171875\n",
      "Batch:\t4 /4000\t:  0.16543984413146973 \t0.9147677421569824 \t5.803648471832275\n",
      "Batch:\t5 /4000\t:  0.17767882347106934 \t1.0941698551177979 \t5.847134590148926\n",
      "Batch:\t6 /4000\t:  0.16019105911254883 \t1.2558844089508057 \t8.713079452514648\n",
      "Batch:\t7 /4000\t:  0.16118431091308594 \t1.418656587600708 \t0.14153289794921875\n",
      "Batch:\t8 /4000\t:  0.21244287490844727 \t1.6333916187286377 \t8.242105484008789\n",
      "Batch:\t9 /4000\t:  0.16406822204589844 \t1.7986960411071777 \t4.805093288421631\n",
      "Batch:\t10 /4000\t:  0.1671311855316162 \t1.9671475887298584 \t2.7528929710388184\n",
      "Batch:\t11 /4000\t:  0.185380220413208 \t2.1583874225616455 \t2.993809223175049\n",
      "Batch:\t12 /4000\t:  0.16131138801574707 \t2.3209612369537354 \t4.6150946617126465\n",
      "Batch:\t13 /4000\t:  0.16682100296020508 \t2.4891011714935303 \t6.64005184173584\n",
      "Batch:\t14 /4000\t:  0.17336225509643555 \t2.663675546646118 \t7.10705041885376\n",
      "Batch:\t15 /4000\t:  0.16037607192993164 \t2.8259730339050293 \t16.388015747070312\n",
      "Batch:\t16 /4000\t:  0.1763172149658203 \t3.0037307739257812 \t3.8129258155822754\n",
      "Batch:\t17 /4000\t:  0.11993408203125 \t3.1298322677612305 \t0.06724214553833008\n",
      "Time: 5.022291421890259\t Loss: 95.4141435623169\t Valdacc: 0.39\t Testacc: 0.415\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  30 / 200\n",
      "Batch:\t0 /4000\t:  0.16811060905456543 \t0.16983962059020996 \t5.911322593688965\n",
      "Batch:\t1 /4000\t:  0.16041111946105957 \t0.3317253589630127 \t6.040746688842773\n",
      "Batch:\t2 /4000\t:  0.16331696510314941 \t0.4967021942138672 \t1.8684988021850586\n",
      "Batch:\t3 /4000\t:  0.16296148300170898 \t0.6610102653503418 \t3.392934799194336\n",
      "Batch:\t4 /4000\t:  0.16077566146850586 \t0.8234987258911133 \t3.682283878326416\n",
      "Batch:\t5 /4000\t:  0.1693577766418457 \t0.9942073822021484 \t2.9246435165405273\n",
      "Batch:\t6 /4000\t:  0.16468453407287598 \t1.1602025032043457 \t1.9109711647033691\n",
      "Batch:\t7 /4000\t:  0.17273759841918945 \t1.334669589996338 \t1.431269645690918\n",
      "Batch:\t8 /4000\t:  0.18819165229797363 \t1.5263049602508545 \t0.0\n",
      "Batch:\t9 /4000\t:  0.16349387168884277 \t1.6913893222808838 \t7.987820625305176\n",
      "Batch:\t10 /4000\t:  0.1694626808166504 \t1.8623600006103516 \t6.335233688354492\n",
      "Batch:\t11 /4000\t:  0.16474223136901855 \t2.028860569000244 \t5.839746475219727\n",
      "Batch:\t12 /4000\t:  0.1678142547607422 \t2.197983503341675 \t8.272647857666016\n",
      "Batch:\t13 /4000\t:  0.16627931594848633 \t2.366013526916504 \t1.549191951751709\n",
      "Batch:\t14 /4000\t:  0.18367528915405273 \t2.5578014850616455 \t4.343366622924805\n",
      "Batch:\t15 /4000\t:  0.16811919212341309 \t2.72772479057312 \t1.9741144180297852\n",
      "Batch:\t16 /4000\t:  0.16666126251220703 \t2.8955600261688232 \t0.4570183753967285\n",
      "Batch:\t17 /4000\t:  0.10354471206665039 \t3.0006682872772217 \t0.5770797729492188\n",
      "Time: 4.926274061203003\t Loss: 64.49889087677002\t Valdacc: 0.4\t Testacc: 0.37\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  31 / 200\n",
      "Batch:\t0 /4000\t:  0.16087555885314941 \t0.16333651542663574 \t6.25408935546875\n",
      "Batch:\t1 /4000\t:  0.16660809516906738 \t0.3314528465270996 \t9.687768936157227\n",
      "Batch:\t2 /4000\t:  0.15990447998046875 \t0.4952528476715088 \t6.045688152313232\n",
      "Batch:\t3 /4000\t:  0.2034914493560791 \t0.6999461650848389 \t6.680274963378906\n",
      "Batch:\t4 /4000\t:  0.1740415096282959 \t0.8758895397186279 \t5.889225006103516\n",
      "Batch:\t5 /4000\t:  0.16297698020935059 \t1.0400662422180176 \t1.9167985916137695\n",
      "Batch:\t6 /4000\t:  0.1616990566253662 \t1.2032816410064697 \t3.320805549621582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t7 /4000\t:  0.17027711868286133 \t1.3747422695159912 \t7.08188533782959\n",
      "Batch:\t8 /4000\t:  0.16205596923828125 \t1.5380959510803223 \t0.0\n",
      "Batch:\t9 /4000\t:  0.16760659217834473 \t1.707442045211792 \t3.7330241203308105\n",
      "Batch:\t10 /4000\t:  0.2036120891571045 \t1.9124252796173096 \t3.833083152770996\n",
      "Batch:\t11 /4000\t:  0.16362929344177246 \t2.0776426792144775 \t3.1333370208740234\n",
      "Batch:\t12 /4000\t:  0.16204142570495605 \t2.241042137145996 \t3.436999559402466\n",
      "Batch:\t13 /4000\t:  0.1841270923614502 \t2.4269018173217773 \t6.403335094451904\n",
      "Batch:\t14 /4000\t:  0.1801154613494873 \t2.6083312034606934 \t1.066253662109375\n",
      "Batch:\t15 /4000\t:  0.16772961616516113 \t2.7777063846588135 \t3.6170806884765625\n",
      "Batch:\t16 /4000\t:  0.1735689640045166 \t2.952669382095337 \t1.9398384094238281\n",
      "Batch:\t17 /4000\t:  0.10175347328186035 \t3.055473804473877 \t2.687373161315918\n",
      "Time: 4.922598123550415\t Loss: 76.72686076164246\t Valdacc: 0.41\t Testacc: 0.405\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  32 / 200\n",
      "Batch:\t0 /4000\t:  0.16064953804016113 \t0.162109375 \t5.212014198303223\n",
      "Batch:\t1 /4000\t:  0.18096709251403809 \t0.34461069107055664 \t10.812792778015137\n",
      "Batch:\t2 /4000\t:  0.16619372367858887 \t0.5122418403625488 \t7.715631484985352\n",
      "Batch:\t3 /4000\t:  0.1646590232849121 \t0.6782228946685791 \t2.371187210083008\n",
      "Batch:\t4 /4000\t:  0.16796422004699707 \t0.8479275703430176 \t7.39523458480835\n",
      "Batch:\t5 /4000\t:  0.16577386856079102 \t1.015188455581665 \t3.949875831604004\n",
      "Batch:\t6 /4000\t:  0.1696300506591797 \t1.1861789226531982 \t0.28826904296875\n",
      "Batch:\t7 /4000\t:  0.17201495170593262 \t1.3599426746368408 \t7.499669075012207\n",
      "Batch:\t8 /4000\t:  0.1720716953277588 \t1.5337066650390625 \t2.708366870880127\n",
      "Batch:\t9 /4000\t:  0.1607804298400879 \t1.6959295272827148 \t4.455057621002197\n",
      "Batch:\t10 /4000\t:  0.18122649192810059 \t1.8794291019439697 \t6.002747535705566\n",
      "Batch:\t11 /4000\t:  0.16361045837402344 \t2.044440507888794 \t7.598262310028076\n",
      "Batch:\t12 /4000\t:  0.16236305236816406 \t2.2079484462738037 \t0.746549129486084\n",
      "Batch:\t13 /4000\t:  0.1745738983154297 \t2.3841664791107178 \t4.639590263366699\n",
      "Batch:\t14 /4000\t:  0.16720199584960938 \t2.5538840293884277 \t3.6021347045898438\n",
      "Batch:\t15 /4000\t:  0.1705009937286377 \t2.7262282371520996 \t1.8141369819641113\n",
      "Batch:\t16 /4000\t:  0.16687488555908203 \t2.900455951690674 \t1.3340644836425781\n",
      "Batch:\t17 /4000\t:  0.09843063354492188 \t2.9999263286590576 \t1.5504531860351562\n",
      "Time: 4.92084002494812\t Loss: 79.69603729248047\t Valdacc: 0.39\t Testacc: 0.425\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  33 / 200\n",
      "Batch:\t0 /4000\t:  0.16657280921936035 \t0.16882777214050293 \t6.6205878257751465\n",
      "Batch:\t1 /4000\t:  0.16638898849487305 \t0.3364439010620117 \t3.58260178565979\n",
      "Batch:\t2 /4000\t:  0.17499899864196777 \t0.516848087310791 \t3.4702634811401367\n",
      "Batch:\t3 /4000\t:  0.16303682327270508 \t0.6811783313751221 \t3.533857822418213\n",
      "Batch:\t4 /4000\t:  0.20351839065551758 \t0.8860073089599609 \t4.565731048583984\n",
      "Batch:\t5 /4000\t:  0.16495466232299805 \t1.0531535148620605 \t2.928009033203125\n",
      "Batch:\t6 /4000\t:  0.16169977188110352 \t1.2164936065673828 \t1.8639211654663086\n",
      "Batch:\t7 /4000\t:  0.1636660099029541 \t1.3815152645111084 \t2.6339001655578613\n",
      "Batch:\t8 /4000\t:  0.16718125343322754 \t1.5501437187194824 \t2.1508712768554688\n",
      "Batch:\t9 /4000\t:  0.16013193130493164 \t1.7116303443908691 \t1.8002376556396484\n",
      "Batch:\t10 /4000\t:  0.16188955307006836 \t1.8749804496765137 \t2.100602149963379\n",
      "Batch:\t11 /4000\t:  0.16030120849609375 \t2.0368199348449707 \t8.579782485961914\n",
      "Batch:\t12 /4000\t:  0.1657247543334961 \t2.2040514945983887 \t3.5499086380004883\n",
      "Batch:\t13 /4000\t:  0.16068625450134277 \t2.3662688732147217 \t2.6045756340026855\n",
      "Batch:\t14 /4000\t:  0.16627931594848633 \t2.533888101577759 \t3.687819480895996\n",
      "Batch:\t15 /4000\t:  0.16484332084655762 \t2.7003722190856934 \t2.7927494049072266\n",
      "Batch:\t16 /4000\t:  0.16072297096252441 \t2.862553834915161 \t4.054152488708496\n",
      "Batch:\t17 /4000\t:  0.10172152519226074 \t2.965723752975464 \t0.6371326446533203\n",
      "Time: 4.866327524185181\t Loss: 61.15670418739319\t Valdacc: 0.44\t Testacc: 0.42\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  34 / 200\n",
      "Batch:\t0 /4000\t:  0.16337943077087402 \t0.164353609085083 \t1.0779376029968262\n",
      "Batch:\t1 /4000\t:  0.1698932647705078 \t0.3356435298919678 \t2.019011974334717\n",
      "Batch:\t2 /4000\t:  0.20335721969604492 \t0.543048620223999 \t7.531119346618652\n",
      "Batch:\t3 /4000\t:  0.18070220947265625 \t0.7255237102508545 \t7.884364604949951\n",
      "Batch:\t4 /4000\t:  0.1629946231842041 \t0.8900630474090576 \t3.2132482528686523\n",
      "Batch:\t5 /4000\t:  0.17158794403076172 \t1.063081979751587 \t6.600581645965576\n",
      "Batch:\t6 /4000\t:  0.16045713424682617 \t1.2248785495758057 \t1.6710972785949707\n",
      "Batch:\t7 /4000\t:  0.1870861053466797 \t1.4136924743652344 \t2.3330464363098145\n",
      "Batch:\t8 /4000\t:  0.18069171905517578 \t1.5957179069519043 \t1.2990384101867676\n",
      "Batch:\t9 /4000\t:  0.16346144676208496 \t1.7609999179840088 \t1.0805044174194336\n",
      "Batch:\t10 /4000\t:  0.17240142822265625 \t1.9350650310516357 \t0.9220428466796875\n",
      "Batch:\t11 /4000\t:  0.17870855331420898 \t2.115647077560425 \t2.779113292694092\n",
      "Batch:\t12 /4000\t:  0.16654396057128906 \t2.283571243286133 \t2.8887176513671875\n",
      "Batch:\t13 /4000\t:  0.18193936347961426 \t2.4675629138946533 \t0.48860788345336914\n",
      "Batch:\t14 /4000\t:  0.16962170600891113 \t2.6406819820404053 \t1.0441842079162598\n",
      "Batch:\t15 /4000\t:  0.16112971305847168 \t2.803330183029175 \t3.8086509704589844\n",
      "Batch:\t16 /4000\t:  0.17016243934631348 \t2.975278854370117 \t5.829987525939941\n",
      "Batch:\t17 /4000\t:  0.13530325889587402 \t3.1132395267486572 \t0.27256298065185547\n",
      "Time: 5.001397132873535\t Loss: 52.74381732940674\t Valdacc: 0.41\t Testacc: 0.395\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  35 / 200\n",
      "Batch:\t0 /4000\t:  0.1907949447631836 \t0.19294977188110352 \t6.533492565155029\n",
      "Batch:\t1 /4000\t:  0.16406655311584473 \t0.35831713676452637 \t4.631064414978027\n",
      "Batch:\t2 /4000\t:  0.16537952423095703 \t0.5253028869628906 \t3.5519180297851562\n",
      "Batch:\t3 /4000\t:  0.18633699417114258 \t0.7127771377563477 \t2.8526675701141357\n",
      "Batch:\t4 /4000\t:  0.16669607162475586 \t0.8813014030456543 \t3.3426265716552734\n",
      "Batch:\t5 /4000\t:  0.17377138137817383 \t1.057206392288208 \t0.7295475006103516\n",
      "Batch:\t6 /4000\t:  0.17624378204345703 \t1.2506787776947021 \t5.365228652954102\n",
      "Batch:\t7 /4000\t:  0.1614694595336914 \t1.4138760566711426 \t9.4735689163208\n",
      "Batch:\t8 /4000\t:  0.1633901596069336 \t1.5790247917175293 \t3.6154866218566895\n",
      "Batch:\t9 /4000\t:  0.16846752166748047 \t1.7491016387939453 \t3.245663642883301\n",
      "Batch:\t10 /4000\t:  0.1659069061279297 \t1.9166953563690186 \t1.2868881225585938\n",
      "Batch:\t11 /4000\t:  0.1609361171722412 \t2.079362154006958 \t1.9578185081481934\n",
      "Batch:\t12 /4000\t:  0.1612224578857422 \t2.242025136947632 \t1.467402458190918\n",
      "Batch:\t13 /4000\t:  0.16100645065307617 \t2.4047372341156006 \t4.146747589111328\n",
      "Batch:\t14 /4000\t:  0.1745586395263672 \t2.580963373184204 \t4.162653923034668\n",
      "Batch:\t15 /4000\t:  0.16086912155151367 \t2.7437970638275146 \t4.8834943771362305\n",
      "Batch:\t16 /4000\t:  0.16240739822387695 \t2.9082987308502197 \t1.1862525939941406\n",
      "Batch:\t17 /4000\t:  0.1031637191772461 \t3.0129029750823975 \t3.1573896408081055\n",
      "Time: 4.914496898651123\t Loss: 65.58991169929504\t Valdacc: 0.41\t Testacc: 0.4\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  36 / 200\n",
      "Batch:\t0 /4000\t:  0.18383193016052246 \t0.1849205493927002 \t3.9099740982055664\n",
      "Batch:\t1 /4000\t:  0.16419768333435059 \t0.3510322570800781 \t1.514657974243164\n",
      "Batch:\t2 /4000\t:  0.16510629653930664 \t0.517784595489502 \t4.3552327156066895\n",
      "Batch:\t3 /4000\t:  0.17386889457702637 \t0.6938846111297607 \t4.408392906188965\n",
      "Batch:\t4 /4000\t:  0.16536188125610352 \t0.8612442016601562 \t1.1932153701782227\n",
      "Batch:\t5 /4000\t:  0.1668412685394287 \t1.0295817852020264 \t5.573631286621094\n",
      "Batch:\t6 /4000\t:  0.16469120979309082 \t1.1957623958587646 \t4.494817733764648\n",
      "Batch:\t7 /4000\t:  0.1810753345489502 \t1.3785862922668457 \t2.7298126220703125\n",
      "Batch:\t8 /4000\t:  0.16731786727905273 \t1.5473453998565674 \t3.6642541885375977\n",
      "Batch:\t9 /4000\t:  0.18182802200317383 \t1.7304966449737549 \t3.5172696113586426\n",
      "Batch:\t10 /4000\t:  0.16531729698181152 \t1.8974850177764893 \t3.781987190246582\n",
      "Batch:\t11 /4000\t:  0.1634538173675537 \t2.0627009868621826 \t2.9559521675109863\n",
      "Batch:\t12 /4000\t:  0.16693472862243652 \t2.231074333190918 \t2.9669394493103027\n",
      "Batch:\t13 /4000\t:  0.1786365509033203 \t2.4112203121185303 \t3.5299692153930664\n",
      "Batch:\t14 /4000\t:  0.168989896774292 \t2.5819249153137207 \t3.0085902214050293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t15 /4000\t:  0.16248655319213867 \t2.745918035507202 \t2.3935580253601074\n",
      "Batch:\t16 /4000\t:  0.17342734336853027 \t2.9214680194854736 \t2.4969043731689453\n",
      "Batch:\t17 /4000\t:  0.10429000854492188 \t3.0270087718963623 \t0.6252143383026123\n",
      "Time: 4.974086761474609\t Loss: 57.120373487472534\t Valdacc: 0.44\t Testacc: 0.395\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  37 / 200\n",
      "Batch:\t0 /4000\t:  0.16432905197143555 \t0.16688799858093262 \t3.044381618499756\n",
      "Batch:\t1 /4000\t:  0.17086172103881836 \t0.3394455909729004 \t2.93886137008667\n",
      "Batch:\t2 /4000\t:  0.16128063201904297 \t0.5020332336425781 \t5.986485004425049\n",
      "Batch:\t3 /4000\t:  0.16185355186462402 \t0.6654746532440186 \t3.262554168701172\n",
      "Batch:\t4 /4000\t:  0.18202972412109375 \t0.8490777015686035 \t1.5531501770019531\n",
      "Batch:\t5 /4000\t:  0.20445609092712402 \t1.0547449588775635 \t1.2133431434631348\n",
      "Batch:\t6 /4000\t:  0.1670076847076416 \t1.2237193584442139 \t0.7739696502685547\n",
      "Batch:\t7 /4000\t:  0.16771292686462402 \t1.3931701183319092 \t2.6887102127075195\n",
      "Batch:\t8 /4000\t:  0.16714239120483398 \t1.5618948936462402 \t5.93259334564209\n",
      "Batch:\t9 /4000\t:  0.20623254776000977 \t1.7699265480041504 \t1.2615890502929688\n",
      "Batch:\t10 /4000\t:  0.16744089126586914 \t1.938990831375122 \t1.0966482162475586\n",
      "Batch:\t11 /4000\t:  0.1622943878173828 \t2.102966785430908 \t4.679211616516113\n",
      "Batch:\t12 /4000\t:  0.1613328456878662 \t2.2661380767822266 \t1.2742778062820435\n",
      "Batch:\t13 /4000\t:  0.17930197715759277 \t2.4472222328186035 \t11.698321342468262\n",
      "Batch:\t14 /4000\t:  0.16751551628112793 \t2.6162781715393066 \t4.297063827514648\n",
      "Batch:\t15 /4000\t:  0.16585659980773926 \t2.7837586402893066 \t2.094320058822632\n",
      "Batch:\t16 /4000\t:  0.16683697700500488 \t2.952136993408203 \t1.706700325012207\n",
      "Batch:\t17 /4000\t:  0.10370635986328125 \t3.057175636291504 \t2.0434041023254395\n",
      "Time: 5.0602803230285645\t Loss: 57.54558503627777\t Valdacc: 0.44\t Testacc: 0.365\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  38 / 200\n",
      "Batch:\t0 /4000\t:  0.16469812393188477 \t0.16554784774780273 \t10.132406234741211\n",
      "Batch:\t1 /4000\t:  0.16346263885498047 \t0.33059120178222656 \t1.6407508850097656\n",
      "Batch:\t2 /4000\t:  0.1635732650756836 \t0.4956531524658203 \t2.0246658325195312\n",
      "Batch:\t3 /4000\t:  0.16651177406311035 \t0.6637833118438721 \t5.697484016418457\n",
      "Batch:\t4 /4000\t:  0.16289925575256348 \t0.8280971050262451 \t2.044875144958496\n",
      "Batch:\t5 /4000\t:  0.1611320972442627 \t0.9909467697143555 \t3.382021188735962\n",
      "Batch:\t6 /4000\t:  0.16731810569763184 \t1.1601195335388184 \t2.3859729766845703\n",
      "Batch:\t7 /4000\t:  0.1607341766357422 \t1.3224172592163086 \t6.074840068817139\n",
      "Batch:\t8 /4000\t:  0.16404414176940918 \t1.4883191585540771 \t4.425506591796875\n",
      "Batch:\t9 /4000\t:  0.16369843482971191 \t1.6534497737884521 \t0.6973123550415039\n",
      "Batch:\t10 /4000\t:  0.16100645065307617 \t1.8159055709838867 \t2.9867496490478516\n",
      "Batch:\t11 /4000\t:  0.16480636596679688 \t1.9820995330810547 \t3.0976781845092773\n",
      "Batch:\t12 /4000\t:  0.16724824905395508 \t2.1511590480804443 \t3.7198312282562256\n",
      "Batch:\t13 /4000\t:  0.17499089241027832 \t2.327899217605591 \t1.9491124153137207\n",
      "Batch:\t14 /4000\t:  0.16520237922668457 \t2.4947612285614014 \t5.129359245300293\n",
      "Batch:\t15 /4000\t:  0.16334080696105957 \t2.659508228302002 \t4.274182319641113\n",
      "Batch:\t16 /4000\t:  0.16123604774475098 \t2.8223230838775635 \t4.358739852905273\n",
      "Batch:\t17 /4000\t:  0.10278630256652832 \t2.9263479709625244 \t3.395467758178711\n",
      "Time: 4.908068656921387\t Loss: 67.41695594787598\t Valdacc: 0.47\t Testacc: 0.4\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  39 / 200\n",
      "Batch:\t0 /4000\t:  0.1637098789215088 \t0.16644716262817383 \t4.127918720245361\n",
      "Batch:\t1 /4000\t:  0.16414260864257812 \t0.33214640617370605 \t2.3936681747436523\n",
      "Batch:\t2 /4000\t:  0.16852045059204102 \t0.5024192333221436 \t1.5780773162841797\n",
      "Batch:\t3 /4000\t:  0.16393613815307617 \t0.668083667755127 \t0.8585271835327148\n",
      "Batch:\t4 /4000\t:  0.17108893394470215 \t0.8415968418121338 \t4.51721715927124\n",
      "Batch:\t5 /4000\t:  0.16443252563476562 \t1.0079212188720703 \t2.079371452331543\n",
      "Batch:\t6 /4000\t:  0.16153407096862793 \t1.1712820529937744 \t2.6041722297668457\n",
      "Batch:\t7 /4000\t:  0.17166852951049805 \t1.3443348407745361 \t1.6405820846557617\n",
      "Batch:\t8 /4000\t:  0.16211485862731934 \t1.5084750652313232 \t6.278127670288086\n",
      "Batch:\t9 /4000\t:  0.1617138385772705 \t1.6715714931488037 \t2.094609260559082\n",
      "Batch:\t10 /4000\t:  0.16310882568359375 \t1.8366975784301758 \t1.8624725341796875\n",
      "Batch:\t11 /4000\t:  0.16576147079467773 \t2.003915309906006 \t2.925747871398926\n",
      "Batch:\t12 /4000\t:  0.16413378715515137 \t2.169734477996826 \t0.7416963577270508\n",
      "Batch:\t13 /4000\t:  0.167466402053833 \t2.3388543128967285 \t1.0529232025146484\n",
      "Batch:\t14 /4000\t:  0.1622631549835205 \t2.5030429363250732 \t4.893291473388672\n",
      "Batch:\t15 /4000\t:  0.16737890243530273 \t2.672163248062134 \t0.3409996032714844\n",
      "Batch:\t16 /4000\t:  0.16669774055480957 \t2.840136766433716 \t4.2618865966796875\n",
      "Batch:\t17 /4000\t:  0.10219049453735352 \t2.943135976791382 \t2.4905967712402344\n",
      "Time: 4.909504413604736\t Loss: 46.74188566207886\t Valdacc: 0.4\t Testacc: 0.4\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  40 / 200\n",
      "Batch:\t0 /4000\t:  0.16341638565063477 \t0.1645369529724121 \t1.725306510925293\n",
      "Batch:\t1 /4000\t:  0.1679978370666504 \t0.33389830589294434 \t7.390013694763184\n",
      "Batch:\t2 /4000\t:  0.1623995304107666 \t0.49793076515197754 \t10.990469932556152\n",
      "Batch:\t3 /4000\t:  0.16047382354736328 \t0.660085916519165 \t4.290526390075684\n",
      "Batch:\t4 /4000\t:  0.20596957206726074 \t0.8680663108825684 \t3.134369373321533\n",
      "Batch:\t5 /4000\t:  0.16165995597839355 \t1.031348705291748 \t4.918654441833496\n",
      "Batch:\t6 /4000\t:  0.16943955421447754 \t1.202181339263916 \t7.76125431060791\n",
      "Batch:\t7 /4000\t:  0.16182279586791992 \t1.3658878803253174 \t1.0297107696533203\n",
      "Batch:\t8 /4000\t:  0.16750526428222656 \t1.5350861549377441 \t1.1633319854736328\n",
      "Batch:\t9 /4000\t:  0.16586589813232422 \t1.702361822128296 \t5.610464096069336\n",
      "Batch:\t10 /4000\t:  0.17009782791137695 \t1.8738620281219482 \t5.02531099319458\n",
      "Batch:\t11 /4000\t:  0.16400814056396484 \t2.039689779281616 \t0.26431119441986084\n",
      "Batch:\t12 /4000\t:  0.16101384162902832 \t2.2020397186279297 \t3.1230039596557617\n",
      "Batch:\t13 /4000\t:  0.16395878791809082 \t2.367840528488159 \t2.503544807434082\n",
      "Batch:\t14 /4000\t:  0.17718791961669922 \t2.5468738079071045 \t3.941769599914551\n",
      "Batch:\t15 /4000\t:  0.1655583381652832 \t2.7174971103668213 \t6.91077184677124\n",
      "Batch:\t16 /4000\t:  0.16881823539733887 \t2.889749526977539 \t7.419179916381836\n",
      "Batch:\t17 /4000\t:  0.10301351547241211 \t2.994124412536621 \t4.723597526550293\n",
      "Time: 4.949974060058594\t Loss: 81.92559134960175\t Valdacc: 0.4\t Testacc: 0.385\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  41 / 200\n",
      "Batch:\t0 /4000\t:  0.16086673736572266 \t0.1633439064025879 \t0.7180709838867188\n",
      "Batch:\t1 /4000\t:  0.1633741855621338 \t0.32816576957702637 \t6.637432098388672\n",
      "Batch:\t2 /4000\t:  0.16217374801635742 \t0.49176669120788574 \t2.937600612640381\n",
      "Batch:\t3 /4000\t:  0.16116786003112793 \t0.6552305221557617 \t3.291682481765747\n",
      "Batch:\t4 /4000\t:  0.16288185119628906 \t0.8198940753936768 \t4.0637102127075195\n",
      "Batch:\t5 /4000\t:  0.1752018928527832 \t0.9967522621154785 \t0.03510475158691406\n",
      "Batch:\t6 /4000\t:  0.16109180450439453 \t1.1593008041381836 \t3.061041831970215\n",
      "Batch:\t7 /4000\t:  0.16099333763122559 \t1.3216798305511475 \t3.8885669708251953\n",
      "Batch:\t8 /4000\t:  0.16921424865722656 \t1.4922749996185303 \t2.2444591522216797\n",
      "Batch:\t9 /4000\t:  0.16574978828430176 \t1.6594820022583008 \t2.9898509979248047\n",
      "Batch:\t10 /4000\t:  0.16671204566955566 \t1.8279175758361816 \t4.334116458892822\n",
      "Batch:\t11 /4000\t:  0.16826128959655762 \t1.9977328777313232 \t0.48320484161376953\n",
      "Batch:\t12 /4000\t:  0.16845417022705078 \t2.1679232120513916 \t2.179473876953125\n",
      "Batch:\t13 /4000\t:  0.16967511177062988 \t2.338932752609253 \t0.8940067291259766\n",
      "Batch:\t14 /4000\t:  0.16439151763916016 \t2.504720449447632 \t1.2423601150512695\n",
      "Batch:\t15 /4000\t:  0.16517424583435059 \t2.671948194503784 \t1.476419448852539\n",
      "Batch:\t16 /4000\t:  0.1748647689819336 \t2.848783016204834 \t4.280489921569824\n",
      "Batch:\t17 /4000\t:  0.10172104835510254 \t2.9519028663635254 \t1.174088478088379\n",
      "Time: 4.872636079788208\t Loss: 45.93167996406555\t Valdacc: 0.41\t Testacc: 0.39\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  42 / 200\n",
      "Batch:\t0 /4000\t:  0.1637868881225586 \t0.1660001277923584 \t2.057368755340576\n",
      "Batch:\t1 /4000\t:  0.17086005210876465 \t0.33880162239074707 \t2.0403494834899902\n",
      "Batch:\t2 /4000\t:  0.179734468460083 \t0.5201046466827393 \t1.472395896911621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t3 /4000\t:  0.1660153865814209 \t0.6873762607574463 \t6.593318939208984\n",
      "Batch:\t4 /4000\t:  0.1682133674621582 \t0.8572421073913574 \t0.42046451568603516\n",
      "Batch:\t5 /4000\t:  0.20322656631469727 \t1.061755895614624 \t3.72003173828125\n",
      "Batch:\t6 /4000\t:  0.16594362258911133 \t1.2296192646026611 \t7.717314720153809\n",
      "Batch:\t7 /4000\t:  0.16062259674072266 \t1.3918769359588623 \t3.1518402099609375\n",
      "Batch:\t8 /4000\t:  0.16690659523010254 \t1.559966802597046 \t3.4758458137512207\n",
      "Batch:\t9 /4000\t:  0.16378235816955566 \t1.7250840663909912 \t3.174295425415039\n",
      "Batch:\t10 /4000\t:  0.166212797164917 \t1.892735242843628 \t0.9407100677490234\n",
      "Batch:\t11 /4000\t:  0.16276001930236816 \t2.0567750930786133 \t3.7987570762634277\n",
      "Batch:\t12 /4000\t:  0.1672968864440918 \t2.2256546020507812 \t1.7197225093841553\n",
      "Batch:\t13 /4000\t:  0.17607760429382324 \t2.4033923149108887 \t0.5661687850952148\n",
      "Batch:\t14 /4000\t:  0.16109609603881836 \t2.5659828186035156 \t7.056042194366455\n",
      "Batch:\t15 /4000\t:  0.16361069679260254 \t2.7313461303710938 \t2.4621710777282715\n",
      "Batch:\t16 /4000\t:  0.16119122505187988 \t2.895087957382202 \t3.996276378631592\n",
      "Batch:\t17 /4000\t:  0.1052393913269043 \t3.001903772354126 \t1.0484390258789062\n",
      "Time: 4.929855585098267\t Loss: 55.41151261329651\t Valdacc: 0.4\t Testacc: 0.395\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  43 / 200\n",
      "Batch:\t0 /4000\t:  0.16627192497253418 \t0.16789984703063965 \t6.2022552490234375\n",
      "Batch:\t1 /4000\t:  0.16672182083129883 \t0.3359408378601074 \t0.0922393798828125\n",
      "Batch:\t2 /4000\t:  0.18222618103027344 \t0.5192697048187256 \t2.9379756450653076\n",
      "Batch:\t3 /4000\t:  0.16420245170593262 \t0.6852478981018066 \t3.8803982734680176\n",
      "Batch:\t4 /4000\t:  0.17342662811279297 \t0.8602721691131592 \t2.8272323608398438\n",
      "Batch:\t5 /4000\t:  0.22625970840454102 \t1.0890247821807861 \t2.0796804428100586\n",
      "Batch:\t6 /4000\t:  0.1662154197692871 \t1.256643533706665 \t1.9082555770874023\n",
      "Batch:\t7 /4000\t:  0.18140792846679688 \t1.4397146701812744 \t1.546827793121338\n",
      "Batch:\t8 /4000\t:  0.16823339462280273 \t1.609989881515503 \t0.3518795967102051\n",
      "Batch:\t9 /4000\t:  0.1611769199371338 \t1.7726118564605713 \t6.054366111755371\n",
      "Batch:\t10 /4000\t:  0.16620993614196777 \t1.9407131671905518 \t1.459193229675293\n",
      "Batch:\t11 /4000\t:  0.1669461727142334 \t2.1090400218963623 \t3.7462234497070312\n",
      "Batch:\t12 /4000\t:  0.16814184188842773 \t2.279062271118164 \t1.5030450820922852\n",
      "Batch:\t13 /4000\t:  0.1618661880493164 \t2.4427638053894043 \t2.882061004638672\n",
      "Batch:\t14 /4000\t:  0.1625194549560547 \t2.6072335243225098 \t2.628538131713867\n",
      "Batch:\t15 /4000\t:  0.18499541282653809 \t2.7938239574432373 \t1.4429054260253906\n",
      "Batch:\t16 /4000\t:  0.18619227409362793 \t2.9825284481048584 \t0.7977151870727539\n",
      "Batch:\t17 /4000\t:  0.11096858978271484 \t3.0950489044189453 \t2.438553810119629\n",
      "Time: 5.188778400421143\t Loss: 44.779345750808716\t Valdacc: 0.45\t Testacc: 0.395\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  44 / 200\n",
      "Batch:\t0 /4000\t:  0.16071534156799316 \t0.16243767738342285 \t1.7174463272094727\n",
      "Batch:\t1 /4000\t:  0.15914607048034668 \t0.3232290744781494 \t0.0\n",
      "Batch:\t2 /4000\t:  0.16672825813293457 \t0.4914212226867676 \t4.098309516906738\n",
      "Batch:\t3 /4000\t:  0.1673436164855957 \t0.6601417064666748 \t1.0787878036499023\n",
      "Batch:\t4 /4000\t:  0.17218375205993652 \t0.8341386318206787 \t0.8372325897216797\n",
      "Batch:\t5 /4000\t:  0.1668870449066162 \t1.0023977756500244 \t0.5543880462646484\n",
      "Batch:\t6 /4000\t:  0.16429638862609863 \t1.1687018871307373 \t1.693580150604248\n",
      "Batch:\t7 /4000\t:  0.16469430923461914 \t1.3352842330932617 \t1.8766288757324219\n",
      "Batch:\t8 /4000\t:  0.1643378734588623 \t1.5011162757873535 \t2.492480754852295\n",
      "Batch:\t9 /4000\t:  0.16678810119628906 \t1.6693010330200195 \t1.9497709274291992\n",
      "Batch:\t10 /4000\t:  0.1641533374786377 \t1.8349401950836182 \t5.012578010559082\n",
      "Batch:\t11 /4000\t:  0.18339180946350098 \t2.020155191421509 \t4.515790939331055\n",
      "Batch:\t12 /4000\t:  0.16378521919250488 \t2.1857306957244873 \t4.362787246704102\n",
      "Batch:\t13 /4000\t:  0.16384005546569824 \t2.3510489463806152 \t1.8080177307128906\n",
      "Batch:\t14 /4000\t:  0.16050243377685547 \t2.5132381916046143 \t1.9516887664794922\n",
      "Batch:\t15 /4000\t:  0.16622328758239746 \t2.6808228492736816 \t1.4569129943847656\n",
      "Batch:\t16 /4000\t:  0.20113229751586914 \t2.8834595680236816 \t0.6236057281494141\n",
      "Batch:\t17 /4000\t:  0.11111140251159668 \t3.009652614593506 \t1.7313108444213867\n",
      "Time: 4.893018484115601\t Loss: 37.76131725311279\t Valdacc: 0.38\t Testacc: 0.4\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  45 / 200\n",
      "Batch:\t0 /4000\t:  0.16316485404968262 \t0.16405177116394043 \t2.7572851181030273\n",
      "Batch:\t1 /4000\t:  0.16069912910461426 \t0.3260002136230469 \t4.076753616333008\n",
      "Batch:\t2 /4000\t:  0.1642465591430664 \t0.4916667938232422 \t0.7065858840942383\n",
      "Batch:\t3 /4000\t:  0.17992854118347168 \t0.6782724857330322 \t0.9361391067504883\n",
      "Batch:\t4 /4000\t:  0.16700029373168945 \t0.8466155529022217 \t2.0572280883789062\n",
      "Batch:\t5 /4000\t:  0.20332050323486328 \t1.0516293048858643 \t3.2405152320861816\n",
      "Batch:\t6 /4000\t:  0.16287684440612793 \t1.216012954711914 \t0.1859426498413086\n",
      "Batch:\t7 /4000\t:  0.16736936569213867 \t1.3851385116577148 \t1.3691272735595703\n",
      "Batch:\t8 /4000\t:  0.16274356842041016 \t1.5493178367614746 \t4.347566604614258\n",
      "Batch:\t9 /4000\t:  0.177262544631958 \t1.7278287410736084 \t4.578549385070801\n",
      "Batch:\t10 /4000\t:  0.16061925888061523 \t1.8901665210723877 \t1.5141048431396484\n",
      "Batch:\t11 /4000\t:  0.16238975524902344 \t2.0544815063476562 \t4.867556571960449\n",
      "Batch:\t12 /4000\t:  0.16468191146850586 \t2.2208471298217773 \t3.062130928039551\n",
      "Batch:\t13 /4000\t:  0.16658782958984375 \t2.3886775970458984 \t1.5208935737609863\n",
      "Batch:\t14 /4000\t:  0.1631324291229248 \t2.5528039932250977 \t5.27075719833374\n",
      "Batch:\t15 /4000\t:  0.16572213172912598 \t2.7200820446014404 \t3.419999122619629\n",
      "Batch:\t16 /4000\t:  0.16318988800048828 \t2.88517689704895 \t0.4211435317993164\n",
      "Batch:\t17 /4000\t:  0.10127091407775879 \t2.987945556640625 \t1.471534013748169\n",
      "Time: 5.0285797119140625\t Loss: 45.803812742233276\t Valdacc: 0.41\t Testacc: 0.4\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  46 / 200\n",
      "Batch:\t0 /4000\t:  0.1607203483581543 \t0.16164398193359375 \t0.3058900833129883\n",
      "Batch:\t1 /4000\t:  0.16272521018981934 \t0.32601046562194824 \t2.957733154296875\n",
      "Batch:\t2 /4000\t:  0.20541024208068848 \t0.5329666137695312 \t1.6855731010437012\n",
      "Batch:\t3 /4000\t:  0.16826558113098145 \t0.7028512954711914 \t2.4863412380218506\n",
      "Batch:\t4 /4000\t:  0.16555404663085938 \t0.869652509689331 \t1.714219570159912\n",
      "Batch:\t5 /4000\t:  0.1621239185333252 \t1.0333805084228516 \t1.6451139450073242\n",
      "Batch:\t6 /4000\t:  0.1700575351715088 \t1.204942226409912 \t3.3910727500915527\n",
      "Batch:\t7 /4000\t:  0.16431879997253418 \t1.3706681728363037 \t5.448250770568848\n",
      "Batch:\t8 /4000\t:  0.20149850845336914 \t1.573594331741333 \t0.8553142547607422\n",
      "Batch:\t9 /4000\t:  0.16501832008361816 \t1.7398381233215332 \t2.6111459732055664\n",
      "Batch:\t10 /4000\t:  0.16379761695861816 \t1.9051239490509033 \t4.259332656860352\n",
      "Batch:\t11 /4000\t:  0.1657876968383789 \t2.07236385345459 \t0.5058059692382812\n",
      "Batch:\t12 /4000\t:  0.16048550605773926 \t2.234665870666504 \t2.1716623306274414\n",
      "Batch:\t13 /4000\t:  0.1639411449432373 \t2.400160551071167 \t0.5016627311706543\n",
      "Batch:\t14 /4000\t:  0.16317152976989746 \t2.5649876594543457 \t1.214123249053955\n",
      "Batch:\t15 /4000\t:  0.20363974571228027 \t2.7699978351593018 \t11.478013038635254\n",
      "Batch:\t16 /4000\t:  0.16705751419067383 \t2.9385087490081787 \t3.521562099456787\n",
      "Batch:\t17 /4000\t:  0.09971976280212402 \t3.039590358734131 \t0.0\n",
      "Time: 4.9349684715271\t Loss: 46.752816915512085\t Valdacc: 0.4\t Testacc: 0.41\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  47 / 200\n",
      "Batch:\t0 /4000\t:  0.16072368621826172 \t0.1617748737335205 \t2.8714942932128906\n",
      "Batch:\t1 /4000\t:  0.16955065727233887 \t0.33309483528137207 \t1.0024375915527344\n",
      "Batch:\t2 /4000\t:  0.17182064056396484 \t0.5064215660095215 \t5.158857822418213\n",
      "Batch:\t3 /4000\t:  0.16909003257751465 \t0.6767854690551758 \t2.49129581451416\n",
      "Batch:\t4 /4000\t:  0.1668391227722168 \t0.8447473049163818 \t0.0\n",
      "Batch:\t5 /4000\t:  0.17808890342712402 \t1.0246944427490234 \t0.8237714767456055\n",
      "Batch:\t6 /4000\t:  0.1601560115814209 \t1.1861903667449951 \t3.5527172088623047\n",
      "Batch:\t7 /4000\t:  0.15990829467773438 \t1.3469297885894775 \t3.2210960388183594\n",
      "Batch:\t8 /4000\t:  0.1657552719116211 \t1.5140202045440674 \t1.2549772262573242\n",
      "Batch:\t9 /4000\t:  0.1625678539276123 \t1.6781134605407715 \t1.2074470520019531\n",
      "Batch:\t10 /4000\t:  0.20173907279968262 \t1.8812260627746582 \t2.3847618103027344\n",
      "Batch:\t11 /4000\t:  0.16114282608032227 \t2.0440096855163574 \t2.331862449645996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t12 /4000\t:  0.16169142723083496 \t2.2072207927703857 \t3.1247215270996094\n",
      "Batch:\t13 /4000\t:  0.16436505317687988 \t2.3733882904052734 \t0.7485256195068359\n",
      "Batch:\t14 /4000\t:  0.16758966445922852 \t2.5422470569610596 \t1.6015052795410156\n",
      "Batch:\t15 /4000\t:  0.16472244262695312 \t2.7084782123565674 \t0.7882595062255859\n",
      "Batch:\t16 /4000\t:  0.16550803184509277 \t2.8756494522094727 \t3.6079816818237305\n",
      "Batch:\t17 /4000\t:  0.10125446319580078 \t2.9779531955718994 \t0.0\n",
      "Time: 4.971222639083862\t Loss: 36.17171239852905\t Valdacc: 0.42\t Testacc: 0.395\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  48 / 200\n",
      "Batch:\t0 /4000\t:  0.16034889221191406 \t0.16223430633544922 \t3.7820241451263428\n",
      "Batch:\t1 /4000\t:  0.1796717643737793 \t0.3432018756866455 \t2.1867833137512207\n",
      "Batch:\t2 /4000\t:  0.16577649116516113 \t0.5102782249450684 \t0.2805004119873047\n",
      "Batch:\t3 /4000\t:  0.16518259048461914 \t0.6771214008331299 \t4.280253887176514\n",
      "Batch:\t4 /4000\t:  0.17511701583862305 \t0.8539979457855225 \t2.8452248573303223\n",
      "Batch:\t5 /4000\t:  0.16030526161193848 \t1.0153567790985107 \t3.669687271118164\n",
      "Batch:\t6 /4000\t:  0.16730523109436035 \t1.1840362548828125 \t3.907848358154297\n",
      "Batch:\t7 /4000\t:  0.18325257301330566 \t1.3762688636779785 \t1.3878259658813477\n",
      "Batch:\t8 /4000\t:  0.1664106845855713 \t1.544407606124878 \t4.909111976623535\n",
      "Batch:\t9 /4000\t:  0.16484546661376953 \t1.7106943130493164 \t1.72139310836792\n",
      "Batch:\t10 /4000\t:  0.17094922065734863 \t1.8833184242248535 \t0.853630542755127\n",
      "Batch:\t11 /4000\t:  0.16643238067626953 \t2.0510146617889404 \t9.140633583068848\n",
      "Batch:\t12 /4000\t:  0.16069698333740234 \t2.2133026123046875 \t7.081936836242676\n",
      "Batch:\t13 /4000\t:  0.16033172607421875 \t2.3751869201660156 \t2.5632309913635254\n",
      "Batch:\t14 /4000\t:  0.1608872413635254 \t2.537468194961548 \t1.3022956848144531\n",
      "Batch:\t15 /4000\t:  0.16833114624023438 \t2.7072668075561523 \t1.4344682693481445\n",
      "Batch:\t16 /4000\t:  0.17154502868652344 \t2.8893728256225586 \t4.193636417388916\n",
      "Batch:\t17 /4000\t:  0.09965896606445312 \t2.9901328086853027 \t0.0\n",
      "Time: 4.860910177230835\t Loss: 55.54048562049866\t Valdacc: 0.41\t Testacc: 0.395\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  49 / 200\n",
      "Batch:\t0 /4000\t:  0.16185879707336426 \t0.1643047332763672 \t4.087550640106201\n",
      "Batch:\t1 /4000\t:  0.16487431526184082 \t0.33110594749450684 \t1.0841352939605713\n",
      "Batch:\t2 /4000\t:  0.16054582595825195 \t0.4934122562408447 \t2.6350107192993164\n",
      "Batch:\t3 /4000\t:  0.16114449501037598 \t0.6562545299530029 \t3.375041961669922\n",
      "Batch:\t4 /4000\t:  0.20055842399597168 \t0.8589403629302979 \t1.91455078125\n",
      "Batch:\t5 /4000\t:  0.16675758361816406 \t1.0270183086395264 \t0.40337181091308594\n",
      "Batch:\t6 /4000\t:  0.1639697551727295 \t1.1923718452453613 \t1.0833148956298828\n",
      "Batch:\t7 /4000\t:  0.1669480800628662 \t1.3610074520111084 \t2.6693873405456543\n",
      "Batch:\t8 /4000\t:  0.16693449020385742 \t1.5296902656555176 \t0.22389888763427734\n",
      "Batch:\t9 /4000\t:  0.1633167266845703 \t1.6950724124908447 \t1.0197181701660156\n",
      "Batch:\t10 /4000\t:  0.16957616806030273 \t1.866225242614746 \t0.42856502532958984\n",
      "Batch:\t11 /4000\t:  0.16070938110351562 \t2.0283946990966797 \t1.3137931823730469\n",
      "Batch:\t12 /4000\t:  0.16460561752319336 \t2.194675922393799 \t0.9837818145751953\n",
      "Batch:\t13 /4000\t:  0.1704096794128418 \t2.366718292236328 \t1.9666757583618164\n",
      "Batch:\t14 /4000\t:  0.16938447952270508 \t2.537431478500366 \t0.6629538536071777\n",
      "Batch:\t15 /4000\t:  0.16689586639404297 \t2.7062768936157227 \t0.0\n",
      "Batch:\t16 /4000\t:  0.20499014854431152 \t2.9128201007843018 \t0.0\n",
      "Batch:\t17 /4000\t:  0.10774540901184082 \t3.02180814743042 \t1.6653213500976562\n",
      "Time: 4.921994924545288\t Loss: 25.51707148551941\t Valdacc: 0.4\t Testacc: 0.37\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  50 / 200\n",
      "Batch:\t0 /4000\t:  0.1606297492980957 \t0.16352033615112305 \t1.9519081115722656\n",
      "Batch:\t1 /4000\t:  0.16945505142211914 \t0.3348071575164795 \t0.4762840270996094\n",
      "Batch:\t2 /4000\t:  0.16867971420288086 \t0.5049431324005127 \t3.2017602920532227\n",
      "Batch:\t3 /4000\t:  0.18857693672180176 \t0.6950321197509766 \t4.725451469421387\n",
      "Batch:\t4 /4000\t:  0.16130471229553223 \t0.8577573299407959 \t1.2484827041625977\n",
      "Batch:\t5 /4000\t:  0.159515380859375 \t1.0186200141906738 \t0.0\n",
      "Batch:\t6 /4000\t:  0.16763901710510254 \t1.1877691745758057 \t0.0\n",
      "Batch:\t7 /4000\t:  0.1609811782836914 \t1.3500068187713623 \t0.7301864624023438\n",
      "Batch:\t8 /4000\t:  0.16625356674194336 \t1.518317699432373 \t4.556155204772949\n",
      "Batch:\t9 /4000\t:  0.17202067375183105 \t1.6920058727264404 \t1.0559043884277344\n",
      "Batch:\t10 /4000\t:  0.1638963222503662 \t1.8574366569519043 \t3.012932777404785\n",
      "Batch:\t11 /4000\t:  0.1638014316558838 \t2.023470401763916 \t0.2653923034667969\n",
      "Batch:\t12 /4000\t:  0.16048717498779297 \t2.1856257915496826 \t0.0\n",
      "Batch:\t13 /4000\t:  0.16914820671081543 \t2.3561837673187256 \t1.6608185768127441\n",
      "Batch:\t14 /4000\t:  0.18378996849060059 \t2.54146409034729 \t2.2078194618225098\n",
      "Batch:\t15 /4000\t:  0.20528340339660645 \t2.7481696605682373 \t0.9192790985107422\n",
      "Batch:\t16 /4000\t:  0.1641550064086914 \t2.9139697551727295 \t0.30584716796875\n",
      "Batch:\t17 /4000\t:  0.10630416870117188 \t3.021810531616211 \t0.5198402404785156\n",
      "Time: 4.923997640609741\t Loss: 26.838062286376953\t Valdacc: 0.41\t Testacc: 0.365\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  51 / 200\n",
      "Batch:\t0 /4000\t:  0.16698169708251953 \t0.16797280311584473 \t0.9484913349151611\n",
      "Batch:\t1 /4000\t:  0.16385984420776367 \t0.3332359790802002 \t4.958703994750977\n",
      "Batch:\t2 /4000\t:  0.16937875747680664 \t0.5038788318634033 \t0.3754119873046875\n",
      "Batch:\t3 /4000\t:  0.17819452285766602 \t0.6839160919189453 \t6.901460647583008\n",
      "Batch:\t4 /4000\t:  0.16660833358764648 \t0.8521971702575684 \t0.019956588745117188\n",
      "Batch:\t5 /4000\t:  0.16485595703125 \t1.0185081958770752 \t0.5777745246887207\n",
      "Batch:\t6 /4000\t:  0.16586637496948242 \t1.1859581470489502 \t0.5190210342407227\n",
      "Batch:\t7 /4000\t:  0.160675048828125 \t1.347942590713501 \t0.3148221969604492\n",
      "Batch:\t8 /4000\t:  0.1693403720855713 \t1.5187220573425293 \t5.638998985290527\n",
      "Batch:\t9 /4000\t:  0.16129302978515625 \t1.6811633110046387 \t0.3430767059326172\n",
      "Batch:\t10 /4000\t:  0.16665029525756836 \t1.8491883277893066 \t0.17206287384033203\n",
      "Batch:\t11 /4000\t:  0.16397356986999512 \t2.0144622325897217 \t2.34517240524292\n",
      "Batch:\t12 /4000\t:  0.16229009628295898 \t2.1785218715667725 \t3.4385628700256348\n",
      "Batch:\t13 /4000\t:  0.16118979454040527 \t2.341052532196045 \t0.9722194671630859\n",
      "Batch:\t14 /4000\t:  0.16784334182739258 \t2.5108556747436523 \t1.7301750183105469\n",
      "Batch:\t15 /4000\t:  0.19954776763916016 \t2.713778018951416 \t2.249744415283203\n",
      "Batch:\t16 /4000\t:  0.16206979751586914 \t2.877763032913208 \t1.666823387145996\n",
      "Batch:\t17 /4000\t:  0.11980557441711426 \t3.0082924365997314 \t2.041341781616211\n",
      "Time: 4.972479343414307\t Loss: 35.21382021903992\t Valdacc: 0.41\t Testacc: 0.4\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  52 / 200\n",
      "Batch:\t0 /4000\t:  0.1782057285308838 \t0.17938995361328125 \t0.017866134643554688\n",
      "Batch:\t1 /4000\t:  0.16771507263183594 \t0.34882664680480957 \t1.3099536895751953\n",
      "Batch:\t2 /4000\t:  0.1726830005645752 \t0.5231735706329346 \t1.7759342193603516\n",
      "Batch:\t3 /4000\t:  0.1600792407989502 \t0.6845865249633789 \t1.0938072204589844\n",
      "Batch:\t4 /4000\t:  0.1601886749267578 \t0.84637451171875 \t3.9447121620178223\n",
      "Batch:\t5 /4000\t:  0.17440128326416016 \t1.0223183631896973 \t4.259552478790283\n",
      "Batch:\t6 /4000\t:  0.20328497886657715 \t1.226982831954956 \t0.9389324188232422\n",
      "Batch:\t7 /4000\t:  0.20325398445129395 \t1.4318549633026123 \t1.0458755493164062\n",
      "Batch:\t8 /4000\t:  0.16017460823059082 \t1.5935657024383545 \t0.8865652084350586\n",
      "Batch:\t9 /4000\t:  0.16514086723327637 \t1.7600493431091309 \t1.3464365005493164\n",
      "Batch:\t10 /4000\t:  0.17034363746643066 \t1.9320380687713623 \t0.16793441772460938\n",
      "Batch:\t11 /4000\t:  0.16144919395446777 \t2.0948123931884766 \t2.033018112182617\n",
      "Batch:\t12 /4000\t:  0.16099309921264648 \t2.2571980953216553 \t1.9047393798828125\n",
      "Batch:\t13 /4000\t:  0.2065284252166748 \t2.4655001163482666 \t2.3063571453094482\n",
      "Batch:\t14 /4000\t:  0.16409993171691895 \t2.63108229637146 \t3.8041563034057617\n",
      "Batch:\t15 /4000\t:  0.18339800834655762 \t2.8164615631103516 \t1.105905532836914\n",
      "Batch:\t16 /4000\t:  0.17142820358276367 \t2.996919870376587 \t2.952986717224121\n",
      "Batch:\t17 /4000\t:  0.10022425651550293 \t3.098475456237793 \t0.0\n",
      "Time: 5.006223917007446\t Loss: 30.8947331905365\t Valdacc: 0.42\t Testacc: 0.4\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  53 / 200\n",
      "Batch:\t0 /4000\t:  0.16393446922302246 \t0.16582775115966797 \t0.5753803253173828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t1 /4000\t:  0.16176462173461914 \t0.32903051376342773 \t5.340479373931885\n",
      "Batch:\t2 /4000\t:  0.16068315505981445 \t0.4912688732147217 \t1.5116796493530273\n",
      "Batch:\t3 /4000\t:  0.1631009578704834 \t0.6559112071990967 \t1.5970611572265625\n",
      "Batch:\t4 /4000\t:  0.1669907569885254 \t0.8243494033813477 \t2.767627716064453\n",
      "Batch:\t5 /4000\t:  0.16442251205444336 \t0.9923639297485352 \t1.3481464385986328\n",
      "Batch:\t6 /4000\t:  0.17589306831359863 \t1.1697442531585693 \t0.0\n",
      "Batch:\t7 /4000\t:  0.17781543731689453 \t1.3492627143859863 \t4.321112632751465\n",
      "Batch:\t8 /4000\t:  0.16067838668823242 \t1.5117428302764893 \t0.8643827438354492\n",
      "Batch:\t9 /4000\t:  0.15746760368347168 \t1.6703312397003174 \t0.0\n",
      "Batch:\t10 /4000\t:  0.1692357063293457 \t1.841230869293213 \t1.9308481216430664\n",
      "Batch:\t11 /4000\t:  0.16415929794311523 \t2.007765293121338 \t2.484889030456543\n",
      "Batch:\t12 /4000\t:  0.1653296947479248 \t2.1744046211242676 \t0.4474325180053711\n",
      "Batch:\t13 /4000\t:  0.16317391395568848 \t2.339186191558838 \t1.7900214195251465\n",
      "Batch:\t14 /4000\t:  0.1660149097442627 \t2.5069427490234375 \t0.7686271667480469\n",
      "Batch:\t15 /4000\t:  0.16612792015075684 \t2.6745071411132812 \t1.388716220855713\n",
      "Batch:\t16 /4000\t:  0.17804813385009766 \t2.8548078536987305 \t1.6391520500183105\n",
      "Batch:\t17 /4000\t:  0.10183310508728027 \t2.9578545093536377 \t0.0\n",
      "Time: 4.845430135726929\t Loss: 28.775556564331055\t Valdacc: 0.41\t Testacc: 0.4\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  54 / 200\n",
      "Batch:\t0 /4000\t:  0.1664435863494873 \t0.16739821434020996 \t1.4731078147888184\n",
      "Batch:\t1 /4000\t:  0.19374728202819824 \t0.3630683422088623 \t0.7476401329040527\n",
      "Batch:\t2 /4000\t:  0.16368913650512695 \t0.5281813144683838 \t2.2289600372314453\n",
      "Batch:\t3 /4000\t:  0.16141676902770996 \t0.6911277770996094 \t5.068809509277344\n",
      "Batch:\t4 /4000\t:  0.175307035446167 \t0.8682901859283447 \t1.0968961715698242\n",
      "Batch:\t5 /4000\t:  0.19233012199401855 \t1.062007188796997 \t1.4461345672607422\n",
      "Batch:\t6 /4000\t:  0.17880034446716309 \t1.242443323135376 \t3.580829620361328\n",
      "Batch:\t7 /4000\t:  0.1750164031982422 \t1.419076681137085 \t1.584151268005371\n",
      "Batch:\t8 /4000\t:  0.1605234146118164 \t1.5810716152191162 \t0.3282160758972168\n",
      "Batch:\t9 /4000\t:  0.16076397895812988 \t1.7434759140014648 \t3.402695417404175\n",
      "Batch:\t10 /4000\t:  0.16048002243041992 \t1.9056529998779297 \t0.0482330322265625\n",
      "Batch:\t11 /4000\t:  0.20492219924926758 \t2.111954927444458 \t2.3871383666992188\n",
      "Batch:\t12 /4000\t:  0.16258645057678223 \t2.276325225830078 \t0.0\n",
      "Batch:\t13 /4000\t:  0.16455864906311035 \t2.442405939102173 \t1.8014755249023438\n",
      "Batch:\t14 /4000\t:  0.1654191017150879 \t2.609854221343994 \t2.7364683151245117\n",
      "Batch:\t15 /4000\t:  0.1672687530517578 \t2.778576135635376 \t0.45343589782714844\n",
      "Batch:\t16 /4000\t:  0.203995943069458 \t2.984696626663208 \t4.492344856262207\n",
      "Batch:\t17 /4000\t:  0.10470056533813477 \t3.0908968448638916 \t0.7817668914794922\n",
      "Time: 5.012987375259399\t Loss: 33.6583034992218\t Valdacc: 0.42\t Testacc: 0.425\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  55 / 200\n",
      "Batch:\t0 /4000\t:  0.1664276123046875 \t0.16730928421020508 \t3.0625553131103516\n",
      "Batch:\t1 /4000\t:  0.1674187183380127 \t0.33620595932006836 \t0.5249238014221191\n",
      "Batch:\t2 /4000\t:  0.162001371383667 \t0.4996471405029297 \t1.6370534896850586\n",
      "Batch:\t3 /4000\t:  0.16061925888061523 \t0.6616559028625488 \t3.7975692749023438\n",
      "Batch:\t4 /4000\t:  0.16508078575134277 \t0.8286752700805664 \t0.46294593811035156\n",
      "Batch:\t5 /4000\t:  0.16840791702270508 \t0.9997055530548096 \t5.908853530883789\n",
      "Batch:\t6 /4000\t:  0.16659784317016602 \t1.1683189868927002 \t1.7214241027832031\n",
      "Batch:\t7 /4000\t:  0.1675105094909668 \t1.3371844291687012 \t2.091978073120117\n",
      "Batch:\t8 /4000\t:  0.17032670974731445 \t1.5092670917510986 \t0.6080036163330078\n",
      "Batch:\t9 /4000\t:  0.18083715438842773 \t1.6915004253387451 \t4.03192138671875\n",
      "Batch:\t10 /4000\t:  0.1725924015045166 \t1.8654983043670654 \t3.6467981338500977\n",
      "Batch:\t11 /4000\t:  0.17033815383911133 \t2.037491798400879 \t0.38452816009521484\n",
      "Batch:\t12 /4000\t:  0.16689324378967285 \t2.205583095550537 \t3.09991455078125\n",
      "Batch:\t13 /4000\t:  0.16372179985046387 \t2.3713765144348145 \t3.618215560913086\n",
      "Batch:\t14 /4000\t:  0.17019414901733398 \t2.543281316757202 \t3.688683032989502\n",
      "Batch:\t15 /4000\t:  0.16880297660827637 \t2.7133195400238037 \t1.3155555725097656\n",
      "Batch:\t16 /4000\t:  0.1774582862854004 \t2.892408847808838 \t1.2637786865234375\n",
      "Batch:\t17 /4000\t:  0.11677289009094238 \t3.0108144283294678 \t0.0\n",
      "Time: 4.9212541580200195\t Loss: 40.864702224731445\t Valdacc: 0.43\t Testacc: 0.41\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  56 / 200\n",
      "Batch:\t0 /4000\t:  0.16386818885803223 \t0.16503596305847168 \t3.660830497741699\n",
      "Batch:\t1 /4000\t:  0.16507506370544434 \t0.33161497116088867 \t1.3858423233032227\n",
      "Batch:\t2 /4000\t:  0.20092082023620605 \t0.534991979598999 \t3.226883888244629\n",
      "Batch:\t3 /4000\t:  0.1662611961364746 \t0.7025763988494873 \t0.1608142852783203\n",
      "Batch:\t4 /4000\t:  0.16170167922973633 \t0.8657443523406982 \t0.0\n",
      "Batch:\t5 /4000\t:  0.22618627548217773 \t1.0959482192993164 \t1.8699865341186523\n",
      "Batch:\t6 /4000\t:  0.16462373733520508 \t1.2620124816894531 \t2.310903549194336\n",
      "Batch:\t7 /4000\t:  0.16530966758728027 \t1.4290471076965332 \t4.938976287841797\n",
      "Batch:\t8 /4000\t:  0.16440486907958984 \t1.595353603363037 \t0.13034629821777344\n",
      "Batch:\t9 /4000\t:  0.16109085083007812 \t1.7577273845672607 \t3.08048152923584\n",
      "Batch:\t10 /4000\t:  0.16155195236206055 \t1.9211359024047852 \t3.019219160079956\n",
      "Batch:\t11 /4000\t:  0.1615126132965088 \t2.0844571590423584 \t7.278080940246582\n",
      "Batch:\t12 /4000\t:  0.1651747226715088 \t2.2512779235839844 \t0.7599716186523438\n",
      "Batch:\t13 /4000\t:  0.16079950332641602 \t2.414196491241455 \t2.113189697265625\n",
      "Batch:\t14 /4000\t:  0.16621971130371094 \t2.5816869735717773 \t0.3617897033691406\n",
      "Batch:\t15 /4000\t:  0.20461416244506836 \t2.7877957820892334 \t0.985107421875\n",
      "Batch:\t16 /4000\t:  0.1638345718383789 \t2.9533820152282715 \t5.746059417724609\n",
      "Batch:\t17 /4000\t:  0.10319662094116211 \t3.057737350463867 \t4.164121627807617\n",
      "Time: 4.9878013134002686\t Loss: 45.192604780197144\t Valdacc: 0.41\t Testacc: 0.39\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.425\n",
      "\n",
      "Epoch:  57 / 200\n",
      "Batch:\t0 /4000\t:  0.1602320671081543 \t0.16258001327514648 \t1.3906240463256836\n",
      "Batch:\t1 /4000\t:  0.162994384765625 \t0.3267059326171875 \t5.343146324157715\n",
      "Batch:\t2 /4000\t:  0.16350078582763672 \t0.49182963371276855 \t2.280954360961914\n",
      "Batch:\t3 /4000\t:  0.1672360897064209 \t0.661139965057373 \t0.14135265350341797\n",
      "Batch:\t4 /4000\t:  0.16872048377990723 \t0.8311655521392822 \t0.10563850402832031\n",
      "Batch:\t5 /4000\t:  0.18169021606445312 \t1.0141630172729492 \t1.0209760665893555\n",
      "Batch:\t6 /4000\t:  0.1616368293762207 \t1.1778392791748047 \t0.5222928524017334\n",
      "Batch:\t7 /4000\t:  0.16189336776733398 \t1.3416388034820557 \t2.0317554473876953\n",
      "Batch:\t8 /4000\t:  0.16053104400634766 \t1.5032556056976318 \t1.644174575805664\n",
      "Batch:\t9 /4000\t:  0.1585402488708496 \t1.66316819190979 \t0.0\n",
      "Batch:\t10 /4000\t:  0.18068480491638184 \t1.845306634902954 \t0.49779605865478516\n",
      "Batch:\t11 /4000\t:  0.15806913375854492 \t2.0134477615356445 \t0.0\n",
      "Batch:\t12 /4000\t:  0.16317248344421387 \t2.1776256561279297 \t0.310516357421875\n",
      "Batch:\t13 /4000\t:  0.160719633102417 \t2.3400800228118896 \t1.1050310134887695\n",
      "Batch:\t14 /4000\t:  0.16611862182617188 \t2.5077505111694336 \t1.0497255325317383\n",
      "Batch:\t15 /4000\t:  0.16452336311340332 \t2.6740753650665283 \t0.1769256591796875\n",
      "Batch:\t16 /4000\t:  0.17645883560180664 \t2.8526036739349365 \t0.0\n",
      "Batch:\t17 /4000\t:  0.1015019416809082 \t2.9552929401397705 \t0.12036705017089844\n",
      "Time: 4.852461338043213\t Loss: 17.741276502609253\t Valdacc: 0.37\t Testacc: 0.465\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  58 / 200\n",
      "Batch:\t0 /4000\t:  0.16538238525390625 \t0.1665658950805664 \t2.5182619094848633\n",
      "Batch:\t1 /4000\t:  0.16765213012695312 \t0.33597683906555176 \t2.064085006713867\n",
      "Batch:\t2 /4000\t:  0.1835925579071045 \t0.5212690830230713 \t3.356353759765625\n",
      "Batch:\t3 /4000\t:  0.1647484302520752 \t0.6876511573791504 \t3.040860176086426\n",
      "Batch:\t4 /4000\t:  0.16417670249938965 \t0.8531250953674316 \t0.9322633743286133\n",
      "Batch:\t5 /4000\t:  0.16116547584533691 \t1.0156910419464111 \t2.241757392883301\n",
      "Batch:\t6 /4000\t:  0.16697239875793457 \t1.1841802597045898 \t0.9321832656860352\n",
      "Batch:\t7 /4000\t:  0.16360831260681152 \t1.3492422103881836 \t4.160935401916504\n",
      "Batch:\t8 /4000\t:  0.18352794647216797 \t1.5347537994384766 \t0.6605615615844727\n",
      "Batch:\t9 /4000\t:  0.16226649284362793 \t1.6981475353240967 \t4.749908924102783\n",
      "Batch:\t10 /4000\t:  0.166276216506958 \t1.8659818172454834 \t3.123730182647705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t11 /4000\t:  0.17817473411560059 \t2.0458860397338867 \t2.691580295562744\n",
      "Batch:\t12 /4000\t:  0.16380000114440918 \t2.211373805999756 \t3.475550651550293\n",
      "Batch:\t13 /4000\t:  0.1695253849029541 \t2.3827102184295654 \t0.47338104248046875\n",
      "Batch:\t14 /4000\t:  0.17872357368469238 \t2.5636775493621826 \t2.7096447944641113\n",
      "Batch:\t15 /4000\t:  0.16301369667053223 \t2.728480815887451 \t2.1356611251831055\n",
      "Batch:\t16 /4000\t:  0.1674952507019043 \t2.897564649581909 \t0.965388298034668\n",
      "Batch:\t17 /4000\t:  0.12258505821228027 \t3.0213351249694824 \t5.05894660949707\n",
      "Time: 4.906586647033691\t Loss: 45.291053771972656\t Valdacc: 0.34\t Testacc: 0.41\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  59 / 200\n",
      "Batch:\t0 /4000\t:  0.16610407829284668 \t0.16695928573608398 \t2.3552608489990234\n",
      "Batch:\t1 /4000\t:  0.16081666946411133 \t0.329085111618042 \t0.4718036651611328\n",
      "Batch:\t2 /4000\t:  0.19705581665039062 \t0.527916431427002 \t3.8188891410827637\n",
      "Batch:\t3 /4000\t:  0.1623067855834961 \t0.6916215419769287 \t0.0\n",
      "Batch:\t4 /4000\t:  0.1647651195526123 \t0.8579108715057373 \t2.7537050247192383\n",
      "Batch:\t5 /4000\t:  0.18416833877563477 \t1.0435941219329834 \t1.0872316360473633\n",
      "Batch:\t6 /4000\t:  0.17570710182189941 \t1.2213459014892578 \t3.0001144409179688\n",
      "Batch:\t7 /4000\t:  0.16684961318969727 \t1.3899023532867432 \t1.0193076133728027\n",
      "Batch:\t8 /4000\t:  0.18094968795776367 \t1.5725724697113037 \t1.2743816375732422\n",
      "Batch:\t9 /4000\t:  0.20638251304626465 \t1.7806365489959717 \t0.10231208801269531\n",
      "Batch:\t10 /4000\t:  0.18951201438903809 \t1.9720971584320068 \t3.920563220977783\n",
      "Batch:\t11 /4000\t:  0.16755366325378418 \t2.140986680984497 \t0.7084579467773438\n",
      "Batch:\t12 /4000\t:  0.17798328399658203 \t2.320347309112549 \t2.06276798248291\n",
      "Batch:\t13 /4000\t:  0.20399713516235352 \t2.5259907245635986 \t0.0\n",
      "Batch:\t14 /4000\t:  0.16162872314453125 \t2.689122438430786 \t1.0024547576904297\n",
      "Batch:\t15 /4000\t:  0.16475558280944824 \t2.855635404586792 \t1.7475881576538086\n",
      "Batch:\t16 /4000\t:  0.177764892578125 \t3.0373990535736084 \t0.04210853576660156\n",
      "Batch:\t17 /4000\t:  0.10067343711853027 \t3.139686346054077 \t2.0738162994384766\n",
      "Time: 5.0403220653533936\t Loss: 27.440762996673584\t Valdacc: 0.38\t Testacc: 0.44\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  60 / 200\n",
      "Batch:\t0 /4000\t:  0.1778402328491211 \t0.17985224723815918 \t0.7179574966430664\n",
      "Batch:\t1 /4000\t:  0.16158008575439453 \t0.34299445152282715 \t4.192041397094727\n",
      "Batch:\t2 /4000\t:  0.181640625 \t0.5288395881652832 \t1.6535882949829102\n",
      "Batch:\t3 /4000\t:  0.16940879821777344 \t0.6995744705200195 \t0.9506702423095703\n",
      "Batch:\t4 /4000\t:  0.16945719718933105 \t0.870525598526001 \t6.540830612182617\n",
      "Batch:\t5 /4000\t:  0.1688098907470703 \t1.0407047271728516 \t0.13969707489013672\n",
      "Batch:\t6 /4000\t:  0.16193866729736328 \t1.2041964530944824 \t0.5013465881347656\n",
      "Batch:\t7 /4000\t:  0.16396832466125488 \t1.3695971965789795 \t2.06417179107666\n",
      "Batch:\t8 /4000\t:  0.16289806365966797 \t1.5339865684509277 \t0.26857566833496094\n",
      "Batch:\t9 /4000\t:  0.16222929954528809 \t1.6977694034576416 \t0.31259822845458984\n",
      "Batch:\t10 /4000\t:  0.16396450996398926 \t1.86311674118042 \t0.02566814422607422\n",
      "Batch:\t11 /4000\t:  0.17641282081604004 \t2.0462608337402344 \t5.047274112701416\n",
      "Batch:\t12 /4000\t:  0.16666102409362793 \t2.214240074157715 \t1.9007596969604492\n",
      "Batch:\t13 /4000\t:  0.16372179985046387 \t2.3791205883026123 \t1.3829717636108398\n",
      "Batch:\t14 /4000\t:  0.17828583717346191 \t2.559490203857422 \t1.364008903503418\n",
      "Batch:\t15 /4000\t:  0.16417884826660156 \t2.725210189819336 \t0.797459602355957\n",
      "Batch:\t16 /4000\t:  0.1849532127380371 \t2.9118103981018066 \t2.8094100952148438\n",
      "Batch:\t17 /4000\t:  0.10139107704162598 \t3.0145294666290283 \t0.6948585510253906\n",
      "Time: 4.9077842235565186\t Loss: 31.363888263702393\t Valdacc: 0.35\t Testacc: 0.415\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  61 / 200\n",
      "Batch:\t0 /4000\t:  0.16396808624267578 \t0.16490674018859863 \t1.1464767456054688\n",
      "Batch:\t1 /4000\t:  0.17943596839904785 \t0.3469686508178711 \t0.0\n",
      "Batch:\t2 /4000\t:  0.16280269622802734 \t0.5110745429992676 \t0.4761676788330078\n",
      "Batch:\t3 /4000\t:  0.16521573066711426 \t0.6775741577148438 \t1.8467364311218262\n",
      "Batch:\t4 /4000\t:  0.1675584316253662 \t0.8465569019317627 \t3.8517160415649414\n",
      "Batch:\t5 /4000\t:  0.1634688377380371 \t1.0113065242767334 \t6.285062313079834\n",
      "Batch:\t6 /4000\t:  0.1624140739440918 \t1.1750209331512451 \t1.0606517791748047\n",
      "Batch:\t7 /4000\t:  0.16634583473205566 \t1.3429698944091797 \t0.3468494415283203\n",
      "Batch:\t8 /4000\t:  0.16613531112670898 \t1.5104658603668213 \t4.498805999755859\n",
      "Batch:\t9 /4000\t:  0.18196392059326172 \t1.6937568187713623 \t1.7209739685058594\n",
      "Batch:\t10 /4000\t:  0.18076419830322266 \t1.8761720657348633 \t0.952021598815918\n",
      "Batch:\t11 /4000\t:  0.1609940528869629 \t2.0387563705444336 \t0.20989990234375\n",
      "Batch:\t12 /4000\t:  0.16138482093811035 \t2.2015419006347656 \t2.4847888946533203\n",
      "Batch:\t13 /4000\t:  0.16234660148620605 \t2.365118980407715 \t2.5065460205078125\n",
      "Batch:\t14 /4000\t:  0.16195344924926758 \t2.528553009033203 \t1.8800029754638672\n",
      "Batch:\t15 /4000\t:  0.16985106468200684 \t2.70041823387146 \t1.383946418762207\n",
      "Batch:\t16 /4000\t:  0.16336989402770996 \t2.8654122352600098 \t1.5642032623291016\n",
      "Batch:\t17 /4000\t:  0.1005702018737793 \t2.9670653343200684 \t0.0\n",
      "Time: 4.889912366867065\t Loss: 32.2148494720459\t Valdacc: 0.44\t Testacc: 0.43\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  62 / 200\n",
      "Batch:\t0 /4000\t:  0.16666841506958008 \t0.16923785209655762 \t2.739826202392578\n",
      "Batch:\t1 /4000\t:  0.16642284393310547 \t0.33748555183410645 \t2.2725887298583984\n",
      "Batch:\t2 /4000\t:  0.16348695755004883 \t0.5025670528411865 \t0.9285821914672852\n",
      "Batch:\t3 /4000\t:  0.2067556381225586 \t0.7108147144317627 \t2.192422389984131\n",
      "Batch:\t4 /4000\t:  0.18141674995422363 \t0.893852710723877 \t1.1629886627197266\n",
      "Batch:\t5 /4000\t:  0.2079780101776123 \t1.1029250621795654 \t1.09716796875\n",
      "Batch:\t6 /4000\t:  0.1602933406829834 \t1.2648539543151855 \t0.36675548553466797\n",
      "Batch:\t7 /4000\t:  0.16354703903198242 \t1.4298858642578125 \t2.7846438884735107\n",
      "Batch:\t8 /4000\t:  0.16599345207214355 \t1.5971462726593018 \t3.1997900009155273\n",
      "Batch:\t9 /4000\t:  0.16466569900512695 \t1.7639808654785156 \t2.587061882019043\n",
      "Batch:\t10 /4000\t:  0.16042327880859375 \t1.926072597503662 \t1.170975685119629\n",
      "Batch:\t11 /4000\t:  0.16399168968200684 \t2.0911128520965576 \t0.4682340621948242\n",
      "Batch:\t12 /4000\t:  0.1601579189300537 \t2.2525761127471924 \t0.006335258483886719\n",
      "Batch:\t13 /4000\t:  0.15988922119140625 \t2.4137167930603027 \t0.593775749206543\n",
      "Batch:\t14 /4000\t:  0.16034531593322754 \t2.5753519535064697 \t2.4141454696655273\n",
      "Batch:\t15 /4000\t:  0.16063761711120605 \t2.7376279830932617 \t1.6155376434326172\n",
      "Batch:\t16 /4000\t:  0.1809372901916504 \t2.9200942516326904 \t1.5310077667236328\n",
      "Batch:\t17 /4000\t:  0.10089492797851562 \t3.0223355293273926 \t0.0\n",
      "Time: 4.970695734024048\t Loss: 27.13183903694153\t Valdacc: 0.45\t Testacc: 0.43\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  63 / 200\n",
      "Batch:\t0 /4000\t:  0.1616959571838379 \t0.16439199447631836 \t1.584625244140625\n",
      "Batch:\t1 /4000\t:  0.16271376609802246 \t0.32900214195251465 \t3.906087875366211\n",
      "Batch:\t2 /4000\t:  0.16807794570922852 \t0.49852967262268066 \t0.8907556533813477\n",
      "Batch:\t3 /4000\t:  0.1639237403869629 \t0.6648209095001221 \t0.07028961181640625\n",
      "Batch:\t4 /4000\t:  0.16809749603271484 \t0.834439754486084 \t0.8888082504272461\n",
      "Batch:\t5 /4000\t:  0.1636359691619873 \t1.0002880096435547 \t2.2160892486572266\n",
      "Batch:\t6 /4000\t:  0.1613478660583496 \t1.163095474243164 \t4.277297496795654\n",
      "Batch:\t7 /4000\t:  0.20421099662780762 \t1.368675708770752 \t0.2546119689941406\n",
      "Batch:\t8 /4000\t:  0.16425418853759766 \t1.534358263015747 \t0.32468318939208984\n",
      "Batch:\t9 /4000\t:  0.18247556686401367 \t1.7187683582305908 \t0.6045989990234375\n",
      "Batch:\t10 /4000\t:  0.1674959659576416 \t1.8883934020996094 \t3.5739545822143555\n",
      "Batch:\t11 /4000\t:  0.16426634788513184 \t2.054094076156616 \t3.0222225189208984\n",
      "Batch:\t12 /4000\t:  0.16286063194274902 \t2.218618392944336 \t6.049930095672607\n",
      "Batch:\t13 /4000\t:  0.1637403964996338 \t2.3838517665863037 \t3.6381964683532715\n",
      "Batch:\t14 /4000\t:  0.15961742401123047 \t2.5450572967529297 \t0.0\n",
      "Batch:\t15 /4000\t:  0.16822218894958496 \t2.7154428958892822 \t1.2903566360473633\n",
      "Batch:\t16 /4000\t:  0.1641395092010498 \t2.881049633026123 \t0.021671295166015625\n",
      "Batch:\t17 /4000\t:  0.10386490821838379 \t2.9861226081848145 \t0.15988922119140625\n",
      "Time: 4.94351601600647\t Loss: 32.7740683555603\t Valdacc: 0.38\t Testacc: 0.425\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  64 / 200\n",
      "Batch:\t0 /4000\t:  0.16151213645935059 \t0.1635587215423584 \t3.277334213256836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t1 /4000\t:  0.1672060489654541 \t0.33245396614074707 \t0.48220229148864746\n",
      "Batch:\t2 /4000\t:  0.1623978614807129 \t0.49634647369384766 \t0.10868167877197266\n",
      "Batch:\t3 /4000\t:  0.15978598594665527 \t0.6575946807861328 \t0.0\n",
      "Batch:\t4 /4000\t:  0.1831812858581543 \t0.8425700664520264 \t0.6282820701599121\n",
      "Batch:\t5 /4000\t:  0.1770620346069336 \t1.021235704421997 \t0.16247844696044922\n",
      "Batch:\t6 /4000\t:  0.16541671752929688 \t1.188232183456421 \t0.9154214859008789\n",
      "Batch:\t7 /4000\t:  0.15923690795898438 \t1.3488414287567139 \t0.0\n",
      "Batch:\t8 /4000\t:  0.17549371719360352 \t1.5257763862609863 \t0.4789905548095703\n",
      "Batch:\t9 /4000\t:  0.16417407989501953 \t1.6917316913604736 \t1.7444233894348145\n",
      "Batch:\t10 /4000\t:  0.1702878475189209 \t1.8634302616119385 \t0.8754549026489258\n",
      "Batch:\t11 /4000\t:  0.16465449333190918 \t2.0297417640686035 \t2.1921820640563965\n",
      "Batch:\t12 /4000\t:  0.1662311553955078 \t2.197383165359497 \t1.0454072952270508\n",
      "Batch:\t13 /4000\t:  0.1640937328338623 \t2.3628246784210205 \t0.07871055603027344\n",
      "Batch:\t14 /4000\t:  0.16429734230041504 \t2.528780698776245 \t1.0888118743896484\n",
      "Batch:\t15 /4000\t:  0.2033371925354004 \t2.7339560985565186 \t0.0\n",
      "Batch:\t16 /4000\t:  0.16378045082092285 \t2.8994498252868652 \t0.24787235260009766\n",
      "Batch:\t17 /4000\t:  0.1026458740234375 \t3.0035078525543213 \t0.2232685089111328\n",
      "Time: 4.972903251647949\t Loss: 13.549521684646606\t Valdacc: 0.35\t Testacc: 0.415\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  65 / 200\n",
      "Batch:\t0 /4000\t:  0.1625528335571289 \t0.1647489070892334 \t6.043907165527344\n",
      "Batch:\t1 /4000\t:  0.16663742065429688 \t0.332977294921875 \t11.291041374206543\n",
      "Batch:\t2 /4000\t:  0.16939926147460938 \t0.5042603015899658 \t0.1659097671508789\n",
      "Batch:\t3 /4000\t:  0.16247153282165527 \t0.6689422130584717 \t1.4003028869628906\n",
      "Batch:\t4 /4000\t:  0.15958189964294434 \t0.83016037940979 \t0.0\n",
      "Batch:\t5 /4000\t:  0.1615309715270996 \t0.9933667182922363 \t1.4428339004516602\n",
      "Batch:\t6 /4000\t:  0.1618952751159668 \t1.156792163848877 \t1.4313569068908691\n",
      "Batch:\t7 /4000\t:  0.16521620750427246 \t1.3236439228057861 \t1.9588193893432617\n",
      "Batch:\t8 /4000\t:  0.17025423049926758 \t1.4957470893859863 \t3.2764511108398438\n",
      "Batch:\t9 /4000\t:  0.16652631759643555 \t1.6637287139892578 \t2.5737295150756836\n",
      "Batch:\t10 /4000\t:  0.16751646995544434 \t1.8332512378692627 \t0.3062553405761719\n",
      "Batch:\t11 /4000\t:  0.16122937202453613 \t1.9964840412139893 \t0.5083131790161133\n",
      "Batch:\t12 /4000\t:  0.1676945686340332 \t2.16589093208313 \t0.8111627101898193\n",
      "Batch:\t13 /4000\t:  0.17132949829101562 \t2.3392221927642822 \t3.0316991806030273\n",
      "Batch:\t14 /4000\t:  0.16965866088867188 \t2.510969877243042 \t0.0\n",
      "Batch:\t15 /4000\t:  0.16256403923034668 \t2.6763765811920166 \t0.6832504272460938\n",
      "Batch:\t16 /4000\t:  0.16435551643371582 \t2.842878818511963 \t1.5047569274902344\n",
      "Batch:\t17 /4000\t:  0.10849976539611816 \t2.9527385234832764 \t0.05400848388671875\n",
      "Time: 4.883647441864014\t Loss: 36.48379826545715\t Valdacc: 0.37\t Testacc: 0.43\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  66 / 200\n",
      "Batch:\t0 /4000\t:  0.16097688674926758 \t0.16309857368469238 \t4.936612129211426\n",
      "Batch:\t1 /4000\t:  0.16713333129882812 \t0.3319556713104248 \t1.7407960891723633\n",
      "Batch:\t2 /4000\t:  0.16901040077209473 \t0.5024387836456299 \t1.9372482299804688\n",
      "Batch:\t3 /4000\t:  0.16123199462890625 \t0.6650805473327637 \t1.6836447715759277\n",
      "Batch:\t4 /4000\t:  0.16434526443481445 \t0.8311328887939453 \t1.0801105499267578\n",
      "Batch:\t5 /4000\t:  0.16683340072631836 \t0.9998822212219238 \t1.492701530456543\n",
      "Batch:\t6 /4000\t:  0.18277382850646973 \t1.1841862201690674 \t1.8785743713378906\n",
      "Batch:\t7 /4000\t:  0.16853094100952148 \t1.3543410301208496 \t0.09365510940551758\n",
      "Batch:\t8 /4000\t:  0.16660261154174805 \t1.5234551429748535 \t0.0\n",
      "Batch:\t9 /4000\t:  0.16583657264709473 \t1.6907143592834473 \t0.06765937805175781\n",
      "Batch:\t10 /4000\t:  0.16840457916259766 \t1.8607838153839111 \t3.1422791481018066\n",
      "Batch:\t11 /4000\t:  0.1707313060760498 \t2.0331077575683594 \t1.6061758995056152\n",
      "Batch:\t12 /4000\t:  0.1668705940246582 \t2.2018046379089355 \t1.337059497833252\n",
      "Batch:\t13 /4000\t:  0.16491389274597168 \t2.3682844638824463 \t0.24353408813476562\n",
      "Batch:\t14 /4000\t:  0.18794465065002441 \t2.558770179748535 \t0.8578176498413086\n",
      "Batch:\t15 /4000\t:  0.15887737274169922 \t2.7190632820129395 \t0.0\n",
      "Batch:\t16 /4000\t:  0.16100168228149414 \t2.8816680908203125 \t1.8038101196289062\n",
      "Batch:\t17 /4000\t:  0.10291123390197754 \t2.985365867614746 \t0.15145063400268555\n",
      "Time: 4.894539833068848\t Loss: 24.053129196166992\t Valdacc: 0.39\t Testacc: 0.42\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  67 / 200\n",
      "Batch:\t0 /4000\t:  0.18116331100463867 \t0.1833972930908203 \t0.13474559783935547\n",
      "Batch:\t1 /4000\t:  0.16217541694641113 \t0.3472867012023926 \t0.2644767761230469\n",
      "Batch:\t2 /4000\t:  0.1824512481689453 \t0.5312573909759521 \t1.0676240921020508\n",
      "Batch:\t3 /4000\t:  0.16672205924987793 \t0.700049638748169 \t0.6953549385070801\n",
      "Batch:\t4 /4000\t:  0.16652822494506836 \t0.8680934906005859 \t3.9156360626220703\n",
      "Batch:\t5 /4000\t:  0.16374945640563965 \t1.0332324504852295 \t0.7306613922119141\n",
      "Batch:\t6 /4000\t:  0.1581437587738037 \t1.1928129196166992 \t0.705510139465332\n",
      "Batch:\t7 /4000\t:  0.1589975357055664 \t1.3533649444580078 \t0.0\n",
      "Batch:\t8 /4000\t:  0.16190791130065918 \t1.5167739391326904 \t0.0\n",
      "Batch:\t9 /4000\t:  0.1707315444946289 \t1.689399242401123 \t0.5086965560913086\n",
      "Batch:\t10 /4000\t:  0.16635704040527344 \t1.8574762344360352 \t0.16442537307739258\n",
      "Batch:\t11 /4000\t:  0.16137123107910156 \t2.0202314853668213 \t0.709254264831543\n",
      "Batch:\t12 /4000\t:  0.16377735137939453 \t2.1854753494262695 \t2.061476707458496\n",
      "Batch:\t13 /4000\t:  0.16189289093017578 \t2.348811626434326 \t1.4764833450317383\n",
      "Batch:\t14 /4000\t:  0.16140961647033691 \t2.5113604068756104 \t3.364757537841797\n",
      "Batch:\t15 /4000\t:  0.16402649879455566 \t2.677809238433838 \t0.6768507957458496\n",
      "Batch:\t16 /4000\t:  0.16488099098205566 \t2.8443620204925537 \t1.9848642349243164\n",
      "Batch:\t17 /4000\t:  0.10440206527709961 \t2.9500412940979004 \t0.6793079376220703\n",
      "Time: 4.93034815788269\t Loss: 19.14012575149536\t Valdacc: 0.41\t Testacc: 0.405\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  68 / 200\n",
      "Batch:\t0 /4000\t:  0.15888404846191406 \t0.1600050926208496 \t0.0\n",
      "Batch:\t1 /4000\t:  0.1633167266845703 \t0.32495880126953125 \t4.564849853515625\n",
      "Batch:\t2 /4000\t:  0.1633439064025879 \t0.4897794723510742 \t2.586522102355957\n",
      "Batch:\t3 /4000\t:  0.1643681526184082 \t0.6555125713348389 \t0.4075450897216797\n",
      "Batch:\t4 /4000\t:  0.1690213680267334 \t0.826291561126709 \t0.02826976776123047\n",
      "Batch:\t5 /4000\t:  0.1670520305633545 \t0.9948163032531738 \t1.0927543640136719\n",
      "Batch:\t6 /4000\t:  0.16537070274353027 \t1.1620516777038574 \t3.024772882461548\n",
      "Batch:\t7 /4000\t:  0.16404438018798828 \t1.3275082111358643 \t2.0584864616394043\n",
      "Batch:\t8 /4000\t:  0.16450285911560059 \t1.4938321113586426 \t0.422698974609375\n",
      "Batch:\t9 /4000\t:  0.16530680656433105 \t1.6621556282043457 \t0.47010040283203125\n",
      "Batch:\t10 /4000\t:  0.1638493537902832 \t1.827436923980713 \t2.487555503845215\n",
      "Batch:\t11 /4000\t:  0.16470074653625488 \t1.9938828945159912 \t1.8511438369750977\n",
      "Batch:\t12 /4000\t:  0.167680025100708 \t2.1630852222442627 \t3.9210758209228516\n",
      "Batch:\t13 /4000\t:  0.16901063919067383 \t2.3335318565368652 \t2.32562255859375\n",
      "Batch:\t14 /4000\t:  0.16792798042297363 \t2.503268003463745 \t1.5415401458740234\n",
      "Batch:\t15 /4000\t:  0.1856091022491455 \t2.690793752670288 \t0.8850164413452148\n",
      "Batch:\t16 /4000\t:  0.16921448707580566 \t2.8615074157714844 \t3.8882365226745605\n",
      "Batch:\t17 /4000\t:  0.10315322875976562 \t2.966094970703125 \t0.3366575241088867\n",
      "Time: 4.843120813369751\t Loss: 31.892848253250122\t Valdacc: 0.41\t Testacc: 0.395\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  69 / 200\n",
      "Batch:\t0 /4000\t:  0.1634378433227539 \t0.164445161819458 \t3.1444969177246094\n",
      "Batch:\t1 /4000\t:  0.16474056243896484 \t0.33050966262817383 \t0.4169807434082031\n",
      "Batch:\t2 /4000\t:  0.166762113571167 \t0.4990692138671875 \t0.1277446746826172\n",
      "Batch:\t3 /4000\t:  0.18158268928527832 \t0.6822047233581543 \t2.3304710388183594\n",
      "Batch:\t4 /4000\t:  0.20325994491577148 \t0.8868393898010254 \t1.3407306671142578\n",
      "Batch:\t5 /4000\t:  0.18837237358093262 \t1.0767955780029297 \t0.32283592224121094\n",
      "Batch:\t6 /4000\t:  0.16022396087646484 \t1.2383382320404053 \t0.7157564163208008\n",
      "Batch:\t7 /4000\t:  0.15989398956298828 \t1.399583339691162 \t0.5329132080078125\n",
      "Batch:\t8 /4000\t:  0.16883635520935059 \t1.5698983669281006 \t0.0\n",
      "Batch:\t9 /4000\t:  0.1659097671508789 \t1.737250804901123 \t0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t10 /4000\t:  0.1681385040283203 \t1.9067418575286865 \t1.6819539070129395\n",
      "Batch:\t11 /4000\t:  0.18700146675109863 \t2.096717357635498 \t1.7955474853515625\n",
      "Batch:\t12 /4000\t:  0.16072416305541992 \t2.25880765914917 \t0.3125324249267578\n",
      "Batch:\t13 /4000\t:  0.16655564308166504 \t2.42716646194458 \t2.833883285522461\n",
      "Batch:\t14 /4000\t:  0.16061186790466309 \t2.5893473625183105 \t0.7757349014282227\n",
      "Batch:\t15 /4000\t:  0.16663146018981934 \t2.7574522495269775 \t1.5432376861572266\n",
      "Batch:\t16 /4000\t:  0.1634979248046875 \t2.922313928604126 \t1.0392990112304688\n",
      "Batch:\t17 /4000\t:  0.10383081436157227 \t3.027521848678589 \t0.0\n",
      "Time: 4.945447683334351\t Loss: 18.91411828994751\t Valdacc: 0.45\t Testacc: 0.39\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  70 / 200\n",
      "Batch:\t0 /4000\t:  0.17338275909423828 \t0.1751391887664795 \t0.0\n",
      "Batch:\t1 /4000\t:  0.16348481178283691 \t0.34003686904907227 \t2.1225357055664062\n",
      "Batch:\t2 /4000\t:  0.20375299453735352 \t0.5451388359069824 \t6.005827903747559\n",
      "Batch:\t3 /4000\t:  0.21434259414672852 \t0.7660207748413086 \t5.310352325439453\n",
      "Batch:\t4 /4000\t:  0.163771390914917 \t0.9311003684997559 \t1.7791954278945923\n",
      "Batch:\t5 /4000\t:  0.1699671745300293 \t1.1024086475372314 \t3.045914649963379\n",
      "Batch:\t6 /4000\t:  0.1618180274963379 \t1.2655115127563477 \t0.0\n",
      "Batch:\t7 /4000\t:  0.16181421279907227 \t1.4287364482879639 \t0.4367189407348633\n",
      "Batch:\t8 /4000\t:  0.18317055702209473 \t1.6132712364196777 \t0.4183875322341919\n",
      "Batch:\t9 /4000\t:  0.16036462783813477 \t1.7751233577728271 \t0.0\n",
      "Batch:\t10 /4000\t:  0.20360636711120605 \t1.9800529479980469 \t3.3913040161132812\n",
      "Batch:\t11 /4000\t:  0.16689562797546387 \t2.1530370712280273 \t0.019937515258789062\n",
      "Batch:\t12 /4000\t:  0.16808271408081055 \t2.3223767280578613 \t0.8553409576416016\n",
      "Batch:\t13 /4000\t:  0.16612935066223145 \t2.490145683288574 \t3.7221899032592773\n",
      "Batch:\t14 /4000\t:  0.16044998168945312 \t2.6518681049346924 \t1.7338895797729492\n",
      "Batch:\t15 /4000\t:  0.16251254081726074 \t2.8159613609313965 \t0.0\n",
      "Batch:\t16 /4000\t:  0.16176891326904297 \t2.9826273918151855 \t0.0\n",
      "Batch:\t17 /4000\t:  0.1066596508026123 \t3.0903077125549316 \t0.2044696807861328\n",
      "Time: 4.996237516403198\t Loss: 29.046064138412476\t Valdacc: 0.44\t Testacc: 0.405\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  71 / 200\n",
      "Batch:\t0 /4000\t:  0.1686093807220459 \t0.17052936553955078 \t0.0\n",
      "Batch:\t1 /4000\t:  0.20232892036437988 \t0.3742513656616211 \t0.5805988311767578\n",
      "Batch:\t2 /4000\t:  0.1617743968963623 \t0.537273645401001 \t0.0\n",
      "Batch:\t3 /4000\t:  0.17020821571350098 \t0.7094717025756836 \t3.586458206176758\n",
      "Batch:\t4 /4000\t:  0.16384196281433105 \t0.874901294708252 \t1.6007299423217773\n",
      "Batch:\t5 /4000\t:  0.16983890533447266 \t1.0464177131652832 \t2.9073972702026367\n",
      "Batch:\t6 /4000\t:  0.16019320487976074 \t1.2083415985107422 \t0.9631919860839844\n",
      "Batch:\t7 /4000\t:  0.1659088134765625 \t1.3754959106445312 \t0.009500503540039062\n",
      "Batch:\t8 /4000\t:  0.16039681434631348 \t1.5373871326446533 \t2.005406379699707\n",
      "Batch:\t9 /4000\t:  0.16659951210021973 \t1.7058937549591064 \t0.4655418395996094\n",
      "Batch:\t10 /4000\t:  0.16622281074523926 \t1.873471736907959 \t0.17307662963867188\n",
      "Batch:\t11 /4000\t:  0.2039346694946289 \t2.07875919342041 \t0.08782291412353516\n",
      "Batch:\t12 /4000\t:  0.166731595993042 \t2.246864080429077 \t0.0\n",
      "Batch:\t13 /4000\t:  0.2071075439453125 \t2.455152988433838 \t2.0099868774414062\n",
      "Batch:\t14 /4000\t:  0.19668912887573242 \t2.6539580821990967 \t0.6797366142272949\n",
      "Batch:\t15 /4000\t:  0.2029883861541748 \t2.858466148376465 \t3.2550745010375977\n",
      "Batch:\t16 /4000\t:  0.1632084846496582 \t3.0232551097869873 \t0.08517837524414062\n",
      "Batch:\t17 /4000\t:  0.10103273391723633 \t3.1257309913635254 \t0.4304828643798828\n",
      "Time: 5.007122755050659\t Loss: 18.8401837348938\t Valdacc: 0.39\t Testacc: 0.395\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  72 / 200\n",
      "Batch:\t0 /4000\t:  0.16863036155700684 \t0.17042040824890137 \t1.5811023712158203\n",
      "Batch:\t1 /4000\t:  0.16298985481262207 \t0.3355247974395752 \t0.036009788513183594\n",
      "Batch:\t2 /4000\t:  0.1678786277770996 \t0.5058205127716064 \t1.001307487487793\n",
      "Batch:\t3 /4000\t:  0.16558051109313965 \t0.6728472709655762 \t3.3486528396606445\n",
      "Batch:\t4 /4000\t:  0.16074180603027344 \t0.8352413177490234 \t0.3540668487548828\n",
      "Batch:\t5 /4000\t:  0.16072416305541992 \t0.9996218681335449 \t5.126459121704102\n",
      "Batch:\t6 /4000\t:  0.16089105606079102 \t1.161808729171753 \t5.687887191772461\n",
      "Batch:\t7 /4000\t:  0.16055703163146973 \t1.3238587379455566 \t0.14144515991210938\n",
      "Batch:\t8 /4000\t:  0.18511509895324707 \t1.5120298862457275 \t1.1887588500976562\n",
      "Batch:\t9 /4000\t:  0.1822652816772461 \t1.695671558380127 \t0.6675987243652344\n",
      "Batch:\t10 /4000\t:  0.17897534370422363 \t1.8758633136749268 \t0.46628284454345703\n",
      "Batch:\t11 /4000\t:  0.16119718551635742 \t2.0388553142547607 \t0.0\n",
      "Batch:\t12 /4000\t:  0.16387224197387695 \t2.204249620437622 \t0.7309503555297852\n",
      "Batch:\t13 /4000\t:  0.18248558044433594 \t2.388373851776123 \t2.4974193572998047\n",
      "Batch:\t14 /4000\t:  0.16420865058898926 \t2.559817314147949 \t0.20621228218078613\n",
      "Batch:\t15 /4000\t:  0.1633298397064209 \t2.7244229316711426 \t0.4096031188964844\n",
      "Batch:\t16 /4000\t:  0.2233743667602539 \t2.950622797012329 \t6.603116989135742\n",
      "Batch:\t17 /4000\t:  0.1015477180480957 \t3.053161859512329 \t0.9523458480834961\n",
      "Time: 5.0281431674957275\t Loss: 30.999219179153442\t Valdacc: 0.38\t Testacc: 0.385\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  73 / 200\n",
      "Batch:\t0 /4000\t:  0.18178415298461914 \t0.18419337272644043 \t2.387698173522949\n",
      "Batch:\t1 /4000\t:  0.16269969940185547 \t0.34827184677124023 \t1.4560956954956055\n",
      "Batch:\t2 /4000\t:  0.20525765419006348 \t0.5549788475036621 \t1.67730712890625\n",
      "Batch:\t3 /4000\t:  0.16144990921020508 \t0.7178044319152832 \t1.2935209274291992\n",
      "Batch:\t4 /4000\t:  0.16692423820495605 \t0.8862066268920898 \t2.3469104766845703\n",
      "Batch:\t5 /4000\t:  0.16619038581848145 \t1.0543267726898193 \t3.7870922088623047\n",
      "Batch:\t6 /4000\t:  0.16397881507873535 \t1.2199218273162842 \t1.740255355834961\n",
      "Batch:\t7 /4000\t:  0.16182708740234375 \t1.3835487365722656 \t1.0967826843261719\n",
      "Batch:\t8 /4000\t:  0.17216849327087402 \t1.557469367980957 \t0.3637351989746094\n",
      "Batch:\t9 /4000\t:  0.16824960708618164 \t1.7272851467132568 \t0.1119089126586914\n",
      "Batch:\t10 /4000\t:  0.16070222854614258 \t1.8897349834442139 \t1.0173463821411133\n",
      "Batch:\t11 /4000\t:  0.16385245323181152 \t2.0550832748413086 \t2.2466983795166016\n",
      "Batch:\t12 /4000\t:  0.16848039627075195 \t2.2250075340270996 \t0.3988990783691406\n",
      "Batch:\t13 /4000\t:  0.16570377349853516 \t2.392465591430664 \t1.3307785987854004\n",
      "Batch:\t14 /4000\t:  0.1719679832458496 \t2.5661795139312744 \t0.0\n",
      "Batch:\t15 /4000\t:  0.20578336715698242 \t2.7735848426818848 \t0.15419960021972656\n",
      "Batch:\t16 /4000\t:  0.17081713676452637 \t2.9460315704345703 \t0.10344505310058594\n",
      "Batch:\t17 /4000\t:  0.10031580924987793 \t3.047816514968872 \t0.0\n",
      "Time: 4.975803375244141\t Loss: 21.51267385482788\t Valdacc: 0.37\t Testacc: 0.38\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  74 / 200\n",
      "Batch:\t0 /4000\t:  0.1611340045928955 \t0.16246342658996582 \t1.1925668716430664\n",
      "Batch:\t1 /4000\t:  0.16165733337402344 \t0.32588934898376465 \t0.8375015258789062\n",
      "Batch:\t2 /4000\t:  0.16534686088562012 \t0.49277663230895996 \t0.9412212371826172\n",
      "Batch:\t3 /4000\t:  0.16724157333374023 \t0.6614203453063965 \t2.8319520950317383\n",
      "Batch:\t4 /4000\t:  0.2074146270751953 \t0.870647668838501 \t2.9927139282226562\n",
      "Batch:\t5 /4000\t:  0.18271565437316895 \t1.0549921989440918 \t1.4878568649291992\n",
      "Batch:\t6 /4000\t:  0.20493745803833008 \t1.261371374130249 \t0.661473274230957\n",
      "Batch:\t7 /4000\t:  0.16307425498962402 \t1.4263560771942139 \t1.4020042419433594\n",
      "Batch:\t8 /4000\t:  0.16488194465637207 \t1.593034267425537 \t0.12620925903320312\n",
      "Batch:\t9 /4000\t:  0.16533446311950684 \t1.7604498863220215 \t4.388022422790527\n",
      "Batch:\t10 /4000\t:  0.18048453330993652 \t1.9428629875183105 \t0.0\n",
      "Batch:\t11 /4000\t:  0.16797089576721191 \t2.1129753589630127 \t3.0636038780212402\n",
      "Batch:\t12 /4000\t:  0.1625216007232666 \t2.276710033416748 \t0.8428773880004883\n",
      "Batch:\t13 /4000\t:  0.17062735557556152 \t2.450131893157959 \t0.6981725692749023\n",
      "Batch:\t14 /4000\t:  0.16245484352111816 \t2.6140482425689697 \t0.0\n",
      "Batch:\t15 /4000\t:  0.18416547775268555 \t2.800135612487793 \t1.7972698211669922\n",
      "Batch:\t16 /4000\t:  0.16495466232299805 \t2.966858386993408 \t2.571439743041992\n",
      "Batch:\t17 /4000\t:  0.10992908477783203 \t3.0781168937683105 \t3.0778942108154297\n",
      "Time: 5.033383846282959\t Loss: 28.912779331207275\t Valdacc: 0.39\t Testacc: 0.375\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  75 / 200\n",
      "Batch:\t0 /4000\t:  0.15834665298461914 \t0.16095232963562012 \t0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t1 /4000\t:  0.1610100269317627 \t0.32370662689208984 \t0.9907550811767578\n",
      "Batch:\t2 /4000\t:  0.16674542427062988 \t0.49160337448120117 \t0.0\n",
      "Batch:\t3 /4000\t:  0.1644432544708252 \t0.6577003002166748 \t0.6700311899185181\n",
      "Batch:\t4 /4000\t:  0.16789984703063965 \t0.827033281326294 \t1.2646236419677734\n",
      "Batch:\t5 /4000\t:  0.16538667678833008 \t0.9941885471343994 \t0.0\n",
      "Batch:\t6 /4000\t:  0.20608091354370117 \t1.2022974491119385 \t0.17803239822387695\n",
      "Batch:\t7 /4000\t:  0.16586947441101074 \t1.3700127601623535 \t3.024775505065918\n",
      "Batch:\t8 /4000\t:  0.1611185073852539 \t1.5328927040100098 \t0.24785709381103516\n",
      "Batch:\t9 /4000\t:  0.1666090488433838 \t1.7011327743530273 \t0.0\n",
      "Batch:\t10 /4000\t:  0.1641547679901123 \t1.8668742179870605 \t4.98444128036499\n",
      "Batch:\t11 /4000\t:  0.16560983657836914 \t2.0343053340911865 \t0.16914939880371094\n",
      "Batch:\t12 /4000\t:  0.1660313606262207 \t2.2020673751831055 \t0.0\n",
      "Batch:\t13 /4000\t:  0.16418194770812988 \t2.367722988128662 \t2.979736804962158\n",
      "Batch:\t14 /4000\t:  0.1700916290283203 \t2.539140462875366 \t0.0\n",
      "Batch:\t15 /4000\t:  0.16396737098693848 \t2.7045388221740723 \t1.89780855178833\n",
      "Batch:\t16 /4000\t:  0.1648263931274414 \t2.8730697631835938 \t2.2561187744140625\n",
      "Batch:\t17 /4000\t:  0.1026003360748291 \t2.9775469303131104 \t0.7166566848754883\n",
      "Time: 4.916789531707764\t Loss: 19.37998640537262\t Valdacc: 0.45\t Testacc: 0.385\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  76 / 200\n",
      "Batch:\t0 /4000\t:  0.16176748275756836 \t0.16437029838562012 \t0.0\n",
      "Batch:\t1 /4000\t:  0.163468599319458 \t0.3294506072998047 \t1.276376724243164\n",
      "Batch:\t2 /4000\t:  0.16042280197143555 \t0.49129533767700195 \t3.0185089111328125\n",
      "Batch:\t3 /4000\t:  0.16133809089660645 \t0.6540403366088867 \t0.0\n",
      "Batch:\t4 /4000\t:  0.2052774429321289 \t0.8610594272613525 \t0.7104930877685547\n",
      "Batch:\t5 /4000\t:  0.1667797565460205 \t1.0295116901397705 \t0.0668191909790039\n",
      "Batch:\t6 /4000\t:  0.1653611660003662 \t1.1962716579437256 \t1.1033730506896973\n",
      "Batch:\t7 /4000\t:  0.1661701202392578 \t1.3640899658203125 \t0.0\n",
      "Batch:\t8 /4000\t:  0.17084264755249023 \t1.536754846572876 \t0.9302263259887695\n",
      "Batch:\t9 /4000\t:  0.16248607635498047 \t1.7010488510131836 \t0.9780445098876953\n",
      "Batch:\t10 /4000\t:  0.16401958465576172 \t1.8674342632293701 \t0.0\n",
      "Batch:\t11 /4000\t:  0.16646027565002441 \t2.035360336303711 \t5.52489709854126\n",
      "Batch:\t12 /4000\t:  0.1643075942993164 \t2.2014434337615967 \t0.562713623046875\n",
      "Batch:\t13 /4000\t:  0.16064143180847168 \t2.3635361194610596 \t0.0\n",
      "Batch:\t14 /4000\t:  0.16902470588684082 \t2.534327745437622 \t0.5673627853393555\n",
      "Batch:\t15 /4000\t:  0.2085740566253662 \t2.7444684505462646 \t2.412436008453369\n",
      "Batch:\t16 /4000\t:  0.1621851921081543 \t2.908134937286377 \t0.0\n",
      "Batch:\t17 /4000\t:  0.12460160255432129 \t3.0339736938476562 \t0.9871253967285156\n",
      "Time: 5.07642126083374\t Loss: 18.138376712799072\t Valdacc: 0.42\t Testacc: 0.385\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  77 / 200\n",
      "Batch:\t0 /4000\t:  0.16762185096740723 \t0.17095708847045898 \t0.49852752685546875\n",
      "Batch:\t1 /4000\t:  0.16066622734069824 \t0.3331427574157715 \t3.242852210998535\n",
      "Batch:\t2 /4000\t:  0.16218113899230957 \t0.49712276458740234 \t3.7090377807617188\n",
      "Batch:\t3 /4000\t:  0.16944503784179688 \t0.6691279411315918 \t0.0\n",
      "Batch:\t4 /4000\t:  0.163499116897583 \t0.8350749015808105 \t1.9170043468475342\n",
      "Batch:\t5 /4000\t:  0.16257834434509277 \t0.9995436668395996 \t1.363790512084961\n",
      "Batch:\t6 /4000\t:  0.16179513931274414 \t1.163623332977295 \t0.5825643539428711\n",
      "Batch:\t7 /4000\t:  0.1626431941986084 \t1.3277318477630615 \t0.0\n",
      "Batch:\t8 /4000\t:  0.1817951202392578 \t1.5110485553741455 \t2.656282424926758\n",
      "Batch:\t9 /4000\t:  0.16495060920715332 \t1.6775481700897217 \t0.6308804750442505\n",
      "Batch:\t10 /4000\t:  0.15975451469421387 \t1.8388848304748535 \t0.0\n",
      "Batch:\t11 /4000\t:  0.16278576850891113 \t2.003481864929199 \t0.18104171752929688\n",
      "Batch:\t12 /4000\t:  0.20423078536987305 \t2.2095460891723633 \t0.24503707885742188\n",
      "Batch:\t13 /4000\t:  0.16124582290649414 \t2.3721401691436768 \t2.409928321838379\n",
      "Batch:\t14 /4000\t:  0.16610956192016602 \t2.539612054824829 \t1.549149513244629\n",
      "Batch:\t15 /4000\t:  0.16552257537841797 \t2.707139253616333 \t0.20212745666503906\n",
      "Batch:\t16 /4000\t:  0.18184351921081543 \t2.890641212463379 \t1.5812091827392578\n",
      "Batch:\t17 /4000\t:  0.10702991485595703 \t3.000082492828369 \t0.34578919410705566\n",
      "Time: 4.979455232620239\t Loss: 21.115222096443176\t Valdacc: 0.4\t Testacc: 0.385\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  78 / 200\n",
      "Batch:\t0 /4000\t:  0.15904021263122559 \t0.16139650344848633 \t0.0\n",
      "Batch:\t1 /4000\t:  0.1641988754272461 \t0.32732462882995605 \t0.027875900268554688\n",
      "Batch:\t2 /4000\t:  0.1656029224395752 \t0.49460411071777344 \t0.0\n",
      "Batch:\t3 /4000\t:  0.16254210472106934 \t0.6588597297668457 \t4.215160369873047\n",
      "Batch:\t4 /4000\t:  0.16157984733581543 \t0.8219802379608154 \t2.1572513580322266\n",
      "Batch:\t5 /4000\t:  0.16129851341247559 \t0.9846725463867188 \t1.9303016662597656\n",
      "Batch:\t6 /4000\t:  0.17293691635131836 \t1.1607062816619873 \t0.0\n",
      "Batch:\t7 /4000\t:  0.15865850448608398 \t1.3207542896270752 \t0.0\n",
      "Batch:\t8 /4000\t:  0.1757657527923584 \t1.4985051155090332 \t0.0\n",
      "Batch:\t9 /4000\t:  0.1839890480041504 \t1.6850039958953857 \t1.495387077331543\n",
      "Batch:\t10 /4000\t:  0.16331052780151367 \t1.850273609161377 \t1.4285955429077148\n",
      "Batch:\t11 /4000\t:  0.16229581832885742 \t2.014369487762451 \t0.5553512573242188\n",
      "Batch:\t12 /4000\t:  0.18735003471374512 \t2.2048990726470947 \t0.28246593475341797\n",
      "Batch:\t13 /4000\t:  0.16439199447631836 \t2.370802402496338 \t5.92908239364624\n",
      "Batch:\t14 /4000\t:  0.17304086685180664 \t2.545398473739624 \t0.10369873046875\n",
      "Batch:\t15 /4000\t:  0.1763322353363037 \t2.7230384349823 \t3.1003799438476562\n",
      "Batch:\t16 /4000\t:  0.1813187599182129 \t2.906693458557129 \t0.12960529327392578\n",
      "Batch:\t17 /4000\t:  0.10761237144470215 \t3.0158958435058594 \t0.4741325378417969\n",
      "Time: 4.909215450286865\t Loss: 21.829288005828857\t Valdacc: 0.41\t Testacc: 0.39\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  79 / 200\n",
      "Batch:\t0 /4000\t:  0.1637108325958252 \t0.1646275520324707 \t0.45062255859375\n",
      "Batch:\t1 /4000\t:  0.20633673667907715 \t0.3724386692047119 \t0.09798717498779297\n",
      "Batch:\t2 /4000\t:  0.17913198471069336 \t0.5680375099182129 \t0.15282249450683594\n",
      "Batch:\t3 /4000\t:  0.16634345054626465 \t0.7357513904571533 \t0.1456296443939209\n",
      "Batch:\t4 /4000\t:  0.16121983528137207 \t0.8984174728393555 \t0.0\n",
      "Batch:\t5 /4000\t:  0.16635394096374512 \t1.0664544105529785 \t0.2737555503845215\n",
      "Batch:\t6 /4000\t:  0.16392731666564941 \t1.2317461967468262 \t1.1011457443237305\n",
      "Batch:\t7 /4000\t:  0.1616194248199463 \t1.394789695739746 \t0.0\n",
      "Batch:\t8 /4000\t:  0.18311476707458496 \t1.581768274307251 \t0.0\n",
      "Batch:\t9 /4000\t:  0.17330718040466309 \t1.7564775943756104 \t3.06333065032959\n",
      "Batch:\t10 /4000\t:  0.1607649326324463 \t1.9190549850463867 \t2.422762393951416\n",
      "Batch:\t11 /4000\t:  0.17929291725158691 \t2.1001157760620117 \t1.9653408527374268\n",
      "Batch:\t12 /4000\t:  0.1606590747833252 \t2.2623002529144287 \t0.0006170272827148438\n",
      "Batch:\t13 /4000\t:  0.16005277633666992 \t2.4236319065093994 \t1.2771759033203125\n",
      "Batch:\t14 /4000\t:  0.16585659980773926 \t2.590775489807129 \t0.15607404708862305\n",
      "Batch:\t15 /4000\t:  0.16499090194702148 \t2.757420063018799 \t2.2144880294799805\n",
      "Batch:\t16 /4000\t:  0.18856358528137207 \t2.9480490684509277 \t0.5839805603027344\n",
      "Batch:\t17 /4000\t:  0.09946656227111816 \t3.048647403717041 \t0.0\n",
      "Time: 4.964104175567627\t Loss: 13.90573263168335\t Valdacc: 0.4\t Testacc: 0.38\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  80 / 200\n",
      "Batch:\t0 /4000\t:  0.16600441932678223 \t0.16823029518127441 \t0.9337425231933594\n",
      "Batch:\t1 /4000\t:  0.16640305519104004 \t0.33604955673217773 \t1.7659616470336914\n",
      "Batch:\t2 /4000\t:  0.16304922103881836 \t0.5006256103515625 \t2.246419906616211\n",
      "Batch:\t3 /4000\t:  0.17073917388916016 \t0.6727428436279297 \t0.44397735595703125\n",
      "Batch:\t4 /4000\t:  0.184112548828125 \t0.8632614612579346 \t1.3740606307983398\n",
      "Batch:\t5 /4000\t:  0.16048026084899902 \t1.0250146389007568 \t1.0206012725830078\n",
      "Batch:\t6 /4000\t:  0.15868878364562988 \t1.185333251953125 \t0.0\n",
      "Batch:\t7 /4000\t:  0.16009902954101562 \t1.3470039367675781 \t0.5923738479614258\n",
      "Batch:\t8 /4000\t:  0.16005754470825195 \t1.5084114074707031 \t0.5077438354492188\n",
      "Batch:\t9 /4000\t:  0.16615748405456543 \t1.675790548324585 \t0.1574077606201172\n",
      "Batch:\t10 /4000\t:  0.1692366600036621 \t1.8465731143951416 \t1.2884416580200195\n",
      "Batch:\t11 /4000\t:  0.20380210876464844 \t2.0516815185546875 \t0.8535537719726562\n",
      "Batch:\t12 /4000\t:  0.19084882736206055 \t2.2439322471618652 \t1.7003440856933594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t13 /4000\t:  0.20296525955200195 \t2.4481348991394043 \t0.40813350677490234\n",
      "Batch:\t14 /4000\t:  0.16278362274169922 \t2.6122748851776123 \t2.2065281867980957\n",
      "Batch:\t15 /4000\t:  0.1699538230895996 \t2.7836947441101074 \t1.9409542083740234\n",
      "Batch:\t16 /4000\t:  0.15780162811279297 \t2.9434092044830322 \t0.0\n",
      "Batch:\t17 /4000\t:  0.10074424743652344 \t3.045325517654419 \t0.0\n",
      "Time: 5.006932258605957\t Loss: 17.44024419784546\t Valdacc: 0.39\t Testacc: 0.385\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  81 / 200\n",
      "Batch:\t0 /4000\t:  0.16148924827575684 \t0.16359806060791016 \t0.0\n",
      "Batch:\t1 /4000\t:  0.1631007194519043 \t0.32810425758361816 \t0.44915771484375\n",
      "Batch:\t2 /4000\t:  0.16738271713256836 \t0.49692869186401367 \t0.21776485443115234\n",
      "Batch:\t3 /4000\t:  0.16369152069091797 \t0.6622142791748047 \t4.224142551422119\n",
      "Batch:\t4 /4000\t:  0.16070938110351562 \t0.824242353439331 \t1.395425796508789\n",
      "Batch:\t5 /4000\t:  0.16552472114562988 \t0.9916481971740723 \t1.500992774963379\n",
      "Batch:\t6 /4000\t:  0.19106602668762207 \t1.1851646900177002 \t0.6670799255371094\n",
      "Batch:\t7 /4000\t:  0.1643838882446289 \t1.3509516716003418 \t1.7554492950439453\n",
      "Batch:\t8 /4000\t:  0.16520357131958008 \t1.5180678367614746 \t3.604790687561035\n",
      "Batch:\t9 /4000\t:  0.16675066947937012 \t1.6865618228912354 \t0.6857538223266602\n",
      "Batch:\t10 /4000\t:  0.16814351081848145 \t1.8572087287902832 \t0.7518062591552734\n",
      "Batch:\t11 /4000\t:  0.1672654151916504 \t2.0259287357330322 \t0.0\n",
      "Batch:\t12 /4000\t:  0.1712353229522705 \t2.208070755004883 \t0.40447425842285156\n",
      "Batch:\t13 /4000\t:  0.1639866828918457 \t2.3740360736846924 \t1.21624755859375\n",
      "Batch:\t14 /4000\t:  0.1919260025024414 \t2.56790828704834 \t2.6393322944641113\n",
      "Batch:\t15 /4000\t:  0.16414165496826172 \t2.7350165843963623 \t0.40468406677246094\n",
      "Batch:\t16 /4000\t:  0.1648259162902832 \t2.901672124862671 \t0.0\n",
      "Batch:\t17 /4000\t:  0.10012459754943848 \t3.0031542778015137 \t0.0\n",
      "Time: 4.900341510772705\t Loss: 19.917101860046387\t Valdacc: 0.41\t Testacc: 0.43\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  82 / 200\n",
      "Batch:\t0 /4000\t:  0.20886731147766113 \t0.2106475830078125 \t0.8371157646179199\n",
      "Batch:\t1 /4000\t:  0.16386795043945312 \t0.3758995532989502 \t0.4668731689453125\n",
      "Batch:\t2 /4000\t:  0.1614832878112793 \t0.539130687713623 \t1.6343438625335693\n",
      "Batch:\t3 /4000\t:  0.1817929744720459 \t0.7225615978240967 \t2.106779098510742\n",
      "Batch:\t4 /4000\t:  0.16392087936401367 \t0.887873649597168 \t1.8969554901123047\n",
      "Batch:\t5 /4000\t:  0.16623306274414062 \t1.0555696487426758 \t0.6007165908813477\n",
      "Batch:\t6 /4000\t:  0.16692638397216797 \t1.2241885662078857 \t0.0\n",
      "Batch:\t7 /4000\t:  0.1612684726715088 \t1.3868372440338135 \t0.0\n",
      "Batch:\t8 /4000\t:  0.16404151916503906 \t1.552473545074463 \t2.6080479621887207\n",
      "Batch:\t9 /4000\t:  0.16433191299438477 \t1.718496561050415 \t0.041210174560546875\n",
      "Batch:\t10 /4000\t:  0.20394182205200195 \t1.9238333702087402 \t0.4463977813720703\n",
      "Batch:\t11 /4000\t:  0.1640176773071289 \t2.0890188217163086 \t2.0230674743652344\n",
      "Batch:\t12 /4000\t:  0.16054630279541016 \t2.2508656978607178 \t1.5406055450439453\n",
      "Batch:\t13 /4000\t:  0.1647799015045166 \t2.417520761489868 \t0.2828793525695801\n",
      "Batch:\t14 /4000\t:  0.16765785217285156 \t2.5869879722595215 \t0.0\n",
      "Batch:\t15 /4000\t:  0.1681079864501953 \t2.756927967071533 \t0.08431243896484375\n",
      "Batch:\t16 /4000\t:  0.1651167869567871 \t2.9239389896392822 \t0.36840248107910156\n",
      "Batch:\t17 /4000\t:  0.13086986541748047 \t3.06428599357605 \t0.20149803161621094\n",
      "Time: 4.979868650436401\t Loss: 15.13920521736145\t Valdacc: 0.42\t Testacc: 0.405\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  83 / 200\n",
      "Batch:\t0 /4000\t:  0.1655411720275879 \t0.16637349128723145 \t0.6157569885253906\n",
      "Batch:\t1 /4000\t:  0.2032942771911621 \t0.3709907531738281 \t1.286625862121582\n",
      "Batch:\t2 /4000\t:  0.16613483428955078 \t0.5384771823883057 \t1.818624496459961\n",
      "Batch:\t3 /4000\t:  0.16232872009277344 \t0.7023382186889648 \t1.3474225997924805\n",
      "Batch:\t4 /4000\t:  0.16883540153503418 \t0.8730170726776123 \t2.209095001220703\n",
      "Batch:\t5 /4000\t:  0.16482925415039062 \t1.039583444595337 \t0.0\n",
      "Batch:\t6 /4000\t:  0.18963027000427246 \t1.2311463356018066 \t4.499868392944336\n",
      "Batch:\t7 /4000\t:  0.1637575626373291 \t1.3965134620666504 \t0.37452220916748047\n",
      "Batch:\t8 /4000\t:  0.16396093368530273 \t1.561880111694336 \t0.4967765808105469\n",
      "Batch:\t9 /4000\t:  0.20403766632080078 \t1.7672739028930664 \t1.050887107849121\n",
      "Batch:\t10 /4000\t:  0.1854557991027832 \t1.9541704654693604 \t1.7813358306884766\n",
      "Batch:\t11 /4000\t:  0.16024565696716309 \t2.1200459003448486 \t0.0\n",
      "Batch:\t12 /4000\t:  0.15769577026367188 \t2.278883457183838 \t0.0\n",
      "Batch:\t13 /4000\t:  0.16479134559631348 \t2.4450440406799316 \t1.299295425415039\n",
      "Batch:\t14 /4000\t:  0.16714143753051758 \t2.6140196323394775 \t0.2257823944091797\n",
      "Batch:\t15 /4000\t:  0.1700446605682373 \t2.785425901412964 \t0.07151031494140625\n",
      "Batch:\t16 /4000\t:  0.16733193397521973 \t2.954388380050659 \t1.688492774963379\n",
      "Batch:\t17 /4000\t:  0.10853052139282227 \t3.064342975616455 \t0.04112720489501953\n",
      "Time: 4.997591972351074\t Loss: 18.8071231842041\t Valdacc: 0.43\t Testacc: 0.41\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  84 / 200\n",
      "Batch:\t0 /4000\t:  0.16054248809814453 \t0.16135716438293457 \t0.4479255676269531\n",
      "Batch:\t1 /4000\t:  0.15973448753356934 \t0.32255125045776367 \t1.0061511993408203\n",
      "Batch:\t2 /4000\t:  0.16293048858642578 \t0.4867837429046631 \t2.021900177001953\n",
      "Batch:\t3 /4000\t:  0.1794877052307129 \t0.6829783916473389 \t0.2500038146972656\n",
      "Batch:\t4 /4000\t:  0.16664695739746094 \t0.8510522842407227 \t2.5722694396972656\n",
      "Batch:\t5 /4000\t:  0.20917534828186035 \t1.0617425441741943 \t2.2694406509399414\n",
      "Batch:\t6 /4000\t:  0.1777207851409912 \t1.2414875030517578 \t1.414626121520996\n",
      "Batch:\t7 /4000\t:  0.1588585376739502 \t1.4015922546386719 \t0.0\n",
      "Batch:\t8 /4000\t:  0.17432737350463867 \t1.577672004699707 \t0.0\n",
      "Batch:\t9 /4000\t:  0.1637418270111084 \t1.7426910400390625 \t0.0\n",
      "Batch:\t10 /4000\t:  0.17411470413208008 \t1.9186019897460938 \t0.8001441955566406\n",
      "Batch:\t11 /4000\t:  0.17968297004699707 \t2.100306510925293 \t1.9292383193969727\n",
      "Batch:\t12 /4000\t:  0.17862272262573242 \t2.280193328857422 \t0.06937980651855469\n",
      "Batch:\t13 /4000\t:  0.1811509132385254 \t2.462672710418701 \t2.9337425231933594\n",
      "Batch:\t14 /4000\t:  0.1689910888671875 \t2.6370158195495605 \t1.4039521217346191\n",
      "Batch:\t15 /4000\t:  0.17768335342407227 \t2.816174268722534 \t0.0\n",
      "Batch:\t16 /4000\t:  0.16469407081604004 \t2.9826176166534424 \t0.3884263038635254\n",
      "Batch:\t17 /4000\t:  0.10315084457397461 \t3.0871400833129883 \t1.1193761825561523\n",
      "Time: 4.977312088012695\t Loss: 18.62657642364502\t Valdacc: 0.42\t Testacc: 0.37\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  85 / 200\n",
      "Batch:\t0 /4000\t:  0.163771390914917 \t0.16605424880981445 \t1.7292776107788086\n",
      "Batch:\t1 /4000\t:  0.16635918617248535 \t0.3337516784667969 \t1.3585786819458008\n",
      "Batch:\t2 /4000\t:  0.16172075271606445 \t0.49723339080810547 \t0.0\n",
      "Batch:\t3 /4000\t:  0.16075849533081055 \t0.6593382358551025 \t0.10706520080566406\n",
      "Batch:\t4 /4000\t:  0.16663813591003418 \t0.8277542591094971 \t2.7269859313964844\n",
      "Batch:\t5 /4000\t:  0.16079473495483398 \t0.9912559986114502 \t0.30263519287109375\n",
      "Batch:\t6 /4000\t:  0.16164398193359375 \t1.1542508602142334 \t0.9232730865478516\n",
      "Batch:\t7 /4000\t:  0.17449259757995605 \t1.330385684967041 \t1.458913803100586\n",
      "Batch:\t8 /4000\t:  0.18119359016418457 \t1.5128388404846191 \t1.2204289436340332\n",
      "Batch:\t9 /4000\t:  0.16895318031311035 \t1.6831691265106201 \t0.24701309204101562\n",
      "Batch:\t10 /4000\t:  0.16378140449523926 \t1.8486378192901611 \t1.5096702575683594\n",
      "Batch:\t11 /4000\t:  0.16334915161132812 \t2.013202428817749 \t6.471930503845215\n",
      "Batch:\t12 /4000\t:  0.1616804599761963 \t2.176541566848755 \t0.6168503761291504\n",
      "Batch:\t13 /4000\t:  0.15998220443725586 \t2.337967872619629 \t0.46745777130126953\n",
      "Batch:\t14 /4000\t:  0.1628859043121338 \t2.5021047592163086 \t1.2048158645629883\n",
      "Batch:\t15 /4000\t:  0.1732177734375 \t2.6768832206726074 \t0.2985258102416992\n",
      "Batch:\t16 /4000\t:  0.16585564613342285 \t2.8443422317504883 \t0.4332294464111328\n",
      "Batch:\t17 /4000\t:  0.09936833381652832 \t2.944779396057129 \t0.0\n",
      "Time: 4.802655458450317\t Loss: 21.076651573181152\t Valdacc: 0.41\t Testacc: 0.375\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  86 / 200\n",
      "Batch:\t0 /4000\t:  0.16103053092956543 \t0.16281390190124512 \t0.1299118995666504\n",
      "Batch:\t1 /4000\t:  0.18486404418945312 \t0.353593111038208 \t3.648387908935547\n",
      "Batch:\t2 /4000\t:  0.16881966590881348 \t0.5241696834564209 \t1.6040163040161133\n",
      "Batch:\t3 /4000\t:  0.16918253898620605 \t0.6948037147521973 \t0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t4 /4000\t:  0.21952176094055176 \t0.9207582473754883 \t2.789022922515869\n",
      "Batch:\t5 /4000\t:  0.16860723495483398 \t1.0907719135284424 \t0.7547607421875\n",
      "Batch:\t6 /4000\t:  0.17659735679626465 \t1.2689597606658936 \t0.04451942443847656\n",
      "Batch:\t7 /4000\t:  0.16901493072509766 \t1.4393434524536133 \t0.8513975143432617\n",
      "Batch:\t8 /4000\t:  0.16408038139343262 \t1.6049385070800781 \t0.14213216304779053\n",
      "Batch:\t9 /4000\t:  0.16956377029418945 \t1.7760679721832275 \t0.4689445495605469\n",
      "Batch:\t10 /4000\t:  0.15921950340270996 \t1.9364473819732666 \t1.7791481018066406\n",
      "Batch:\t11 /4000\t:  0.16485357284545898 \t2.1028857231140137 \t1.8088951110839844\n",
      "Batch:\t12 /4000\t:  0.17177987098693848 \t2.276125907897949 \t0.5993976593017578\n",
      "Batch:\t13 /4000\t:  0.16730475425720215 \t2.44486665725708 \t0.26203441619873047\n",
      "Batch:\t14 /4000\t:  0.1860485076904297 \t2.6326258182525635 \t0.0\n",
      "Batch:\t15 /4000\t:  0.21405744552612305 \t2.8560144901275635 \t1.6999931335449219\n",
      "Batch:\t16 /4000\t:  0.16366028785705566 \t3.020853042602539 \t0.4616575241088867\n",
      "Batch:\t17 /4000\t:  0.09964251518249512 \t3.1215927600860596 \t0.0\n",
      "Time: 5.014455080032349\t Loss: 17.044219374656677\t Valdacc: 0.42\t Testacc: 0.37\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  87 / 200\n",
      "Batch:\t0 /4000\t:  0.15992021560668945 \t0.16106915473937988 \t2.2834854125976562\n",
      "Batch:\t1 /4000\t:  0.16360998153686523 \t0.3257272243499756 \t0.10291481018066406\n",
      "Batch:\t2 /4000\t:  0.16386127471923828 \t0.4913477897644043 \t1.4830131530761719\n",
      "Batch:\t3 /4000\t:  0.18060994148254395 \t0.680548906326294 \t0.19924259185791016\n",
      "Batch:\t4 /4000\t:  0.16666650772094727 \t0.8484909534454346 \t0.8092880249023438\n",
      "Batch:\t5 /4000\t:  0.1785883903503418 \t1.0288214683532715 \t3.3981056213378906\n",
      "Batch:\t6 /4000\t:  0.16188788414001465 \t1.1923532485961914 \t0.0\n",
      "Batch:\t7 /4000\t:  0.159804105758667 \t1.3532211780548096 \t2.9711923599243164\n",
      "Batch:\t8 /4000\t:  0.16846394538879395 \t1.5233204364776611 \t1.9743022918701172\n",
      "Batch:\t9 /4000\t:  0.1627359390258789 \t1.6875715255737305 \t2.43394136428833\n",
      "Batch:\t10 /4000\t:  0.1622762680053711 \t1.8511557579040527 \t1.467705249786377\n",
      "Batch:\t11 /4000\t:  0.17836999893188477 \t2.0311601161956787 \t0.0\n",
      "Batch:\t12 /4000\t:  0.16515016555786133 \t2.1976542472839355 \t0.0037555694580078125\n",
      "Batch:\t13 /4000\t:  0.18374228477478027 \t2.3825783729553223 \t3.972287178039551\n",
      "Batch:\t14 /4000\t:  0.16769170761108398 \t2.5517592430114746 \t0.0\n",
      "Batch:\t15 /4000\t:  0.16934990882873535 \t2.7226431369781494 \t1.7648963928222656\n",
      "Batch:\t16 /4000\t:  0.17541265487670898 \t2.8997445106506348 \t0.7218775749206543\n",
      "Batch:\t17 /4000\t:  0.10105133056640625 \t3.0019783973693848 \t0.0\n",
      "Time: 4.89890718460083\t Loss: 23.586007595062256\t Valdacc: 0.41\t Testacc: 0.405\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  88 / 200\n",
      "Batch:\t0 /4000\t:  0.16756415367126465 \t0.1684412956237793 \t1.1769390106201172\n",
      "Batch:\t1 /4000\t:  0.16719412803649902 \t0.3370816707611084 \t1.0680789947509766\n",
      "Batch:\t2 /4000\t:  0.16185379028320312 \t0.5006468296051025 \t0.43098926544189453\n",
      "Batch:\t3 /4000\t:  0.1613171100616455 \t0.6635782718658447 \t0.9301013946533203\n",
      "Batch:\t4 /4000\t:  0.16609764099121094 \t0.8312451839447021 \t0.0\n",
      "Batch:\t5 /4000\t:  0.16104578971862793 \t0.9940822124481201 \t0.0\n",
      "Batch:\t6 /4000\t:  0.16445279121398926 \t1.1603779792785645 \t0.0045928955078125\n",
      "Batch:\t7 /4000\t:  0.1592116355895996 \t1.3212366104125977 \t0.0\n",
      "Batch:\t8 /4000\t:  0.16395020484924316 \t1.487145185470581 \t2.093693733215332\n",
      "Batch:\t9 /4000\t:  0.20472407341003418 \t1.6936492919921875 \t0.9487370252609253\n",
      "Batch:\t10 /4000\t:  0.16327714920043945 \t1.8589234352111816 \t0.0\n",
      "Batch:\t11 /4000\t:  0.16472959518432617 \t2.0253965854644775 \t0.0\n",
      "Batch:\t12 /4000\t:  0.1644740104675293 \t2.1913483142852783 \t0.19645309448242188\n",
      "Batch:\t13 /4000\t:  0.16077756881713867 \t2.3536386489868164 \t0.0\n",
      "Batch:\t14 /4000\t:  0.1615147590637207 \t2.5170583724975586 \t1.0015945434570312\n",
      "Batch:\t15 /4000\t:  0.15909314155578613 \t2.6775903701782227 \t0.0\n",
      "Batch:\t16 /4000\t:  0.164170503616333 \t2.8434197902679443 \t0.10298788547515869\n",
      "Batch:\t17 /4000\t:  0.1040189266204834 \t2.9551289081573486 \t0.0\n",
      "Time: 5.015606880187988\t Loss: 7.95416784286499\t Valdacc: 0.39\t Testacc: 0.39\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  89 / 200\n",
      "Batch:\t0 /4000\t:  0.19098353385925293 \t0.19380664825439453 \t0.0\n",
      "Batch:\t1 /4000\t:  0.16115331649780273 \t0.35646891593933105 \t3.5046958923339844\n",
      "Batch:\t2 /4000\t:  0.1619713306427002 \t0.5199520587921143 \t0.20299911499023438\n",
      "Batch:\t3 /4000\t:  0.16529178619384766 \t0.6869611740112305 \t0.0\n",
      "Batch:\t4 /4000\t:  0.1672074794769287 \t0.8558578491210938 \t2.6076555252075195\n",
      "Batch:\t5 /4000\t:  0.18100214004516602 \t1.0386950969696045 \t0.11252403259277344\n",
      "Batch:\t6 /4000\t:  0.18256497383117676 \t1.2228939533233643 \t0.24270248413085938\n",
      "Batch:\t7 /4000\t:  0.1638774871826172 \t1.3882124423980713 \t1.4152336120605469\n",
      "Batch:\t8 /4000\t:  0.16146326065063477 \t1.5514311790466309 \t1.425248146057129\n",
      "Batch:\t9 /4000\t:  0.1651625633239746 \t1.7181239128112793 \t0.7647199630737305\n",
      "Batch:\t10 /4000\t:  0.1649482250213623 \t1.8845832347869873 \t0.0\n",
      "Batch:\t11 /4000\t:  0.1619584560394287 \t2.0479774475097656 \t0.0\n",
      "Batch:\t12 /4000\t:  0.21138954162597656 \t2.261284828186035 \t0.0\n",
      "Batch:\t13 /4000\t:  0.16391968727111816 \t2.4269776344299316 \t0.16949462890625\n",
      "Batch:\t14 /4000\t:  0.17911553382873535 \t2.607703685760498 \t0.0\n",
      "Batch:\t15 /4000\t:  0.1646137237548828 \t2.774200916290283 \t1.065195083618164\n",
      "Batch:\t16 /4000\t:  0.16175580024719238 \t2.937723159790039 \t0.5169086456298828\n",
      "Batch:\t17 /4000\t:  0.10400962829589844 \t3.0443053245544434 \t0.0\n",
      "Time: 5.174123048782349\t Loss: 12.027377128601074\t Valdacc: 0.43\t Testacc: 0.395\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  90 / 200\n",
      "Batch:\t0 /4000\t:  0.17088747024536133 \t0.17298030853271484 \t0.4993767738342285\n",
      "Batch:\t1 /4000\t:  0.17232251167297363 \t0.3467273712158203 \t0.377199649810791\n",
      "Batch:\t2 /4000\t:  0.18206453323364258 \t0.5309963226318359 \t4.740934371948242\n",
      "Batch:\t3 /4000\t:  0.1615617275238037 \t0.6939291954040527 \t1.099320888519287\n",
      "Batch:\t4 /4000\t:  0.1674044132232666 \t0.8631777763366699 \t0.0\n",
      "Batch:\t5 /4000\t:  0.1620938777923584 \t1.0275542736053467 \t0.0\n",
      "Batch:\t6 /4000\t:  0.20650410652160645 \t1.2355890274047852 \t2.2138328552246094\n",
      "Batch:\t7 /4000\t:  0.16227936744689941 \t1.3993546962738037 \t0.736086368560791\n",
      "Batch:\t8 /4000\t:  0.16765642166137695 \t1.5685036182403564 \t0.2833976745605469\n",
      "Batch:\t9 /4000\t:  0.16213440895080566 \t1.7324471473693848 \t0.7441930770874023\n",
      "Batch:\t10 /4000\t:  0.16265201568603516 \t1.8965835571289062 \t3.5615267753601074\n",
      "Batch:\t11 /4000\t:  0.16840744018554688 \t2.066692590713501 \t0.0\n",
      "Batch:\t12 /4000\t:  0.16463828086853027 \t2.233111619949341 \t2.6980419158935547\n",
      "Batch:\t13 /4000\t:  0.16510391235351562 \t2.400074005126953 \t0.5943751335144043\n",
      "Batch:\t14 /4000\t:  0.16132497787475586 \t2.562549591064453 \t3.5307626724243164\n",
      "Batch:\t15 /4000\t:  0.18166899681091309 \t2.746062994003296 \t4.816556930541992\n",
      "Batch:\t16 /4000\t:  0.16714835166931152 \t2.914813280105591 \t0.36107492446899414\n",
      "Batch:\t17 /4000\t:  0.10028719902038574 \t3.016430377960205 \t0.0\n",
      "Time: 5.024816274642944\t Loss: 26.256680011749268\t Valdacc: 0.39\t Testacc: 0.395\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  91 / 200\n",
      "Batch:\t0 /4000\t:  0.16447758674621582 \t0.1656346321105957 \t0.5549726486206055\n",
      "Batch:\t1 /4000\t:  0.20395636558532715 \t0.3711080551147461 \t0.0\n",
      "Batch:\t2 /4000\t:  0.16278290748596191 \t0.5351202487945557 \t0.41788673400878906\n",
      "Batch:\t3 /4000\t:  0.1594855785369873 \t0.6963939666748047 \t0.0\n",
      "Batch:\t4 /4000\t:  0.1715831756591797 \t0.8697433471679688 \t0.2594175338745117\n",
      "Batch:\t5 /4000\t:  0.20705175399780273 \t1.0784428119659424 \t0.8900623321533203\n",
      "Batch:\t6 /4000\t:  0.17292332649230957 \t1.252826452255249 \t0.15148353576660156\n",
      "Batch:\t7 /4000\t:  0.1592578887939453 \t1.4138333797454834 \t0.0\n",
      "Batch:\t8 /4000\t:  0.1616230010986328 \t1.5769257545471191 \t3.257638454437256\n",
      "Batch:\t9 /4000\t:  0.1679999828338623 \t1.7472643852233887 \t0.6681623458862305\n",
      "Batch:\t10 /4000\t:  0.16109895706176758 \t1.9098005294799805 \t0.0\n",
      "Batch:\t11 /4000\t:  0.16988563537597656 \t2.081498622894287 \t2.5427637100219727\n",
      "Batch:\t12 /4000\t:  0.1600799560546875 \t2.2436466217041016 \t0.0\n",
      "Batch:\t13 /4000\t:  0.19318366050720215 \t2.4385013580322266 \t0.014697074890136719\n",
      "Batch:\t14 /4000\t:  0.16611623764038086 \t2.606215715408325 \t0.2304859161376953\n",
      "Batch:\t15 /4000\t:  0.17868661880493164 \t2.7864463329315186 \t0.6162185668945312\n",
      "Batch:\t16 /4000\t:  0.16200685501098633 \t2.949909210205078 \t0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t17 /4000\t:  0.10248279571533203 \t3.0540785789489746 \t0.0\n",
      "Time: 5.033752679824829\t Loss: 9.60378885269165\t Valdacc: 0.39\t Testacc: 0.4\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  92 / 200\n",
      "Batch:\t0 /4000\t:  0.16465544700622559 \t0.16638779640197754 \t0.40178680419921875\n",
      "Batch:\t1 /4000\t:  0.1649188995361328 \t0.3326566219329834 \t0.5207405090332031\n",
      "Batch:\t2 /4000\t:  0.1639566421508789 \t0.4982640743255615 \t0.0\n",
      "Batch:\t3 /4000\t:  0.1656324863433838 \t0.6653501987457275 \t0.0\n",
      "Batch:\t4 /4000\t:  0.1727604866027832 \t0.840294599533081 \t0.20294570922851562\n",
      "Batch:\t5 /4000\t:  0.16620802879333496 \t1.008507490158081 \t0.8211450576782227\n",
      "Batch:\t6 /4000\t:  0.1660327911376953 \t1.1765501499176025 \t0.7600507736206055\n",
      "Batch:\t7 /4000\t:  0.20450282096862793 \t1.3826971054077148 \t1.8308029174804688\n",
      "Batch:\t8 /4000\t:  0.18306612968444824 \t1.567307472229004 \t0.8079376220703125\n",
      "Batch:\t9 /4000\t:  0.16424989700317383 \t1.7330679893493652 \t0.0\n",
      "Batch:\t10 /4000\t:  0.16266798973083496 \t1.8972456455230713 \t0.0\n",
      "Batch:\t11 /4000\t:  0.16309905052185059 \t2.062283754348755 \t4.509065628051758\n",
      "Batch:\t12 /4000\t:  0.18994569778442383 \t2.2540581226348877 \t0.0\n",
      "Batch:\t13 /4000\t:  0.15946221351623535 \t2.4149670600891113 \t0.0\n",
      "Batch:\t14 /4000\t:  0.16511821746826172 \t2.5821757316589355 \t0.0\n",
      "Batch:\t15 /4000\t:  0.1932227611541748 \t2.7771623134613037 \t0.0\n",
      "Batch:\t16 /4000\t:  0.16633224487304688 \t2.945171594619751 \t0.0\n",
      "Batch:\t17 /4000\t:  0.10138916969299316 \t3.047715187072754 \t0.0\n",
      "Time: 5.037346839904785\t Loss: 9.854475021362305\t Valdacc: 0.4\t Testacc: 0.385\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  93 / 200\n",
      "Batch:\t0 /4000\t:  0.15960097312927246 \t0.16101717948913574 \t0.0\n",
      "Batch:\t1 /4000\t:  0.1673569679260254 \t0.3297390937805176 \t0.8010044097900391\n",
      "Batch:\t2 /4000\t:  0.16365456581115723 \t0.49524998664855957 \t0.0\n",
      "Batch:\t3 /4000\t:  0.17106103897094727 \t0.6680612564086914 \t0.0\n",
      "Batch:\t4 /4000\t:  0.18061542510986328 \t0.8503901958465576 \t0.0\n",
      "Batch:\t5 /4000\t:  0.1609203815460205 \t1.0133602619171143 \t0.4074077606201172\n",
      "Batch:\t6 /4000\t:  0.17478060722351074 \t1.1896262168884277 \t0.0029134750366210938\n",
      "Batch:\t7 /4000\t:  0.16372275352478027 \t1.354888677597046 \t0.0\n",
      "Batch:\t8 /4000\t:  0.1819460391998291 \t1.5383577346801758 \t0.52203369140625\n",
      "Batch:\t9 /4000\t:  0.16560816764831543 \t1.705721378326416 \t0.0\n",
      "Batch:\t10 /4000\t:  0.1645519733428955 \t1.8719463348388672 \t2.7473278045654297\n",
      "Batch:\t11 /4000\t:  0.16857457160949707 \t2.041994333267212 \t1.2263507843017578\n",
      "Batch:\t12 /4000\t:  0.1658916473388672 \t2.2096757888793945 \t0.0\n",
      "Batch:\t13 /4000\t:  0.1627509593963623 \t2.373948097229004 \t3.0158581733703613\n",
      "Batch:\t14 /4000\t:  0.1636655330657959 \t2.539268970489502 \t0.0\n",
      "Batch:\t15 /4000\t:  0.20770478248596191 \t2.748739719390869 \t0.0\n",
      "Batch:\t16 /4000\t:  0.16267156600952148 \t2.913330078125 \t1.2726316452026367\n",
      "Batch:\t17 /4000\t:  0.10646271705627441 \t3.0214905738830566 \t0.7948837280273438\n",
      "Time: 4.92645525932312\t Loss: 10.790411472320557\t Valdacc: 0.39\t Testacc: 0.405\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  94 / 200\n",
      "Batch:\t0 /4000\t:  0.16434192657470703 \t0.16518664360046387 \t0.41146135330200195\n",
      "Batch:\t1 /4000\t:  0.16473817825317383 \t0.3314080238342285 \t3.267841339111328\n",
      "Batch:\t2 /4000\t:  0.2031545639038086 \t0.536517858505249 \t0.0\n",
      "Batch:\t3 /4000\t:  0.15916895866394043 \t0.6970672607421875 \t0.0\n",
      "Batch:\t4 /4000\t:  0.16608262062072754 \t0.8645970821380615 \t0.0\n",
      "Batch:\t5 /4000\t:  0.16549348831176758 \t1.032020092010498 \t2.1288280487060547\n",
      "Batch:\t6 /4000\t:  0.16421985626220703 \t1.1978747844696045 \t1.4849824905395508\n",
      "Batch:\t7 /4000\t:  0.16549324989318848 \t1.3654060363769531 \t0.17226123809814453\n",
      "Batch:\t8 /4000\t:  0.16074371337890625 \t1.5279300212860107 \t0.0\n",
      "Batch:\t9 /4000\t:  0.15908408164978027 \t1.688645839691162 \t0.0\n",
      "Batch:\t10 /4000\t:  0.16451072692871094 \t1.854647159576416 \t0.7202062606811523\n",
      "Batch:\t11 /4000\t:  0.162445068359375 \t2.0187747478485107 \t2.194369316101074\n",
      "Batch:\t12 /4000\t:  0.16144537925720215 \t2.1816799640655518 \t0.9900696277618408\n",
      "Batch:\t13 /4000\t:  0.16247272491455078 \t2.3460981845855713 \t0.9860916137695312\n",
      "Batch:\t14 /4000\t:  0.15880775451660156 \t2.5063247680664062 \t0.0\n",
      "Batch:\t15 /4000\t:  0.20459699630737305 \t2.7126245498657227 \t0.3290691375732422\n",
      "Batch:\t16 /4000\t:  0.1631941795349121 \t2.8777542114257812 \t0.5250790119171143\n",
      "Batch:\t17 /4000\t:  0.1006009578704834 \t2.9795472621917725 \t0.0\n",
      "Time: 4.869908332824707\t Loss: 13.210259437561035\t Valdacc: 0.39\t Testacc: 0.4\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  95 / 200\n",
      "Batch:\t0 /4000\t:  0.1625525951385498 \t0.16422128677368164 \t3.5138425827026367\n",
      "Batch:\t1 /4000\t:  0.16358566284179688 \t0.3297543525695801 \t0.705449104309082\n",
      "Batch:\t2 /4000\t:  0.16630268096923828 \t0.49752235412597656 \t1.999272346496582\n",
      "Batch:\t3 /4000\t:  0.158599853515625 \t0.6574287414550781 \t0.0\n",
      "Batch:\t4 /4000\t:  0.16635370254516602 \t0.8253262042999268 \t0.4999704360961914\n",
      "Batch:\t5 /4000\t:  0.20379257202148438 \t1.0306894779205322 \t2.031247138977051\n",
      "Batch:\t6 /4000\t:  0.1621077060699463 \t1.1946046352386475 \t2.320718765258789\n",
      "Batch:\t7 /4000\t:  0.16661310195922852 \t1.3626940250396729 \t0.0\n",
      "Batch:\t8 /4000\t:  0.1797940731048584 \t1.5443918704986572 \t0.0\n",
      "Batch:\t9 /4000\t:  0.16207003593444824 \t1.7081773281097412 \t1.2587299346923828\n",
      "Batch:\t10 /4000\t:  0.16524410247802734 \t1.8749244213104248 \t0.0\n",
      "Batch:\t11 /4000\t:  0.183671236038208 \t2.060509204864502 \t0.32839107513427734\n",
      "Batch:\t12 /4000\t:  0.16589117050170898 \t2.227848768234253 \t0.0\n",
      "Batch:\t13 /4000\t:  0.162184476852417 \t2.3918886184692383 \t0.21378707885742188\n",
      "Batch:\t14 /4000\t:  0.1673736572265625 \t2.5610275268554688 \t1.271073341369629\n",
      "Batch:\t15 /4000\t:  0.1915891170501709 \t2.755171060562134 \t0.0\n",
      "Batch:\t16 /4000\t:  0.1647052764892578 \t2.9215667247772217 \t1.3119869232177734\n",
      "Batch:\t17 /4000\t:  0.09969854354858398 \t3.027404546737671 \t0.0\n",
      "Time: 4.970490217208862\t Loss: 15.454468727111816\t Valdacc: 0.41\t Testacc: 0.39\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  96 / 200\n",
      "Batch:\t0 /4000\t:  0.17287206649780273 \t0.17367029190063477 \t0.4470396041870117\n",
      "Batch:\t1 /4000\t:  0.16622281074523926 \t0.34182238578796387 \t3.5763206481933594\n",
      "Batch:\t2 /4000\t:  0.19036173820495605 \t0.5407376289367676 \t1.5758342742919922\n",
      "Batch:\t3 /4000\t:  0.16071844100952148 \t0.7053780555725098 \t3.2680845260620117\n",
      "Batch:\t4 /4000\t:  0.17891240119934082 \t0.8855941295623779 \t0.0\n",
      "Batch:\t5 /4000\t:  0.17783284187316895 \t1.0651273727416992 \t1.1655158996582031\n",
      "Batch:\t6 /4000\t:  0.16620850563049316 \t1.2326650619506836 \t0.8750495910644531\n",
      "Batch:\t7 /4000\t:  0.16363906860351562 \t1.3976726531982422 \t0.0\n",
      "Batch:\t8 /4000\t:  0.18514466285705566 \t1.5844979286193848 \t0.233673095703125\n",
      "Batch:\t9 /4000\t:  0.1664738655090332 \t1.7524220943450928 \t0.7372894287109375\n",
      "Batch:\t10 /4000\t:  0.16179728507995605 \t1.9159328937530518 \t0.0\n",
      "Batch:\t11 /4000\t:  0.16392850875854492 \t2.0816991329193115 \t2.0757789611816406\n",
      "Batch:\t12 /4000\t:  0.16093182563781738 \t2.244070529937744 \t0.9818973541259766\n",
      "Batch:\t13 /4000\t:  0.17864537239074707 \t2.424201011657715 \t0.0\n",
      "Batch:\t14 /4000\t:  0.16582179069519043 \t2.591656446456909 \t1.7805042266845703\n",
      "Batch:\t15 /4000\t:  0.16768670082092285 \t2.7611632347106934 \t0.3826560974121094\n",
      "Batch:\t16 /4000\t:  0.1882925033569336 \t2.9511656761169434 \t1.1258716583251953\n",
      "Batch:\t17 /4000\t:  0.10391497611999512 \t3.0563509464263916 \t0.9123497009277344\n",
      "Time: 4.938988208770752\t Loss: 19.13786506652832\t Valdacc: 0.41\t Testacc: 0.38\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  97 / 200\n",
      "Batch:\t0 /4000\t:  0.1630096435546875 \t0.1650247573852539 \t1.1713523864746094\n",
      "Batch:\t1 /4000\t:  0.2080855369567871 \t0.37500619888305664 \t1.1769475936889648\n",
      "Batch:\t2 /4000\t:  0.19420099258422852 \t0.5706639289855957 \t0.29598140716552734\n",
      "Batch:\t3 /4000\t:  0.16096806526184082 \t0.732994556427002 \t2.1900110244750977\n",
      "Batch:\t4 /4000\t:  0.1737384796142578 \t0.9085550308227539 \t0.0\n",
      "Batch:\t5 /4000\t:  0.16333317756652832 \t1.0733051300048828 \t0.07941818237304688\n",
      "Batch:\t6 /4000\t:  0.16577839851379395 \t1.2408740520477295 \t1.107522964477539\n",
      "Batch:\t7 /4000\t:  0.1694340705871582 \t1.4120826721191406 \t0.9310417175292969\n",
      "Batch:\t8 /4000\t:  0.16408419609069824 \t1.5775744915008545 \t1.485971450805664\n",
      "Batch:\t9 /4000\t:  0.16552162170410156 \t1.7446563243865967 \t1.899177074432373\n",
      "Batch:\t10 /4000\t:  0.1686267852783203 \t1.9151389598846436 \t0.0\n",
      "Batch:\t11 /4000\t:  0.16201281547546387 \t2.0785088539123535 \t1.9010229110717773\n",
      "Batch:\t12 /4000\t:  0.1621992588043213 \t2.2421045303344727 \t0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t13 /4000\t:  0.16377949714660645 \t2.407855987548828 \t0.5928425788879395\n",
      "Batch:\t14 /4000\t:  0.16083717346191406 \t2.5701422691345215 \t0.03634214401245117\n",
      "Batch:\t15 /4000\t:  0.15826082229614258 \t2.7298195362091064 \t0.0\n",
      "Batch:\t16 /4000\t:  0.1605677604675293 \t2.892077922821045 \t0.032505035400390625\n",
      "Batch:\t17 /4000\t:  0.10192513465881348 \t2.995180130004883 \t2.392918586730957\n",
      "Time: 4.924261569976807\t Loss: 15.293055057525635\t Valdacc: 0.36\t Testacc: 0.38\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  98 / 200\n",
      "Batch:\t0 /4000\t:  0.16862249374389648 \t0.17082500457763672 \t2.945261001586914\n",
      "Batch:\t1 /4000\t:  0.1601426601409912 \t0.3325061798095703 \t1.524249792098999\n",
      "Batch:\t2 /4000\t:  0.20304298400878906 \t0.5369384288787842 \t0.13271141052246094\n",
      "Batch:\t3 /4000\t:  0.16392064094543457 \t0.7023389339447021 \t0.0\n",
      "Batch:\t4 /4000\t:  0.1663215160369873 \t0.8699855804443359 \t0.5230093002319336\n",
      "Batch:\t5 /4000\t:  0.1665055751800537 \t1.0378618240356445 \t0.8664827346801758\n",
      "Batch:\t6 /4000\t:  0.18095731735229492 \t1.2204196453094482 \t0.4152517318725586\n",
      "Batch:\t7 /4000\t:  0.16128110885620117 \t1.383587121963501 \t0.722599983215332\n",
      "Batch:\t8 /4000\t:  0.16359829902648926 \t1.54866623878479 \t0.877197265625\n",
      "Batch:\t9 /4000\t:  0.17432141304016113 \t1.7248530387878418 \t1.603579044342041\n",
      "Batch:\t10 /4000\t:  0.16207265853881836 \t1.8889858722686768 \t1.4478893280029297\n",
      "Batch:\t11 /4000\t:  0.16469168663024902 \t2.0551536083221436 \t1.2242116928100586\n",
      "Batch:\t12 /4000\t:  0.19634318351745605 \t2.2612433433532715 \t2.830091953277588\n",
      "Batch:\t13 /4000\t:  0.16190099716186523 \t2.424556255340576 \t0.0\n",
      "Batch:\t14 /4000\t:  0.16692280769348145 \t2.5926804542541504 \t0.5172309875488281\n",
      "Batch:\t15 /4000\t:  0.18620562553405762 \t2.7808122634887695 \t0.0\n",
      "Batch:\t16 /4000\t:  0.16677260398864746 \t2.9489920139312744 \t1.1423759460449219\n",
      "Batch:\t17 /4000\t:  0.10731029510498047 \t3.0576257705688477 \t0.0\n",
      "Time: 4.9223034381866455\t Loss: 16.77214217185974\t Valdacc: 0.4\t Testacc: 0.395\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  99 / 200\n",
      "Batch:\t0 /4000\t:  0.16293072700500488 \t0.16452789306640625 \t0.9746599197387695\n",
      "Batch:\t1 /4000\t:  0.1657571792602539 \t0.3319883346557617 \t1.610433578491211\n",
      "Batch:\t2 /4000\t:  0.16338515281677246 \t0.49684858322143555 \t0.0\n",
      "Batch:\t3 /4000\t:  0.15869975090026855 \t0.6567935943603516 \t0.0\n",
      "Batch:\t4 /4000\t:  0.17792630195617676 \t0.836193323135376 \t0.0\n",
      "Batch:\t5 /4000\t:  0.16524577140808105 \t1.0032150745391846 \t0.0\n",
      "Batch:\t6 /4000\t:  0.16086578369140625 \t1.1654722690582275 \t0.5095691680908203\n",
      "Batch:\t7 /4000\t:  0.16106629371643066 \t1.3279001712799072 \t0.0\n",
      "Batch:\t8 /4000\t:  0.16276097297668457 \t1.491828203201294 \t3.0507254600524902\n",
      "Batch:\t9 /4000\t:  0.18081164360046387 \t1.6739084720611572 \t2.5752620697021484\n",
      "Batch:\t10 /4000\t:  0.18780517578125 \t1.8633382320404053 \t1.6784296035766602\n",
      "Batch:\t11 /4000\t:  0.17494893074035645 \t2.039517641067505 \t0.0\n",
      "Batch:\t12 /4000\t:  0.16907763481140137 \t2.2101218700408936 \t0.7606172561645508\n",
      "Batch:\t13 /4000\t:  0.16537904739379883 \t2.3769171237945557 \t1.7888660430908203\n",
      "Batch:\t14 /4000\t:  0.1683816909790039 \t2.5466156005859375 \t0.9141426086425781\n",
      "Batch:\t15 /4000\t:  0.16753053665161133 \t2.715879440307617 \t0.1583251953125\n",
      "Batch:\t16 /4000\t:  0.15803170204162598 \t2.8755078315734863 \t0.0\n",
      "Batch:\t17 /4000\t:  0.10351991653442383 \t2.9801435470581055 \t0.0\n",
      "Time: 4.860879898071289\t Loss: 14.021030902862549\t Valdacc: 0.43\t Testacc: 0.4\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  100 / 200\n",
      "Batch:\t0 /4000\t:  0.1611006259918213 \t0.16296744346618652 \t1.1816024780273438\n",
      "Batch:\t1 /4000\t:  0.16304397583007812 \t0.3275187015533447 \t1.2301340103149414\n",
      "Batch:\t2 /4000\t:  0.15941905975341797 \t0.4886491298675537 \t0.0\n",
      "Batch:\t3 /4000\t:  0.1608431339263916 \t0.6505684852600098 \t0.5089836120605469\n",
      "Batch:\t4 /4000\t:  0.17496395111083984 \t0.8270959854125977 \t2.2336015701293945\n",
      "Batch:\t5 /4000\t:  0.18205952644348145 \t1.0122056007385254 \t0.9023151397705078\n",
      "Batch:\t6 /4000\t:  0.15854454040527344 \t1.1723225116729736 \t0.0\n",
      "Batch:\t7 /4000\t:  0.16077303886413574 \t1.334745168685913 \t3.9225826263427734\n",
      "Batch:\t8 /4000\t:  0.1676194667816162 \t1.504037618637085 \t0.0\n",
      "Batch:\t9 /4000\t:  0.16061067581176758 \t1.6662344932556152 \t0.6221332550048828\n",
      "Batch:\t10 /4000\t:  0.1657238006591797 \t1.8333113193511963 \t1.0925064086914062\n",
      "Batch:\t11 /4000\t:  0.16353678703308105 \t1.9980909824371338 \t0.0\n",
      "Batch:\t12 /4000\t:  0.16729426383972168 \t2.167086124420166 \t0.9266490936279297\n",
      "Batch:\t13 /4000\t:  0.15858149528503418 \t2.3269691467285156 \t0.0\n",
      "Batch:\t14 /4000\t:  0.16069436073303223 \t2.4893178939819336 \t0.10693073272705078\n",
      "Batch:\t15 /4000\t:  0.16069483757019043 \t2.6513001918792725 \t1.6808719635009766\n",
      "Batch:\t16 /4000\t:  0.1615307331085205 \t2.81459641456604 \t0.0\n",
      "Batch:\t17 /4000\t:  0.09998226165771484 \t2.9159510135650635 \t0.0\n",
      "Time: 4.821810245513916\t Loss: 14.408310890197754\t Valdacc: 0.41\t Testacc: 0.39\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  101 / 200\n",
      "Batch:\t0 /4000\t:  0.16393089294433594 \t0.16475558280944824 \t1.0615425109863281\n",
      "Batch:\t1 /4000\t:  0.16254878044128418 \t0.3284916877746582 \t0.9690656661987305\n",
      "Batch:\t2 /4000\t:  0.1856982707977295 \t0.5156834125518799 \t0.36255931854248047\n",
      "Batch:\t3 /4000\t:  0.1580660343170166 \t0.6752583980560303 \t0.0\n",
      "Batch:\t4 /4000\t:  0.16562294960021973 \t0.8421988487243652 \t0.0\n",
      "Batch:\t5 /4000\t:  0.17358899116516113 \t1.017198085784912 \t0.36398887634277344\n",
      "Batch:\t6 /4000\t:  0.18211746215820312 \t1.2011306285858154 \t1.4899625778198242\n",
      "Batch:\t7 /4000\t:  0.1608257293701172 \t1.3633079528808594 \t1.116678237915039\n",
      "Batch:\t8 /4000\t:  0.19797945022583008 \t1.5665743350982666 \t3.4890923500061035\n",
      "Batch:\t9 /4000\t:  0.16121268272399902 \t1.7291219234466553 \t0.0\n",
      "Batch:\t10 /4000\t:  0.20391416549682617 \t1.9346859455108643 \t3.563436508178711\n",
      "Batch:\t11 /4000\t:  0.1706695556640625 \t2.1104137897491455 \t0.0\n",
      "Batch:\t12 /4000\t:  0.16397666931152344 \t2.275656223297119 \t0.0\n",
      "Batch:\t13 /4000\t:  0.16106891632080078 \t2.4381086826324463 \t1.366347312927246\n",
      "Batch:\t14 /4000\t:  0.17781877517700195 \t2.619502544403076 \t0.0\n",
      "Batch:\t15 /4000\t:  0.16283726692199707 \t2.7834768295288086 \t0.4245882034301758\n",
      "Batch:\t16 /4000\t:  0.1698007583618164 \t2.9545986652374268 \t0.4463186264038086\n",
      "Batch:\t17 /4000\t:  0.10251688957214355 \t3.0584776401519775 \t1.5272352695465088\n",
      "Time: 4.970630407333374\t Loss: 16.18081545829773\t Valdacc: 0.41\t Testacc: 0.385\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  102 / 200\n",
      "Batch:\t0 /4000\t:  0.1607058048248291 \t0.16296792030334473 \t1.124511480331421\n",
      "Batch:\t1 /4000\t:  0.16170620918273926 \t0.32651805877685547 \t0.0\n",
      "Batch:\t2 /4000\t:  0.16573190689086914 \t0.49361681938171387 \t0.5271205902099609\n",
      "Batch:\t3 /4000\t:  0.16064214706420898 \t0.6554598808288574 \t0.21277999877929688\n",
      "Batch:\t4 /4000\t:  0.15947651863098145 \t0.8167355060577393 \t0.0\n",
      "Batch:\t5 /4000\t:  0.17906975746154785 \t0.9974536895751953 \t0.6097736358642578\n",
      "Batch:\t6 /4000\t:  0.15833735466003418 \t1.1572279930114746 \t0.0\n",
      "Batch:\t7 /4000\t:  0.16237330436706543 \t1.321061372756958 \t3.360774040222168\n",
      "Batch:\t8 /4000\t:  0.16002655029296875 \t1.482386827468872 \t0.31234121322631836\n",
      "Batch:\t9 /4000\t:  0.16491317749023438 \t1.6485216617584229 \t0.0\n",
      "Batch:\t10 /4000\t:  0.16838932037353516 \t1.8182249069213867 \t0.3701133728027344\n",
      "Batch:\t11 /4000\t:  0.17150497436523438 \t1.991788625717163 \t1.561234474182129\n",
      "Batch:\t12 /4000\t:  0.16622281074523926 \t2.1596484184265137 \t3.0131912231445312\n",
      "Batch:\t13 /4000\t:  0.1606612205505371 \t2.3215835094451904 \t1.5875930786132812\n",
      "Batch:\t14 /4000\t:  0.2053670883178711 \t2.5283775329589844 \t0.0\n",
      "Batch:\t15 /4000\t:  0.16010379791259766 \t2.689939260482788 \t2.7101516723632812\n",
      "Batch:\t16 /4000\t:  0.16568398475646973 \t2.856844663619995 \t0.6783351898193359\n",
      "Batch:\t17 /4000\t:  0.1131749153137207 \t2.9713268280029297 \t0.0\n",
      "Time: 4.843611240386963\t Loss: 16.067919969558716\t Valdacc: 0.43\t Testacc: 0.4\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  103 / 200\n",
      "Batch:\t0 /4000\t:  0.16652941703796387 \t0.1673872470855713 \t1.4163777828216553\n",
      "Batch:\t1 /4000\t:  0.16664791107177734 \t0.33516573905944824 \t0.6035451889038086\n",
      "Batch:\t2 /4000\t:  0.16593170166015625 \t0.5026137828826904 \t0.0\n",
      "Batch:\t3 /4000\t:  0.16500568389892578 \t0.66922926902771 \t0.7050085067749023\n",
      "Batch:\t4 /4000\t:  0.16372394561767578 \t0.8344380855560303 \t1.1279687881469727\n",
      "Batch:\t5 /4000\t:  0.18256330490112305 \t1.0188548564910889 \t0.0\n",
      "Batch:\t6 /4000\t:  0.16794300079345703 \t1.188378095626831 \t0.9122037887573242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t7 /4000\t:  0.16289329528808594 \t1.3524413108825684 \t3.5453076362609863\n",
      "Batch:\t8 /4000\t:  0.16783857345581055 \t1.5218286514282227 \t0.0\n",
      "Batch:\t9 /4000\t:  0.16954255104064941 \t1.693011999130249 \t1.8325109481811523\n",
      "Batch:\t10 /4000\t:  0.1782841682434082 \t1.8727190494537354 \t0.0\n",
      "Batch:\t11 /4000\t:  0.16709256172180176 \t2.041144371032715 \t0.4297924041748047\n",
      "Batch:\t12 /4000\t:  0.16051173210144043 \t2.203392744064331 \t0.638275146484375\n",
      "Batch:\t13 /4000\t:  0.16188526153564453 \t2.368762254714966 \t0.4473562240600586\n",
      "Batch:\t14 /4000\t:  0.16390037536621094 \t2.53434419631958 \t0.0\n",
      "Batch:\t15 /4000\t:  0.17898011207580566 \t2.7149441242218018 \t0.0\n",
      "Batch:\t16 /4000\t:  0.16871213912963867 \t2.885093927383423 \t0.553156852722168\n",
      "Batch:\t17 /4000\t:  0.1065208911895752 \t2.9933838844299316 \t0.2470111846923828\n",
      "Time: 4.920405626296997\t Loss: 12.45851445198059\t Valdacc: 0.41\t Testacc: 0.375\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  104 / 200\n",
      "Batch:\t0 /4000\t:  0.15881085395812988 \t0.16037440299987793 \t0.0\n",
      "Batch:\t1 /4000\t:  0.16495227813720703 \t0.32721662521362305 \t1.2291202545166016\n",
      "Batch:\t2 /4000\t:  0.16416668891906738 \t0.4928865432739258 \t0.0\n",
      "Batch:\t3 /4000\t:  0.1605818271636963 \t0.654841423034668 \t0.6204738616943359\n",
      "Batch:\t4 /4000\t:  0.16483616828918457 \t0.8210504055023193 \t1.0859298706054688\n",
      "Batch:\t5 /4000\t:  0.16458582878112793 \t0.9874210357666016 \t0.0\n",
      "Batch:\t6 /4000\t:  0.16645312309265137 \t1.1557114124298096 \t0.0\n",
      "Batch:\t7 /4000\t:  0.1673145294189453 \t1.3244743347167969 \t1.940643310546875\n",
      "Batch:\t8 /4000\t:  0.1805121898651123 \t1.5067431926727295 \t0.0\n",
      "Batch:\t9 /4000\t:  0.16433262825012207 \t1.6725189685821533 \t3.47613525390625\n",
      "Batch:\t10 /4000\t:  0.16788268089294434 \t1.8420367240905762 \t0.5785961151123047\n",
      "Batch:\t11 /4000\t:  0.18549180030822754 \t2.0341200828552246 \t0.08738899230957031\n",
      "Batch:\t12 /4000\t:  0.1668696403503418 \t2.202813148498535 \t0.1394786834716797\n",
      "Batch:\t13 /4000\t:  0.16289734840393066 \t2.3675317764282227 \t0.0\n",
      "Batch:\t14 /4000\t:  0.15921449661254883 \t2.528602123260498 \t0.0\n",
      "Batch:\t15 /4000\t:  0.16271352767944336 \t2.6929376125335693 \t0.5716915130615234\n",
      "Batch:\t16 /4000\t:  0.1611776351928711 \t2.8555665016174316 \t1.7138338088989258\n",
      "Batch:\t17 /4000\t:  0.10024714469909668 \t2.957266092300415 \t0.0\n",
      "Time: 4.8884735107421875\t Loss: 11.443291664123535\t Valdacc: 0.4\t Testacc: 0.395\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  105 / 200\n",
      "Batch:\t0 /4000\t:  0.15955829620361328 \t0.16201162338256836 \t0.0\n",
      "Batch:\t1 /4000\t:  0.16098809242248535 \t0.3245117664337158 \t2.4661130905151367\n",
      "Batch:\t2 /4000\t:  0.15889549255371094 \t0.4851565361022949 \t0.0\n",
      "Batch:\t3 /4000\t:  0.16697430610656738 \t0.6535041332244873 \t3.164785385131836\n",
      "Batch:\t4 /4000\t:  0.16554808616638184 \t0.8210976123809814 \t0.8634777069091797\n",
      "Batch:\t5 /4000\t:  0.16855406761169434 \t0.991417407989502 \t0.587824821472168\n",
      "Batch:\t6 /4000\t:  0.18139195442199707 \t1.1742925643920898 \t1.1618366241455078\n",
      "Batch:\t7 /4000\t:  0.16220521926879883 \t1.3380136489868164 \t0.0\n",
      "Batch:\t8 /4000\t:  0.16682982444763184 \t1.5065522193908691 \t0.0\n",
      "Batch:\t9 /4000\t:  0.16077542304992676 \t1.6687726974487305 \t0.5803394317626953\n",
      "Batch:\t10 /4000\t:  0.16283631324768066 \t1.8331811428070068 \t0.4533348083496094\n",
      "Batch:\t11 /4000\t:  0.15961217880249023 \t1.9945368766784668 \t0.0\n",
      "Batch:\t12 /4000\t:  0.17232203483581543 \t2.1687018871307373 \t0.9122037887573242\n",
      "Batch:\t13 /4000\t:  0.1664292812347412 \t2.3368873596191406 \t3.5703420639038086\n",
      "Batch:\t14 /4000\t:  0.16397857666015625 \t2.502671957015991 \t1.9545116424560547\n",
      "Batch:\t15 /4000\t:  0.16046452522277832 \t2.6650466918945312 \t0.0\n",
      "Batch:\t16 /4000\t:  0.1716322898864746 \t2.838353395462036 \t0.3748159408569336\n",
      "Batch:\t17 /4000\t:  0.11108088493347168 \t2.9509432315826416 \t0.0\n",
      "Time: 4.8666253089904785\t Loss: 16.089585304260254\t Valdacc: 0.42\t Testacc: 0.4\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  106 / 200\n",
      "Batch:\t0 /4000\t:  0.16619253158569336 \t0.16877102851867676 \t0.09856843948364258\n",
      "Batch:\t1 /4000\t:  0.16864895820617676 \t0.3388965129852295 \t0.18389272689819336\n",
      "Batch:\t2 /4000\t:  0.1589360237121582 \t0.49921083450317383 \t0.0\n",
      "Batch:\t3 /4000\t:  0.1649150848388672 \t0.6657900810241699 \t0.0\n",
      "Batch:\t4 /4000\t:  0.16696977615356445 \t0.834522008895874 \t0.0\n",
      "Batch:\t5 /4000\t:  0.16945815086364746 \t1.005608320236206 \t0.0\n",
      "Batch:\t6 /4000\t:  0.15944218635559082 \t1.1669399738311768 \t0.0\n",
      "Batch:\t7 /4000\t:  0.2032151222229004 \t1.3715550899505615 \t0.0\n",
      "Batch:\t8 /4000\t:  0.16480040550231934 \t1.540140151977539 \t2.940298557281494\n",
      "Batch:\t9 /4000\t:  0.17340397834777832 \t1.7149920463562012 \t1.8817920684814453\n",
      "Batch:\t10 /4000\t:  0.1618642807006836 \t1.8783659934997559 \t0.35239410400390625\n",
      "Batch:\t11 /4000\t:  0.16419219970703125 \t2.044139862060547 \t0.0\n",
      "Batch:\t12 /4000\t:  0.1652665138244629 \t2.2109389305114746 \t0.6951065063476562\n",
      "Batch:\t13 /4000\t:  0.16258740425109863 \t2.3752989768981934 \t0.0\n",
      "Batch:\t14 /4000\t:  0.16947436332702637 \t2.5467193126678467 \t0.5615882873535156\n",
      "Batch:\t15 /4000\t:  0.16465973854064941 \t2.7129969596862793 \t1.7257647514343262\n",
      "Batch:\t16 /4000\t:  0.2025618553161621 \t2.917276620864868 \t0.0\n",
      "Batch:\t17 /4000\t:  0.10404419898986816 \t3.02286696434021 \t0.05251574516296387\n",
      "Time: 4.929421663284302\t Loss: 8.491921186447144\t Valdacc: 0.39\t Testacc: 0.39\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  107 / 200\n",
      "Batch:\t0 /4000\t:  0.20160508155822754 \t0.2037804126739502 \t0.0\n",
      "Batch:\t1 /4000\t:  0.16435718536376953 \t0.3698263168334961 \t0.24904632568359375\n",
      "Batch:\t2 /4000\t:  0.16562891006469727 \t0.5368847846984863 \t0.0\n",
      "Batch:\t3 /4000\t:  0.16508150100708008 \t0.7037148475646973 \t2.7742671966552734\n",
      "Batch:\t4 /4000\t:  0.15917015075683594 \t0.8646113872528076 \t0.0\n",
      "Batch:\t5 /4000\t:  0.16476058959960938 \t1.0308432579040527 \t0.0\n",
      "Batch:\t6 /4000\t:  0.1748974323272705 \t1.2074048519134521 \t0.0\n",
      "Batch:\t7 /4000\t:  0.16128206253051758 \t1.3705544471740723 \t0.3222033977508545\n",
      "Batch:\t8 /4000\t:  0.158616304397583 \t1.5305464267730713 \t0.0\n",
      "Batch:\t9 /4000\t:  0.17397475242614746 \t1.7090110778808594 \t3.374919891357422\n",
      "Batch:\t10 /4000\t:  0.1617274284362793 \t1.8724699020385742 \t1.0052070617675781\n",
      "Batch:\t11 /4000\t:  0.16408491134643555 \t2.038482427597046 \t0.7341527938842773\n",
      "Batch:\t12 /4000\t:  0.1795508861541748 \t2.219632863998413 \t0.63128662109375\n",
      "Batch:\t13 /4000\t:  0.162825345993042 \t2.3838422298431396 \t0.0\n",
      "Batch:\t14 /4000\t:  0.16126632690429688 \t2.549828052520752 \t3.0097899436950684\n",
      "Batch:\t15 /4000\t:  0.16720843315124512 \t2.7187764644622803 \t0.4380970001220703\n",
      "Batch:\t16 /4000\t:  0.16204404830932617 \t2.8822624683380127 \t2.246548652648926\n",
      "Batch:\t17 /4000\t:  0.1038963794708252 \t2.987717390060425 \t0.0\n",
      "Time: 5.117925405502319\t Loss: 14.785518884658813\t Valdacc: 0.42\t Testacc: 0.405\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  108 / 200\n",
      "Batch:\t0 /4000\t:  0.1638627052307129 \t0.16560053825378418 \t0.5107688903808594\n",
      "Batch:\t1 /4000\t:  0.16887426376342773 \t0.33649468421936035 \t0.0\n",
      "Batch:\t2 /4000\t:  0.1632246971130371 \t0.5013608932495117 \t1.7319145202636719\n",
      "Batch:\t3 /4000\t:  0.1588430404663086 \t0.661590576171875 \t0.0\n",
      "Batch:\t4 /4000\t:  0.16377663612365723 \t0.8268511295318604 \t0.8015155792236328\n",
      "Batch:\t5 /4000\t:  0.16372346878051758 \t0.9919638633728027 \t1.7175397872924805\n",
      "Batch:\t6 /4000\t:  0.17325210571289062 \t1.166990041732788 \t3.132650852203369\n",
      "Batch:\t7 /4000\t:  0.16173529624938965 \t1.330066442489624 \t0.23826313018798828\n",
      "Batch:\t8 /4000\t:  0.20967435836791992 \t1.5412774085998535 \t0.4992809295654297\n",
      "Batch:\t9 /4000\t:  0.16491436958312988 \t1.7079713344573975 \t0.24059200286865234\n",
      "Batch:\t10 /4000\t:  0.19387531280517578 \t1.903357982635498 \t4.509560585021973\n",
      "Batch:\t11 /4000\t:  0.1675584316253662 \t2.072443723678589 \t1.098555564880371\n",
      "Batch:\t12 /4000\t:  0.15979313850402832 \t2.2338616847991943 \t0.0\n",
      "Batch:\t13 /4000\t:  0.17987751960754395 \t2.415400743484497 \t0.8583831787109375\n",
      "Batch:\t14 /4000\t:  0.1832869052886963 \t2.600470781326294 \t0.0\n",
      "Batch:\t15 /4000\t:  0.16675162315368652 \t2.769171714782715 \t2.351994514465332\n",
      "Batch:\t16 /4000\t:  0.17314529418945312 \t2.943786859512329 \t0.532012939453125\n",
      "Batch:\t17 /4000\t:  0.10221648216247559 \t3.0475988388061523 \t0.0\n",
      "Time: 5.069272756576538\t Loss: 18.223032474517822\t Valdacc: 0.42\t Testacc: 0.415\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  109 / 200\n",
      "Batch:\t0 /4000\t:  0.17540502548217773 \t0.17797160148620605 \t0.42194557189941406\n",
      "Batch:\t1 /4000\t:  0.16245508193969727 \t0.3422834873199463 \t4.35101318359375\n",
      "Batch:\t2 /4000\t:  0.16188502311706543 \t0.5056846141815186 \t0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t3 /4000\t:  0.16530776023864746 \t0.6724402904510498 \t0.0\n",
      "Batch:\t4 /4000\t:  0.16866517066955566 \t0.8429369926452637 \t0.0\n",
      "Batch:\t5 /4000\t:  0.16772794723510742 \t1.012033224105835 \t1.082921028137207\n",
      "Batch:\t6 /4000\t:  0.16658759117126465 \t1.1804957389831543 \t0.0\n",
      "Batch:\t7 /4000\t:  0.16211962699890137 \t1.343937873840332 \t1.0369024276733398\n",
      "Batch:\t8 /4000\t:  0.16055846214294434 \t1.506072759628296 \t0.0\n",
      "Batch:\t9 /4000\t:  0.1650981903076172 \t1.6735444068908691 \t0.0\n",
      "Batch:\t10 /4000\t:  0.16568970680236816 \t1.8412878513336182 \t2.2235336303710938\n",
      "Batch:\t11 /4000\t:  0.16843843460083008 \t2.011131525039673 \t0.0\n",
      "Batch:\t12 /4000\t:  0.16099214553833008 \t2.1763999462127686 \t0.0\n",
      "Batch:\t13 /4000\t:  0.16527414321899414 \t2.3431434631347656 \t0.012958526611328125\n",
      "Batch:\t14 /4000\t:  0.16383600234985352 \t2.5088961124420166 \t0.0\n",
      "Batch:\t15 /4000\t:  0.20531249046325684 \t2.716033697128296 \t1.3531436920166016\n",
      "Batch:\t16 /4000\t:  0.1596205234527588 \t2.87751841545105 \t0.0\n",
      "Batch:\t17 /4000\t:  0.10325503349304199 \t2.9820423126220703 \t1.0135526657104492\n",
      "Time: 4.9876368045806885\t Loss: 11.495970726013184\t Valdacc: 0.43\t Testacc: 0.4\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  110 / 200\n",
      "Batch:\t0 /4000\t:  0.1612412929534912 \t0.1641225814819336 \t0.20166397094726562\n",
      "Batch:\t1 /4000\t:  0.1645188331604004 \t0.33071422576904297 \t0.0\n",
      "Batch:\t2 /4000\t:  0.16980910301208496 \t0.5029194355010986 \t0.11140632629394531\n",
      "Batch:\t3 /4000\t:  0.16289305686950684 \t0.6677367687225342 \t0.0\n",
      "Batch:\t4 /4000\t:  0.1665797233581543 \t0.8359310626983643 \t0.0\n",
      "Batch:\t5 /4000\t:  0.16158151626586914 \t0.9988367557525635 \t1.724012851715088\n",
      "Batch:\t6 /4000\t:  0.1583254337310791 \t1.1586592197418213 \t0.0\n",
      "Batch:\t7 /4000\t:  0.1700139045715332 \t1.3303263187408447 \t0.22899913787841797\n",
      "Batch:\t8 /4000\t:  0.16939115524291992 \t1.5009820461273193 \t1.894613265991211\n",
      "Batch:\t9 /4000\t:  0.1643383502960205 \t1.6669416427612305 \t0.16136837005615234\n",
      "Batch:\t10 /4000\t:  0.16219377517700195 \t1.8307342529296875 \t0.0\n",
      "Batch:\t11 /4000\t:  0.16706418991088867 \t1.9991531372070312 \t0.31418800354003906\n",
      "Batch:\t12 /4000\t:  0.16844892501831055 \t2.1715526580810547 \t0.11568641662597656\n",
      "Batch:\t13 /4000\t:  0.16674160957336426 \t2.3398070335388184 \t3.0467653274536133\n",
      "Batch:\t14 /4000\t:  0.16932225227355957 \t2.5105934143066406 \t0.1099386215209961\n",
      "Batch:\t15 /4000\t:  0.16075348854064941 \t2.672956705093384 \t0.569911003112793\n",
      "Batch:\t16 /4000\t:  0.1612076759338379 \t2.8356151580810547 \t1.6861286163330078\n",
      "Batch:\t17 /4000\t:  0.1034083366394043 \t2.940443754196167 \t0.08119964599609375\n",
      "Time: 4.9382970333099365\t Loss: 10.2458815574646\t Valdacc: 0.42\t Testacc: 0.445\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  111 / 200\n",
      "Batch:\t0 /4000\t:  0.16376996040344238 \t0.1647634506225586 \t0.0\n",
      "Batch:\t1 /4000\t:  0.1608409881591797 \t0.32744455337524414 \t0.9678525924682617\n",
      "Batch:\t2 /4000\t:  0.16113662719726562 \t0.4904203414916992 \t1.0182390213012695\n",
      "Batch:\t3 /4000\t:  0.20360827445983887 \t0.6953897476196289 \t0.36112213134765625\n",
      "Batch:\t4 /4000\t:  0.16813039779663086 \t0.8649947643280029 \t2.2894086837768555\n",
      "Batch:\t5 /4000\t:  0.16731643676757812 \t1.033909559249878 \t0.7494449615478516\n",
      "Batch:\t6 /4000\t:  0.16757416725158691 \t1.2029163837432861 \t2.7918834686279297\n",
      "Batch:\t7 /4000\t:  0.2041018009185791 \t1.408708095550537 \t0.1818389892578125\n",
      "Batch:\t8 /4000\t:  0.16019225120544434 \t1.570328712463379 \t1.0122032165527344\n",
      "Batch:\t9 /4000\t:  0.1621091365814209 \t1.733872413635254 \t0.0\n",
      "Batch:\t10 /4000\t:  0.1616804599761963 \t1.897207498550415 \t0.0\n",
      "Batch:\t11 /4000\t:  0.16416215896606445 \t2.0628137588500977 \t2.2296438217163086\n",
      "Batch:\t12 /4000\t:  0.18125128746032715 \t2.246027946472168 \t0.4445376396179199\n",
      "Batch:\t13 /4000\t:  0.1629188060760498 \t2.410438060760498 \t0.0\n",
      "Batch:\t14 /4000\t:  0.17926955223083496 \t2.5913634300231934 \t0.0\n",
      "Batch:\t15 /4000\t:  0.15934348106384277 \t2.7522695064544678 \t0.0\n",
      "Batch:\t16 /4000\t:  0.15904641151428223 \t2.912609338760376 \t0.0\n",
      "Batch:\t17 /4000\t:  0.10185098648071289 \t3.0156567096710205 \t0.3429737091064453\n",
      "Time: 4.996848106384277\t Loss: 12.389148235321045\t Valdacc: 0.38\t Testacc: 0.42\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  112 / 200\n",
      "Batch:\t0 /4000\t:  0.20425081253051758 \t0.20659661293029785 \t1.542684555053711\n",
      "Batch:\t1 /4000\t:  0.1748669147491455 \t0.3855609893798828 \t0.0\n",
      "Batch:\t2 /4000\t:  0.16261935234069824 \t0.5664429664611816 \t0.49216651916503906\n",
      "Batch:\t3 /4000\t:  0.16185784339904785 \t0.7298693656921387 \t0.013684272766113281\n",
      "Batch:\t4 /4000\t:  0.1782979965209961 \t0.9099133014678955 \t0.6506328582763672\n",
      "Batch:\t5 /4000\t:  0.16378998756408691 \t1.075089931488037 \t1.1819374561309814\n",
      "Batch:\t6 /4000\t:  0.20312857627868652 \t1.2802050113677979 \t0.0\n",
      "Batch:\t7 /4000\t:  0.18697738647460938 \t1.4692118167877197 \t0.02381134033203125\n",
      "Batch:\t8 /4000\t:  0.1657850742340088 \t1.6363904476165771 \t2.0619688034057617\n",
      "Batch:\t9 /4000\t:  0.22635674476623535 \t1.8651962280273438 \t1.2380714416503906\n",
      "Batch:\t10 /4000\t:  0.16659235954284668 \t2.033174991607666 \t0.17226028442382812\n",
      "Batch:\t11 /4000\t:  0.16579222679138184 \t2.200798749923706 \t1.6238117218017578\n",
      "Batch:\t12 /4000\t:  0.1745162010192871 \t2.377046585083008 \t2.6785049438476562\n",
      "Batch:\t13 /4000\t:  0.16378116607666016 \t2.5427591800689697 \t3.522892951965332\n",
      "Batch:\t14 /4000\t:  0.1637423038482666 \t2.7082102298736572 \t0.7105932235717773\n",
      "Batch:\t15 /4000\t:  0.18088436126708984 \t2.8901894092559814 \t0.0\n",
      "Batch:\t16 /4000\t:  0.1615159511566162 \t3.0533392429351807 \t1.0643959045410156\n",
      "Batch:\t17 /4000\t:  0.10491633415222168 \t3.1601545810699463 \t0.6794252395629883\n",
      "Time: 5.04477334022522\t Loss: 17.65684151649475\t Valdacc: 0.43\t Testacc: 0.415\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  113 / 200\n",
      "Batch:\t0 /4000\t:  0.1604902744293213 \t0.162675142288208 \t2.903806686401367\n",
      "Batch:\t1 /4000\t:  0.1634361743927002 \t0.32750678062438965 \t0.0\n",
      "Batch:\t2 /4000\t:  0.18814802169799805 \t0.5175092220306396 \t1.3350324630737305\n",
      "Batch:\t3 /4000\t:  0.1824488639831543 \t0.7018933296203613 \t0.31325340270996094\n",
      "Batch:\t4 /4000\t:  0.16440677642822266 \t0.8677108287811279 \t0.0\n",
      "Batch:\t5 /4000\t:  0.17816638946533203 \t1.0476067066192627 \t0.0\n",
      "Batch:\t6 /4000\t:  0.1784989833831787 \t1.2284619808197021 \t1.1569633483886719\n",
      "Batch:\t7 /4000\t:  0.16114401817321777 \t1.3909971714019775 \t3.50887393951416\n",
      "Batch:\t8 /4000\t:  0.1643209457397461 \t1.5568604469299316 \t0.31978607177734375\n",
      "Batch:\t9 /4000\t:  0.16147208213806152 \t1.719989538192749 \t0.0\n",
      "Batch:\t10 /4000\t:  0.20328259468078613 \t1.9246904850006104 \t0.2404003143310547\n",
      "Batch:\t11 /4000\t:  0.16508793830871582 \t2.0911219120025635 \t0.0\n",
      "Batch:\t12 /4000\t:  0.16094255447387695 \t2.2534217834472656 \t5.2723798751831055\n",
      "Batch:\t13 /4000\t:  0.16113829612731934 \t2.4162490367889404 \t3.6474075317382812\n",
      "Batch:\t14 /4000\t:  0.16366076469421387 \t2.5813097953796387 \t3.540454864501953\n",
      "Batch:\t15 /4000\t:  0.1661841869354248 \t2.7488937377929688 \t1.3013219833374023\n",
      "Batch:\t16 /4000\t:  0.16571545600891113 \t2.9164187908172607 \t1.2808427810668945\n",
      "Batch:\t17 /4000\t:  0.09999704360961914 \t3.0176568031311035 \t0.0\n",
      "Time: 4.884143829345703\t Loss: 24.820523262023926\t Valdacc: 0.43\t Testacc: 0.41\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  114 / 200\n",
      "Batch:\t0 /4000\t:  0.1640312671661377 \t0.16552352905273438 \t3.500303268432617\n",
      "Batch:\t1 /4000\t:  0.16276192665100098 \t0.3303205966949463 \t1.155423879623413\n",
      "Batch:\t2 /4000\t:  0.18263745307922363 \t0.5147514343261719 \t0.25522708892822266\n",
      "Batch:\t3 /4000\t:  0.1604468822479248 \t0.676572322845459 \t0.424285888671875\n",
      "Batch:\t4 /4000\t:  0.16564226150512695 \t0.8438210487365723 \t0.4696359634399414\n",
      "Batch:\t5 /4000\t:  0.1755211353302002 \t1.0210633277893066 \t0.0\n",
      "Batch:\t6 /4000\t:  0.16463947296142578 \t1.187178611755371 \t0.0\n",
      "Batch:\t7 /4000\t:  0.16218876838684082 \t1.3511033058166504 \t0.0\n",
      "Batch:\t8 /4000\t:  0.18066787719726562 \t1.5334904193878174 \t0.0742340087890625\n",
      "Batch:\t9 /4000\t:  0.16831278800964355 \t1.7033147811889648 \t1.8332128524780273\n",
      "Batch:\t10 /4000\t:  0.18625903129577637 \t1.891435146331787 \t1.457657814025879\n",
      "Batch:\t11 /4000\t:  0.16166973114013672 \t2.054414987564087 \t0.0\n",
      "Batch:\t12 /4000\t:  0.16190648078918457 \t2.218092441558838 \t0.0\n",
      "Batch:\t13 /4000\t:  0.1761338710784912 \t2.3959200382232666 \t0.44969749450683594\n",
      "Batch:\t14 /4000\t:  0.16123247146606445 \t2.5585567951202393 \t0.0\n",
      "Batch:\t15 /4000\t:  0.16552042961120605 \t2.725987672805786 \t1.5469446182250977\n",
      "Batch:\t16 /4000\t:  0.16695737838745117 \t2.894688606262207 \t0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t17 /4000\t:  0.10139083862304688 \t2.9972641468048096 \t0.0\n",
      "Time: 4.8809654712677\t Loss: 11.166622877120972\t Valdacc: 0.41\t Testacc: 0.4\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  115 / 200\n",
      "Batch:\t0 /4000\t:  0.16220331192016602 \t0.163330078125 \t0.7354950904846191\n",
      "Batch:\t1 /4000\t:  0.2031712532043457 \t0.3679938316345215 \t0.8154754638671875\n",
      "Batch:\t2 /4000\t:  0.1958160400390625 \t0.5662879943847656 \t0.0\n",
      "Batch:\t3 /4000\t:  0.1635141372680664 \t0.7311289310455322 \t1.5877809524536133\n",
      "Batch:\t4 /4000\t:  0.16082167625427246 \t0.8937852382659912 \t0.4962010383605957\n",
      "Batch:\t5 /4000\t:  0.17443037033081055 \t1.070131778717041 \t0.09721088409423828\n",
      "Batch:\t6 /4000\t:  0.20073604583740234 \t1.2721583843231201 \t0.0\n",
      "Batch:\t7 /4000\t:  0.16388463973999023 \t1.4379560947418213 \t0.0\n",
      "Batch:\t8 /4000\t:  0.1791553497314453 \t1.6188464164733887 \t2.7735157012939453\n",
      "Batch:\t9 /4000\t:  0.16294288635253906 \t1.7831101417541504 \t0.0\n",
      "Batch:\t10 /4000\t:  0.17343997955322266 \t1.9583194255828857 \t0.0\n",
      "Batch:\t11 /4000\t:  0.18047356605529785 \t2.14139461517334 \t1.9538216590881348\n",
      "Batch:\t12 /4000\t:  0.16366839408874512 \t2.3066861629486084 \t0.12806320190429688\n",
      "Batch:\t13 /4000\t:  0.20416569709777832 \t2.512711524963379 \t0.6346426010131836\n",
      "Batch:\t14 /4000\t:  0.1624279022216797 \t2.6768248081207275 \t0.0\n",
      "Batch:\t15 /4000\t:  0.16157221794128418 \t2.8399832248687744 \t0.0\n",
      "Batch:\t16 /4000\t:  0.1676802635192871 \t3.009364366531372 \t1.2807283401489258\n",
      "Batch:\t17 /4000\t:  0.10119891166687012 \t3.1119186878204346 \t0.0\n",
      "Time: 5.064386367797852\t Loss: 10.50293493270874\t Valdacc: 0.41\t Testacc: 0.385\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  116 / 200\n",
      "Batch:\t0 /4000\t:  0.16350531578063965 \t0.1653907299041748 \t0.17254161834716797\n",
      "Batch:\t1 /4000\t:  0.16582179069519043 \t0.3326880931854248 \t3.1542348861694336\n",
      "Batch:\t2 /4000\t:  0.16085362434387207 \t0.49501585960388184 \t0.8744974136352539\n",
      "Batch:\t3 /4000\t:  0.16602683067321777 \t0.6636083126068115 \t0.0\n",
      "Batch:\t4 /4000\t:  0.15784740447998047 \t0.8249804973602295 \t0.0\n",
      "Batch:\t5 /4000\t:  0.1700456142425537 \t0.996241569519043 \t0.0\n",
      "Batch:\t6 /4000\t:  0.20128464698791504 \t1.2173855304718018 \t0.0\n",
      "Batch:\t7 /4000\t:  0.1632828712463379 \t1.3818910121917725 \t0.24823284149169922\n",
      "Batch:\t8 /4000\t:  0.17010116577148438 \t1.5536830425262451 \t4.2561187744140625\n",
      "Batch:\t9 /4000\t:  0.15816354751586914 \t1.7135326862335205 \t0.0\n",
      "Batch:\t10 /4000\t:  0.16332197189331055 \t1.878415822982788 \t0.48147010803222656\n",
      "Batch:\t11 /4000\t:  0.16080069541931152 \t2.0404348373413086 \t0.0\n",
      "Batch:\t12 /4000\t:  0.1664137840270996 \t2.208404541015625 \t0.0\n",
      "Batch:\t13 /4000\t:  0.16584038734436035 \t2.375488758087158 \t1.8439273834228516\n",
      "Batch:\t14 /4000\t:  0.17447376251220703 \t2.5516059398651123 \t0.0\n",
      "Batch:\t15 /4000\t:  0.20388340950012207 \t2.756977081298828 \t2.3645248413085938\n",
      "Batch:\t16 /4000\t:  0.19342875480651855 \t2.951738119125366 \t2.186716079711914\n",
      "Batch:\t17 /4000\t:  0.11329913139343262 \t3.0661661624908447 \t0.31403350830078125\n",
      "Time: 4.95121693611145\t Loss: 15.896297454833984\t Valdacc: 0.41\t Testacc: 0.375\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  117 / 200\n",
      "Batch:\t0 /4000\t:  0.16085076332092285 \t0.16332578659057617 \t0.0\n",
      "Batch:\t1 /4000\t:  0.16042351722717285 \t0.3252549171447754 \t1.4097843170166016\n",
      "Batch:\t2 /4000\t:  0.16298937797546387 \t0.48961782455444336 \t0.9739513397216797\n",
      "Batch:\t3 /4000\t:  0.16530561447143555 \t0.6561405658721924 \t3.159491539001465\n",
      "Batch:\t4 /4000\t:  0.16498255729675293 \t0.8226318359375 \t0.0\n",
      "Batch:\t5 /4000\t:  0.16379094123840332 \t0.9875528812408447 \t2.127058982849121\n",
      "Batch:\t6 /4000\t:  0.16710424423217773 \t1.1559560298919678 \t0.0\n",
      "Batch:\t7 /4000\t:  0.166473388671875 \t1.3237438201904297 \t0.0\n",
      "Batch:\t8 /4000\t:  0.16202545166015625 \t1.4874615669250488 \t0.8668270111083984\n",
      "Batch:\t9 /4000\t:  0.16220355033874512 \t1.6510303020477295 \t0.5119795799255371\n",
      "Batch:\t10 /4000\t:  0.20754408836364746 \t1.860245943069458 \t0.0\n",
      "Batch:\t11 /4000\t:  0.16427922248840332 \t2.025878429412842 \t1.5256233215332031\n",
      "Batch:\t12 /4000\t:  0.163956880569458 \t2.1912717819213867 \t1.7489538192749023\n",
      "Batch:\t13 /4000\t:  0.17114663124084473 \t2.363860845565796 \t1.7726795673370361\n",
      "Batch:\t14 /4000\t:  0.16095900535583496 \t2.5258290767669678 \t0.08609580993652344\n",
      "Batch:\t15 /4000\t:  0.15823888778686523 \t2.6854970455169678 \t0.0\n",
      "Batch:\t16 /4000\t:  0.16334986686706543 \t2.8504679203033447 \t1.2199296951293945\n",
      "Batch:\t17 /4000\t:  0.1180574893951416 \t2.969691514968872 \t2.6104698181152344\n",
      "Time: 4.864079475402832\t Loss: 18.012844800949097\t Valdacc: 0.38\t Testacc: 0.395\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  118 / 200\n",
      "Batch:\t0 /4000\t:  0.16054368019104004 \t0.16239166259765625 \t0.44478416442871094\n",
      "Batch:\t1 /4000\t:  0.15865063667297363 \t0.32264232635498047 \t0.0\n",
      "Batch:\t2 /4000\t:  0.1640472412109375 \t0.48802661895751953 \t0.0\n",
      "Batch:\t3 /4000\t:  0.16195034980773926 \t0.6512823104858398 \t0.13888263702392578\n",
      "Batch:\t4 /4000\t:  0.1807422637939453 \t0.8335058689117432 \t0.3573026657104492\n",
      "Batch:\t5 /4000\t:  0.1690680980682373 \t1.003955602645874 \t1.76251220703125\n",
      "Batch:\t6 /4000\t:  0.16577720642089844 \t1.1706781387329102 \t2.0402822494506836\n",
      "Batch:\t7 /4000\t:  0.17341399192810059 \t1.3456602096557617 \t0.0\n",
      "Batch:\t8 /4000\t:  0.16087651252746582 \t1.5080313682556152 \t0.0\n",
      "Batch:\t9 /4000\t:  0.16506218910217285 \t1.6747961044311523 \t1.7983808517456055\n",
      "Batch:\t10 /4000\t:  0.1701653003692627 \t1.845937967300415 \t0.0\n",
      "Batch:\t11 /4000\t:  0.16078400611877441 \t2.008174180984497 \t2.0200395584106445\n",
      "Batch:\t12 /4000\t:  0.17747163772583008 \t2.1873257160186768 \t0.0\n",
      "Batch:\t13 /4000\t:  0.1704087257385254 \t2.3592817783355713 \t0.0\n",
      "Batch:\t14 /4000\t:  0.16031885147094727 \t2.5209898948669434 \t2.111471652984619\n",
      "Batch:\t15 /4000\t:  0.1586005687713623 \t2.6807544231414795 \t0.0\n",
      "Batch:\t16 /4000\t:  0.17658066749572754 \t2.859435558319092 \t0.0\n",
      "Batch:\t17 /4000\t:  0.10224390029907227 \t2.9629428386688232 \t0.5708103179931641\n",
      "Time: 4.8712663650512695\t Loss: 11.244466304779053\t Valdacc: 0.38\t Testacc: 0.405\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  119 / 200\n",
      "Batch:\t0 /4000\t:  0.16051864624023438 \t0.16273713111877441 \t0.0\n",
      "Batch:\t1 /4000\t:  0.16230177879333496 \t0.3266899585723877 \t0.42351245880126953\n",
      "Batch:\t2 /4000\t:  0.15903568267822266 \t0.487581729888916 \t0.0\n",
      "Batch:\t3 /4000\t:  0.16512465476989746 \t0.6542491912841797 \t1.4653196334838867\n",
      "Batch:\t4 /4000\t:  0.17409586906433105 \t0.8302206993103027 \t2.0660510063171387\n",
      "Batch:\t5 /4000\t:  0.17014312744140625 \t1.0021107196807861 \t0.4031648635864258\n",
      "Batch:\t6 /4000\t:  0.1596682071685791 \t1.1630384922027588 \t0.0\n",
      "Batch:\t7 /4000\t:  0.17110133171081543 \t1.3359298706054688 \t0.2929048538208008\n",
      "Batch:\t8 /4000\t:  0.1592409610748291 \t1.496586799621582 \t0.0\n",
      "Batch:\t9 /4000\t:  0.16542625427246094 \t1.6638884544372559 \t0.5443229675292969\n",
      "Batch:\t10 /4000\t:  0.16522908210754395 \t1.8306326866149902 \t0.0\n",
      "Batch:\t11 /4000\t:  0.16146612167358398 \t1.9935426712036133 \t0.6342735290527344\n",
      "Batch:\t12 /4000\t:  0.18529081344604492 \t2.180541753768921 \t0.4369373321533203\n",
      "Batch:\t13 /4000\t:  0.16742777824401855 \t2.349579095840454 \t1.0450973510742188\n",
      "Batch:\t14 /4000\t:  0.18446564674377441 \t2.535600423812866 \t0.7106709480285645\n",
      "Batch:\t15 /4000\t:  0.1627662181854248 \t2.700070381164551 \t0.0\n",
      "Batch:\t16 /4000\t:  0.16169476509094238 \t2.8637802600860596 \t0.5492057800292969\n",
      "Batch:\t17 /4000\t:  0.09897232055664062 \t2.9643242359161377 \t0.0\n",
      "Time: 4.910709619522095\t Loss: 8.571460723876953\t Valdacc: 0.37\t Testacc: 0.375\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  120 / 200\n",
      "Batch:\t0 /4000\t:  0.15897679328918457 \t0.16116094589233398 \t0.0\n",
      "Batch:\t1 /4000\t:  0.18111276626586914 \t0.3438870906829834 \t0.3813056945800781\n",
      "Batch:\t2 /4000\t:  0.16431450843811035 \t0.5100398063659668 \t2.72208309173584\n",
      "Batch:\t3 /4000\t:  0.16739654541015625 \t0.6788220405578613 \t0.17238807678222656\n",
      "Batch:\t4 /4000\t:  0.16796422004699707 \t0.8492493629455566 \t0.14117145538330078\n",
      "Batch:\t5 /4000\t:  0.1918797492980957 \t1.0426077842712402 \t0.0\n",
      "Batch:\t6 /4000\t:  0.16400480270385742 \t1.2080965042114258 \t0.5287958383560181\n",
      "Batch:\t7 /4000\t:  0.16230010986328125 \t1.3723626136779785 \t1.172276496887207\n",
      "Batch:\t8 /4000\t:  0.1663377285003662 \t1.5404057502746582 \t0.0\n",
      "Batch:\t9 /4000\t:  0.1636180877685547 \t1.7054109573364258 \t1.084254264831543\n",
      "Batch:\t10 /4000\t:  0.1692490577697754 \t1.8762729167938232 \t0.0\n",
      "Batch:\t11 /4000\t:  0.16858124732971191 \t2.0466115474700928 \t1.0125131607055664\n",
      "Batch:\t12 /4000\t:  0.16561198234558105 \t2.2136952877044678 \t0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t13 /4000\t:  0.20386672019958496 \t2.419414520263672 \t0.0\n",
      "Batch:\t14 /4000\t:  0.18181443214416504 \t2.6033518314361572 \t0.0\n",
      "Batch:\t15 /4000\t:  0.16261029243469238 \t2.767407178878784 \t0.0\n",
      "Batch:\t16 /4000\t:  0.16149616241455078 \t2.930349349975586 \t0.03967094421386719\n",
      "Batch:\t17 /4000\t:  0.10688638687133789 \t3.0387256145477295 \t0.4383125305175781\n",
      "Time: 5.0856428146362305\t Loss: 7.692771553993225\t Valdacc: 0.38\t Testacc: 0.405\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  121 / 200\n",
      "Batch:\t0 /4000\t:  0.17578577995300293 \t0.17770862579345703 \t0.7933645248413086\n",
      "Batch:\t1 /4000\t:  0.1668243408203125 \t0.3462951183319092 \t0.8201169967651367\n",
      "Batch:\t2 /4000\t:  0.16364383697509766 \t0.5115776062011719 \t0.48837852478027344\n",
      "Batch:\t3 /4000\t:  0.16488003730773926 \t0.6778542995452881 \t3.61856746673584\n",
      "Batch:\t4 /4000\t:  0.1822338104248047 \t0.8619821071624756 \t4.192618370056152\n",
      "Batch:\t5 /4000\t:  0.16158819198608398 \t1.0252537727355957 \t0.8209381103515625\n",
      "Batch:\t6 /4000\t:  0.16537022590637207 \t1.1922857761383057 \t0.17876768112182617\n",
      "Batch:\t7 /4000\t:  0.16175556182861328 \t1.356226921081543 \t0.021039962768554688\n",
      "Batch:\t8 /4000\t:  0.18074250221252441 \t1.5384948253631592 \t1.3578457832336426\n",
      "Batch:\t9 /4000\t:  0.1701645851135254 \t1.7100515365600586 \t1.381850242614746\n",
      "Batch:\t10 /4000\t:  0.16991424560546875 \t1.883976936340332 \t0.1552290916442871\n",
      "Batch:\t11 /4000\t:  0.18311500549316406 \t2.069190502166748 \t1.5583305358886719\n",
      "Batch:\t12 /4000\t:  0.16729187965393066 \t2.238314151763916 \t1.8813953399658203\n",
      "Batch:\t13 /4000\t:  0.1597757339477539 \t2.3996551036834717 \t0.0\n",
      "Batch:\t14 /4000\t:  0.16199088096618652 \t2.5633339881896973 \t0.0\n",
      "Batch:\t15 /4000\t:  0.16175103187561035 \t2.7268600463867188 \t0.4931678771972656\n",
      "Batch:\t16 /4000\t:  0.18325185775756836 \t2.9131698608398438 \t1.9583005905151367\n",
      "Batch:\t17 /4000\t:  0.10395431518554688 \t3.0186123847961426 \t0.33750152587890625\n",
      "Time: 4.961085081100464\t Loss: 20.05741262435913\t Valdacc: 0.39\t Testacc: 0.42\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  122 / 200\n",
      "Batch:\t0 /4000\t:  0.16543149948120117 \t0.1673717498779297 \t0.0\n",
      "Batch:\t1 /4000\t:  0.16455721855163574 \t0.3339717388153076 \t0.4306061267852783\n",
      "Batch:\t2 /4000\t:  0.1601550579071045 \t0.49622297286987305 \t0.0\n",
      "Batch:\t3 /4000\t:  0.18976902961730957 \t0.6873912811279297 \t0.0\n",
      "Batch:\t4 /4000\t:  0.1633625030517578 \t0.8523495197296143 \t0.0\n",
      "Batch:\t5 /4000\t:  0.1593492031097412 \t1.0131657123565674 \t0.0\n",
      "Batch:\t6 /4000\t:  0.16791510581970215 \t1.1828944683074951 \t0.8399734497070312\n",
      "Batch:\t7 /4000\t:  0.16182398796081543 \t1.3463718891143799 \t0.0\n",
      "Batch:\t8 /4000\t:  0.16359162330627441 \t1.5113685131072998 \t0.27207183837890625\n",
      "Batch:\t9 /4000\t:  0.16500282287597656 \t1.6778779029846191 \t0.026485443115234375\n",
      "Batch:\t10 /4000\t:  0.15960168838500977 \t1.8392443656921387 \t0.0\n",
      "Batch:\t11 /4000\t:  0.16444039344787598 \t2.0047929286956787 \t0.0\n",
      "Batch:\t12 /4000\t:  0.17885637283325195 \t2.1848533153533936 \t0.0\n",
      "Batch:\t13 /4000\t:  0.16373729705810547 \t2.350175380706787 \t0.0\n",
      "Batch:\t14 /4000\t:  0.1610722541809082 \t2.512983798980713 \t0.0\n",
      "Batch:\t15 /4000\t:  0.16200041770935059 \t2.6764414310455322 \t0.0\n",
      "Batch:\t16 /4000\t:  0.16541004180908203 \t2.8436295986175537 \t0.7106056213378906\n",
      "Batch:\t17 /4000\t:  0.11728286743164062 \t2.962132215499878 \t0.0\n",
      "Time: 4.920351266860962\t Loss: 2.279742479324341\t Valdacc: 0.33\t Testacc: 0.425\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  123 / 200\n",
      "Batch:\t0 /4000\t:  0.1646120548248291 \t0.1655268669128418 \t0.0\n",
      "Batch:\t1 /4000\t:  0.19417452812194824 \t0.36127305030822754 \t0.6668796539306641\n",
      "Batch:\t2 /4000\t:  0.16520166397094727 \t0.5281064510345459 \t0.0\n",
      "Batch:\t3 /4000\t:  0.1649336814880371 \t0.6948223114013672 \t0.0\n",
      "Batch:\t4 /4000\t:  0.17252206802368164 \t0.8690061569213867 \t2.596282958984375\n",
      "Batch:\t5 /4000\t:  0.16571736335754395 \t1.0365846157073975 \t2.1968297958374023\n",
      "Batch:\t6 /4000\t:  0.1798563003540039 \t1.218414306640625 \t1.102243423461914\n",
      "Batch:\t7 /4000\t:  0.15973520278930664 \t1.3798885345458984 \t0.0\n",
      "Batch:\t8 /4000\t:  0.16091442108154297 \t1.5430562496185303 \t0.0\n",
      "Batch:\t9 /4000\t:  0.22288823127746582 \t1.7699902057647705 \t0.3203620910644531\n",
      "Batch:\t10 /4000\t:  0.16539692878723145 \t1.9373579025268555 \t3.369870662689209\n",
      "Batch:\t11 /4000\t:  0.18255376815795898 \t2.1212828159332275 \t1.5464563369750977\n",
      "Batch:\t12 /4000\t:  0.16701173782348633 \t2.2897918224334717 \t0.7653017044067383\n",
      "Batch:\t13 /4000\t:  0.1657273769378662 \t2.456972360610962 \t0.0\n",
      "Batch:\t14 /4000\t:  0.17290186882019043 \t2.6316540241241455 \t0.0\n",
      "Batch:\t15 /4000\t:  0.16139435768127441 \t2.794485330581665 \t0.7141351699829102\n",
      "Batch:\t16 /4000\t:  0.1694927215576172 \t2.965472936630249 \t0.4477663040161133\n",
      "Batch:\t17 /4000\t:  0.1021125316619873 \t3.0690698623657227 \t0.0\n",
      "Time: 5.025147914886475\t Loss: 13.726128101348877\t Valdacc: 0.34\t Testacc: 0.425\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  124 / 200\n",
      "Batch:\t0 /4000\t:  0.1616365909576416 \t0.16416692733764648 \t0.9488744735717773\n",
      "Batch:\t1 /4000\t:  0.16079211235046387 \t0.3272271156311035 \t0.0\n",
      "Batch:\t2 /4000\t:  0.16428375244140625 \t0.4930264949798584 \t0.2046833038330078\n",
      "Batch:\t3 /4000\t:  0.16556835174560547 \t0.6600544452667236 \t0.1771411895751953\n",
      "Batch:\t4 /4000\t:  0.1678171157836914 \t0.8302583694458008 \t0.5246257781982422\n",
      "Batch:\t5 /4000\t:  0.17528414726257324 \t1.0081303119659424 \t0.0\n",
      "Batch:\t6 /4000\t:  0.203080415725708 \t1.2126693725585938 \t0.0\n",
      "Batch:\t7 /4000\t:  0.16233158111572266 \t1.3771467208862305 \t1.0596275329589844\n",
      "Batch:\t8 /4000\t:  0.17426061630249023 \t1.552915096282959 \t0.5877408981323242\n",
      "Batch:\t9 /4000\t:  0.16518664360046387 \t1.7196934223175049 \t1.376943588256836\n",
      "Batch:\t10 /4000\t:  0.1664142608642578 \t1.8883910179138184 \t0.9538230895996094\n",
      "Batch:\t11 /4000\t:  0.1771385669708252 \t2.0675220489501953 \t0.0\n",
      "Batch:\t12 /4000\t:  0.15942811965942383 \t2.2287518978118896 \t0.0\n",
      "Batch:\t13 /4000\t:  0.16660451889038086 \t2.396852493286133 \t0.4214359521865845\n",
      "Batch:\t14 /4000\t:  0.16399192810058594 \t2.5622012615203857 \t0.0\n",
      "Batch:\t15 /4000\t:  0.17965936660766602 \t2.743586301803589 \t0.0\n",
      "Batch:\t16 /4000\t:  0.17154526710510254 \t2.9171411991119385 \t0.619133710861206\n",
      "Batch:\t17 /4000\t:  0.09984683990478516 \t3.0189359188079834 \t0.0\n",
      "Time: 4.96538782119751\t Loss: 6.874029517173767\t Valdacc: 0.4\t Testacc: 0.4\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  125 / 200\n",
      "Batch:\t0 /4000\t:  0.16196846961975098 \t0.16448163986206055 \t0.0\n",
      "Batch:\t1 /4000\t:  0.1622164249420166 \t0.3284013271331787 \t1.549530029296875\n",
      "Batch:\t2 /4000\t:  0.16278409957885742 \t0.4928767681121826 \t0.0\n",
      "Batch:\t3 /4000\t:  0.17949151992797852 \t0.6738002300262451 \t0.0\n",
      "Batch:\t4 /4000\t:  0.16564679145812988 \t0.8414709568023682 \t0.7040977478027344\n",
      "Batch:\t5 /4000\t:  0.1590867042541504 \t1.002356767654419 \t0.0\n",
      "Batch:\t6 /4000\t:  0.16856169700622559 \t1.1724083423614502 \t3.254366874694824\n",
      "Batch:\t7 /4000\t:  0.1613316535949707 \t1.335794448852539 \t0.0\n",
      "Batch:\t8 /4000\t:  0.17885065078735352 \t1.5163555145263672 \t1.950338363647461\n",
      "Batch:\t9 /4000\t:  0.16748523712158203 \t1.6856088638305664 \t0.0\n",
      "Batch:\t10 /4000\t:  0.16141033172607422 \t1.8485217094421387 \t0.5069046020507812\n",
      "Batch:\t11 /4000\t:  0.16076421737670898 \t2.0109920501708984 \t0.0\n",
      "Batch:\t12 /4000\t:  0.1594688892364502 \t2.1721103191375732 \t0.0\n",
      "Batch:\t13 /4000\t:  0.16022062301635742 \t2.3349311351776123 \t0.0\n",
      "Batch:\t14 /4000\t:  0.20550012588500977 \t2.5420596599578857 \t0.2066059112548828\n",
      "Batch:\t15 /4000\t:  0.20524382591247559 \t2.748908519744873 \t8.010472297668457\n",
      "Batch:\t16 /4000\t:  0.16509008407592773 \t2.9169089794158936 \t1.9044380187988281\n",
      "Batch:\t17 /4000\t:  0.10430359840393066 \t3.0228629112243652 \t0.0\n",
      "Time: 4.985137701034546\t Loss: 18.086753845214844\t Valdacc: 0.4\t Testacc: 0.415\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  126 / 200\n",
      "Batch:\t0 /4000\t:  0.16461563110351562 \t0.1666393280029297 \t0.5667152404785156\n",
      "Batch:\t1 /4000\t:  0.16249608993530273 \t0.3312196731567383 \t0.7087101936340332\n",
      "Batch:\t2 /4000\t:  0.15978026390075684 \t0.4927852153778076 \t0.0\n",
      "Batch:\t3 /4000\t:  0.1649761199951172 \t0.659529447555542 \t0.48607730865478516\n",
      "Batch:\t4 /4000\t:  0.1655876636505127 \t0.8267240524291992 \t0.35358524322509766\n",
      "Batch:\t5 /4000\t:  0.16199040412902832 \t0.9919085502624512 \t4.064798355102539\n",
      "Batch:\t6 /4000\t:  0.16704702377319336 \t1.1607656478881836 \t0.6015892028808594\n",
      "Batch:\t7 /4000\t:  0.16514849662780762 \t1.3278203010559082 \t0.9800186157226562\n",
      "Batch:\t8 /4000\t:  0.1608729362487793 \t1.4906442165374756 \t0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t9 /4000\t:  0.1656322479248047 \t1.6577682495117188 \t0.0\n",
      "Batch:\t10 /4000\t:  0.16553926467895508 \t1.8270161151885986 \t0.0\n",
      "Batch:\t11 /4000\t:  0.16200804710388184 \t1.9905657768249512 \t0.06341361999511719\n",
      "Batch:\t12 /4000\t:  0.15926456451416016 \t2.151524782180786 \t0.0\n",
      "Batch:\t13 /4000\t:  0.16621947288513184 \t2.3196165561676025 \t0.26616859436035156\n",
      "Batch:\t14 /4000\t:  0.20269274711608887 \t2.523805856704712 \t0.0\n",
      "Batch:\t15 /4000\t:  0.16477727890014648 \t2.6898884773254395 \t0.9541492462158203\n",
      "Batch:\t16 /4000\t:  0.16966605186462402 \t2.8611655235290527 \t1.1051902770996094\n",
      "Batch:\t17 /4000\t:  0.10228991508483887 \t2.964832305908203 \t0.0\n",
      "Time: 4.958075046539307\t Loss: 10.150415897369385\t Valdacc: 0.41\t Testacc: 0.405\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  127 / 200\n",
      "Batch:\t0 /4000\t:  0.15999674797058105 \t0.16196203231811523 \t0.0\n",
      "Batch:\t1 /4000\t:  0.15911388397216797 \t0.32260990142822266 \t0.0\n",
      "Batch:\t2 /4000\t:  0.18025684356689453 \t0.5048246383666992 \t0.0\n",
      "Batch:\t3 /4000\t:  0.2060089111328125 \t0.7122476100921631 \t0.6847982406616211\n",
      "Batch:\t4 /4000\t:  0.1665503978729248 \t0.8803973197937012 \t0.585963249206543\n",
      "Batch:\t5 /4000\t:  0.21932387351989746 \t1.1014704704284668 \t0.7302875518798828\n",
      "Batch:\t6 /4000\t:  0.16522479057312012 \t1.2685279846191406 \t0.0\n",
      "Batch:\t7 /4000\t:  0.1617908477783203 \t1.4318222999572754 \t0.15944957733154297\n",
      "Batch:\t8 /4000\t:  0.17317962646484375 \t1.6072719097137451 \t0.09093379974365234\n",
      "Batch:\t9 /4000\t:  0.17669939994812012 \t1.7866168022155762 \t2.5106048583984375\n",
      "Batch:\t10 /4000\t:  0.16218829154968262 \t1.9507224559783936 \t1.1014842987060547\n",
      "Batch:\t11 /4000\t:  0.16543865203857422 \t2.117842435836792 \t0.2592741847038269\n",
      "Batch:\t12 /4000\t:  0.16001033782958984 \t2.279484510421753 \t0.0\n",
      "Batch:\t13 /4000\t:  0.17187118530273438 \t2.453345537185669 \t0.0\n",
      "Batch:\t14 /4000\t:  0.1635596752166748 \t2.6184957027435303 \t0.0\n",
      "Batch:\t15 /4000\t:  0.16541600227355957 \t2.7853927612304688 \t1.6968650817871094\n",
      "Batch:\t16 /4000\t:  0.16382932662963867 \t2.9511396884918213 \t0.0668487548828125\n",
      "Batch:\t17 /4000\t:  0.11148262023925781 \t3.0642356872558594 \t0.11458301544189453\n",
      "Time: 5.122655391693115\t Loss: 8.001092612743378\t Valdacc: 0.41\t Testacc: 0.415\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  128 / 200\n",
      "Batch:\t0 /4000\t:  0.1681504249572754 \t0.16912031173706055 \t1.4803094863891602\n",
      "Batch:\t1 /4000\t:  0.16260814666748047 \t0.3337137699127197 \t1.0338354110717773\n",
      "Batch:\t2 /4000\t:  0.16423988342285156 \t0.4998326301574707 \t0.0\n",
      "Batch:\t3 /4000\t:  0.16673731803894043 \t0.6679770946502686 \t3.656585693359375\n",
      "Batch:\t4 /4000\t:  0.18155789375305176 \t0.8511734008789062 \t2.163454055786133\n",
      "Batch:\t5 /4000\t:  0.20240020751953125 \t1.0549259185791016 \t0.0\n",
      "Batch:\t6 /4000\t:  0.16605091094970703 \t1.2226030826568604 \t0.0\n",
      "Batch:\t7 /4000\t:  0.16793584823608398 \t1.392296552658081 \t2.451101303100586\n",
      "Batch:\t8 /4000\t:  0.1853494644165039 \t1.579592227935791 \t0.0\n",
      "Batch:\t9 /4000\t:  0.16116857528686523 \t1.7423439025878906 \t0.13591957092285156\n",
      "Batch:\t10 /4000\t:  0.15967392921447754 \t1.903864860534668 \t0.0\n",
      "Batch:\t11 /4000\t:  0.17126846313476562 \t2.0768659114837646 \t0.3862190246582031\n",
      "Batch:\t12 /4000\t:  0.20945978164672852 \t2.2879207134246826 \t2.2570133209228516\n",
      "Batch:\t13 /4000\t:  0.17314553260803223 \t2.4630167484283447 \t0.2552003860473633\n",
      "Batch:\t14 /4000\t:  0.16466450691223145 \t2.6293153762817383 \t0.0\n",
      "Batch:\t15 /4000\t:  0.1613755226135254 \t2.7927865982055664 \t0.0\n",
      "Batch:\t16 /4000\t:  0.16437935829162598 \t2.9586076736450195 \t2.422504425048828\n",
      "Batch:\t17 /4000\t:  0.10067009925842285 \t3.0608699321746826 \t0.0\n",
      "Time: 5.051358938217163\t Loss: 16.24214267730713\t Valdacc: 0.43\t Testacc: 0.415\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  129 / 200\n",
      "Batch:\t0 /4000\t:  0.16872239112854004 \t0.17150568962097168 \t0.5492048263549805\n",
      "Batch:\t1 /4000\t:  0.16885685920715332 \t0.34200263023376465 \t0.6403026580810547\n",
      "Batch:\t2 /4000\t:  0.15862202644348145 \t0.5021371841430664 \t0.0\n",
      "Batch:\t3 /4000\t:  0.17087912559509277 \t0.6746156215667725 \t0.24909019470214844\n",
      "Batch:\t4 /4000\t:  0.15933680534362793 \t0.8357155323028564 \t0.0\n",
      "Batch:\t5 /4000\t:  0.16458964347839355 \t1.002096176147461 \t0.08460235595703125\n",
      "Batch:\t6 /4000\t:  0.20221662521362305 \t1.2059965133666992 \t0.0\n",
      "Batch:\t7 /4000\t:  0.16790175437927246 \t1.3751370906829834 \t0.3185462951660156\n",
      "Batch:\t8 /4000\t:  0.16322803497314453 \t1.5404810905456543 \t2.5375442504882812\n",
      "Batch:\t9 /4000\t:  0.16188693046569824 \t1.7039604187011719 \t0.0\n",
      "Batch:\t10 /4000\t:  0.16733646392822266 \t1.8729825019836426 \t0.0291290283203125\n",
      "Batch:\t11 /4000\t:  0.16346502304077148 \t2.037932872772217 \t0.0005044937133789062\n",
      "Batch:\t12 /4000\t:  0.16783404350280762 \t2.2075488567352295 \t1.9648151397705078\n",
      "Batch:\t13 /4000\t:  0.187530517578125 \t2.397334337234497 \t0.0\n",
      "Batch:\t14 /4000\t:  0.15904879570007324 \t2.557830810546875 \t0.0\n",
      "Batch:\t15 /4000\t:  0.1660480499267578 \t2.7258973121643066 \t3.929088592529297\n",
      "Batch:\t16 /4000\t:  0.1618194580078125 \t2.8888421058654785 \t0.0\n",
      "Batch:\t17 /4000\t:  0.10144424438476562 \t2.991762161254883 \t0.0\n",
      "Time: 5.003532886505127\t Loss: 10.302827835083008\t Valdacc: 0.42\t Testacc: 0.41\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  130 / 200\n",
      "Batch:\t0 /4000\t:  0.16275787353515625 \t0.16561579704284668 \t0.0\n",
      "Batch:\t1 /4000\t:  0.161698579788208 \t0.32869815826416016 \t0.0\n",
      "Batch:\t2 /4000\t:  0.16228985786437988 \t0.49247097969055176 \t0.0\n",
      "Batch:\t3 /4000\t:  0.17187738418579102 \t0.6659684181213379 \t1.3119935989379883\n",
      "Batch:\t4 /4000\t:  0.16066265106201172 \t0.8283731937408447 \t0.0\n",
      "Batch:\t5 /4000\t:  0.15922188758850098 \t0.9893524646759033 \t0.0\n",
      "Batch:\t6 /4000\t:  0.16156315803527832 \t1.1523261070251465 \t0.6208877563476562\n",
      "Batch:\t7 /4000\t:  0.1673135757446289 \t1.3210453987121582 \t0.6255626678466797\n",
      "Batch:\t8 /4000\t:  0.16204452514648438 \t1.4845540523529053 \t0.0\n",
      "Batch:\t9 /4000\t:  0.16149282455444336 \t1.6478354930877686 \t0.0\n",
      "Batch:\t10 /4000\t:  0.16215014457702637 \t1.8115818500518799 \t0.4228496551513672\n",
      "Batch:\t11 /4000\t:  0.16568255424499512 \t1.9793212413787842 \t0.37564754486083984\n",
      "Batch:\t12 /4000\t:  0.1721363067626953 \t2.1533761024475098 \t1.0833463668823242\n",
      "Batch:\t13 /4000\t:  0.18182659149169922 \t2.3368585109710693 \t2.5946245193481445\n",
      "Batch:\t14 /4000\t:  0.16240477561950684 \t2.5007472038269043 \t0.0\n",
      "Batch:\t15 /4000\t:  0.16876792907714844 \t2.671292304992676 \t0.7171072959899902\n",
      "Batch:\t16 /4000\t:  0.1644759178161621 \t2.8374295234680176 \t0.6803903579711914\n",
      "Batch:\t17 /4000\t:  0.10310912132263184 \t2.942396879196167 \t0.0\n",
      "Time: 4.940129995346069\t Loss: 8.432409763336182\t Valdacc: 0.4\t Testacc: 0.395\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  131 / 200\n",
      "Batch:\t0 /4000\t:  0.16175508499145508 \t0.16418671607971191 \t0.6056194305419922\n",
      "Batch:\t1 /4000\t:  0.16172218322753906 \t0.32735252380371094 \t0.0\n",
      "Batch:\t2 /4000\t:  0.16224026679992676 \t0.4911537170410156 \t0.0\n",
      "Batch:\t3 /4000\t:  0.16171836853027344 \t0.6542956829071045 \t1.3380708694458008\n",
      "Batch:\t4 /4000\t:  0.19031023979187012 \t0.8466207981109619 \t1.751277208328247\n",
      "Batch:\t5 /4000\t:  0.1827244758605957 \t1.0307955741882324 \t2.4716672897338867\n",
      "Batch:\t6 /4000\t:  0.16190171241760254 \t1.194248914718628 \t1.9593019485473633\n",
      "Batch:\t7 /4000\t:  0.16562175750732422 \t1.3613533973693848 \t0.0\n",
      "Batch:\t8 /4000\t:  0.16342401504516602 \t1.52589750289917 \t0.25444984436035156\n",
      "Batch:\t9 /4000\t:  0.16588902473449707 \t1.6932237148284912 \t0.0\n",
      "Batch:\t10 /4000\t:  0.18060922622680664 \t1.875490665435791 \t0.0\n",
      "Batch:\t11 /4000\t:  0.1613764762878418 \t2.0382587909698486 \t0.36413002014160156\n",
      "Batch:\t12 /4000\t:  0.15916705131530762 \t2.1993141174316406 \t0.0\n",
      "Batch:\t13 /4000\t:  0.1590404510498047 \t2.3597559928894043 \t0.0\n",
      "Batch:\t14 /4000\t:  0.16643095016479492 \t2.527862548828125 \t0.0\n",
      "Batch:\t15 /4000\t:  0.16927218437194824 \t2.6991517543792725 \t0.7008466720581055\n",
      "Batch:\t16 /4000\t:  0.16261959075927734 \t2.863464117050171 \t0.0\n",
      "Batch:\t17 /4000\t:  0.10818791389465332 \t2.9732532501220703 \t0.3285808563232422\n",
      "Time: 4.949605703353882\t Loss: 9.77394413948059\t Valdacc: 0.44\t Testacc: 0.4\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  132 / 200\n",
      "Batch:\t0 /4000\t:  0.18276691436767578 \t0.18436121940612793 \t0.21073055267333984\n",
      "Batch:\t1 /4000\t:  0.16191315650939941 \t0.3485107421875 \t0.0\n",
      "Batch:\t2 /4000\t:  0.15923380851745605 \t0.5095198154449463 \t0.0\n",
      "Batch:\t3 /4000\t:  0.1668848991394043 \t0.6780388355255127 \t2.8691158294677734\n",
      "Batch:\t4 /4000\t:  0.16236281394958496 \t0.8423125743865967 \t0.6788349151611328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t5 /4000\t:  0.16399741172790527 \t1.007965087890625 \t0.16196441650390625\n",
      "Batch:\t6 /4000\t:  0.16587615013122559 \t1.1759474277496338 \t0.08994674682617188\n",
      "Batch:\t7 /4000\t:  0.16853117942810059 \t1.3462772369384766 \t1.3516407012939453\n",
      "Batch:\t8 /4000\t:  0.16128206253051758 \t1.509047031402588 \t0.3660259246826172\n",
      "Batch:\t9 /4000\t:  0.16359305381774902 \t1.67447829246521 \t0.7400331497192383\n",
      "Batch:\t10 /4000\t:  0.16802215576171875 \t1.8439736366271973 \t0.0\n",
      "Batch:\t11 /4000\t:  0.16381120681762695 \t2.009214401245117 \t1.729140281677246\n",
      "Batch:\t12 /4000\t:  0.173905611038208 \t2.1850953102111816 \t0.0\n",
      "Batch:\t13 /4000\t:  0.16657567024230957 \t2.3536477088928223 \t0.0\n",
      "Batch:\t14 /4000\t:  0.15880537033081055 \t2.5141422748565674 \t0.0\n",
      "Batch:\t15 /4000\t:  0.1688854694366455 \t2.6844844818115234 \t0.5904703140258789\n",
      "Batch:\t16 /4000\t:  0.16640853881835938 \t2.852407693862915 \t1.5911731719970703\n",
      "Batch:\t17 /4000\t:  0.10137367248535156 \t2.9554898738861084 \t0.0\n",
      "Time: 4.856563568115234\t Loss: 10.37907600402832\t Valdacc: 0.4\t Testacc: 0.425\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  133 / 200\n",
      "Batch:\t0 /4000\t:  0.16953420639038086 \t0.17119812965393066 \t0.14091205596923828\n",
      "Batch:\t1 /4000\t:  0.16724610328674316 \t0.3399314880371094 \t0.0\n",
      "Batch:\t2 /4000\t:  0.1898496150970459 \t0.5318930149078369 \t0.40590667724609375\n",
      "Batch:\t3 /4000\t:  0.16190862655639648 \t0.6949396133422852 \t0.11247634887695312\n",
      "Batch:\t4 /4000\t:  0.17318201065063477 \t0.8699023723602295 \t0.11368370056152344\n",
      "Batch:\t5 /4000\t:  0.16969752311706543 \t1.0416691303253174 \t0.0\n",
      "Batch:\t6 /4000\t:  0.1653575897216797 \t1.2086048126220703 \t0.0\n",
      "Batch:\t7 /4000\t:  0.20598411560058594 \t1.416060209274292 \t1.064178466796875\n",
      "Batch:\t8 /4000\t:  0.16550230979919434 \t1.5830543041229248 \t0.0\n",
      "Batch:\t9 /4000\t:  0.16255474090576172 \t1.7473480701446533 \t0.0\n",
      "Batch:\t10 /4000\t:  0.16409587860107422 \t1.9129981994628906 \t1.4797992706298828\n",
      "Batch:\t11 /4000\t:  0.1848456859588623 \t2.0995163917541504 \t0.7506046295166016\n",
      "Batch:\t12 /4000\t:  0.16408514976501465 \t2.265382766723633 \t2.638519287109375\n",
      "Batch:\t13 /4000\t:  0.16377019882202148 \t2.430619478225708 \t0.076812744140625\n",
      "Batch:\t14 /4000\t:  0.1645219326019287 \t2.596616506576538 \t5.597792625427246\n",
      "Batch:\t15 /4000\t:  0.16512274742126465 \t2.763223171234131 \t2.0336380004882812\n",
      "Batch:\t16 /4000\t:  0.1876838207244873 \t2.953227996826172 \t0.17369365692138672\n",
      "Batch:\t17 /4000\t:  0.10017085075378418 \t3.054746389389038 \t0.0\n",
      "Time: 4.98969841003418\t Loss: 14.588017463684082\t Valdacc: 0.39\t Testacc: 0.435\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  134 / 200\n",
      "Batch:\t0 /4000\t:  0.16457772254943848 \t0.16544103622436523 \t0.0\n",
      "Batch:\t1 /4000\t:  0.1619100570678711 \t0.32872748374938965 \t0.855015754699707\n",
      "Batch:\t2 /4000\t:  0.2037215232849121 \t0.5339503288269043 \t0.0\n",
      "Batch:\t3 /4000\t:  0.16487812995910645 \t0.7002346515655518 \t1.7272281646728516\n",
      "Batch:\t4 /4000\t:  0.19008517265319824 \t0.8920257091522217 \t0.0\n",
      "Batch:\t5 /4000\t:  0.16164231300354004 \t1.0551156997680664 \t0.0\n",
      "Batch:\t6 /4000\t:  0.16235041618347168 \t1.2188081741333008 \t1.456009864807129\n",
      "Batch:\t7 /4000\t:  0.17368674278259277 \t1.3942351341247559 \t0.07718658447265625\n",
      "Batch:\t8 /4000\t:  0.16274762153625488 \t1.558734655380249 \t1.5203399658203125\n",
      "Batch:\t9 /4000\t:  0.20540952682495117 \t1.7660236358642578 \t0.0\n",
      "Batch:\t10 /4000\t:  0.16216063499450684 \t1.9295415878295898 \t0.0\n",
      "Batch:\t11 /4000\t:  0.1645219326019287 \t2.0957698822021484 \t3.407625198364258\n",
      "Batch:\t12 /4000\t:  0.17367291450500488 \t2.2715578079223633 \t0.0\n",
      "Batch:\t13 /4000\t:  0.16193199157714844 \t2.43510103225708 \t0.0\n",
      "Batch:\t14 /4000\t:  0.16404056549072266 \t2.600940465927124 \t0.0972433090209961\n",
      "Batch:\t15 /4000\t:  0.1816716194152832 \t2.7845406532287598 \t0.8877372741699219\n",
      "Batch:\t16 /4000\t:  0.15974092483520508 \t2.946240186691284 \t0.0\n",
      "Batch:\t17 /4000\t:  0.09862828254699707 \t3.048213005065918 \t0.0\n",
      "Time: 4.937101125717163\t Loss: 10.028386116027832\t Valdacc: 0.42\t Testacc: 0.43\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  135 / 200\n",
      "Batch:\t0 /4000\t:  0.16348958015441895 \t0.16541647911071777 \t0.38544464111328125\n",
      "Batch:\t1 /4000\t:  0.1609327793121338 \t0.32768893241882324 \t1.7161493301391602\n",
      "Batch:\t2 /4000\t:  0.15988874435424805 \t0.4896247386932373 \t0.0\n",
      "Batch:\t3 /4000\t:  0.17955732345581055 \t0.6705563068389893 \t0.0\n",
      "Batch:\t4 /4000\t:  0.16748976707458496 \t0.8403306007385254 \t0.0\n",
      "Batch:\t5 /4000\t:  0.1648569107055664 \t1.0066840648651123 \t1.2224435806274414\n",
      "Batch:\t6 /4000\t:  0.18718910217285156 \t1.1953976154327393 \t1.211904525756836\n",
      "Batch:\t7 /4000\t:  0.16363930702209473 \t1.363511085510254 \t0.0049114227294921875\n",
      "Batch:\t8 /4000\t:  0.16116642951965332 \t1.526045799255371 \t2.39849853515625\n",
      "Batch:\t9 /4000\t:  0.16440033912658691 \t1.6921412944793701 \t0.0\n",
      "Batch:\t10 /4000\t:  0.18258070945739746 \t1.8761322498321533 \t0.05975151062011719\n",
      "Batch:\t11 /4000\t:  0.16716670989990234 \t2.0445659160614014 \t0.0\n",
      "Batch:\t12 /4000\t:  0.16648483276367188 \t2.212700605392456 \t0.7939658164978027\n",
      "Batch:\t13 /4000\t:  0.163527250289917 \t2.3777432441711426 \t3.098581314086914\n",
      "Batch:\t14 /4000\t:  0.19063758850097656 \t2.5702433586120605 \t0.0\n",
      "Batch:\t15 /4000\t:  0.194488525390625 \t2.7657554149627686 \t0.0\n",
      "Batch:\t16 /4000\t:  0.16337323188781738 \t2.9308032989501953 \t0.7188787460327148\n",
      "Batch:\t17 /4000\t:  0.12390780448913574 \t3.056405544281006 \t0.0\n",
      "Time: 5.01248836517334\t Loss: 11.61052942276001\t Valdacc: 0.41\t Testacc: 0.41\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  136 / 200\n",
      "Batch:\t0 /4000\t:  0.16050958633422852 \t0.16343188285827637 \t4.608333587646484\n",
      "Batch:\t1 /4000\t:  0.16557645797729492 \t0.33054256439208984 \t1.2615537643432617\n",
      "Batch:\t2 /4000\t:  0.1630845069885254 \t0.49492740631103516 \t1.5501594543457031\n",
      "Batch:\t3 /4000\t:  0.16406941413879395 \t0.6604428291320801 \t0.0\n",
      "Batch:\t4 /4000\t:  0.16193103790283203 \t0.8242533206939697 \t0.11140108108520508\n",
      "Batch:\t5 /4000\t:  0.16647958755493164 \t0.9921786785125732 \t0.6917781829833984\n",
      "Batch:\t6 /4000\t:  0.15885519981384277 \t1.1524875164031982 \t0.0\n",
      "Batch:\t7 /4000\t:  0.16481351852416992 \t1.3186326026916504 \t0.0\n",
      "Batch:\t8 /4000\t:  0.16204547882080078 \t1.4823250770568848 \t0.28493499755859375\n",
      "Batch:\t9 /4000\t:  0.1629772186279297 \t1.6467654705047607 \t0.6467685699462891\n",
      "Batch:\t10 /4000\t:  0.16009521484375 \t1.8082220554351807 \t3.0238866806030273\n",
      "Batch:\t11 /4000\t:  0.169663667678833 \t1.9793379306793213 \t0.08259868621826172\n",
      "Batch:\t12 /4000\t:  0.18081879615783691 \t2.1615185737609863 \t1.0638313293457031\n",
      "Batch:\t13 /4000\t:  0.17840099334716797 \t2.3415870666503906 \t0.0\n",
      "Batch:\t14 /4000\t:  0.16484475135803223 \t2.514232635498047 \t0.0\n",
      "Batch:\t15 /4000\t:  0.16375970840454102 \t2.6793553829193115 \t3.2337770462036133\n",
      "Batch:\t16 /4000\t:  0.18937897682189941 \t2.8765387535095215 \t0.0\n",
      "Batch:\t17 /4000\t:  0.10322332382202148 \t2.9808151721954346 \t0.0\n",
      "Time: 4.866589784622192\t Loss: 16.55902338027954\t Valdacc: 0.43\t Testacc: 0.43\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  137 / 200\n",
      "Batch:\t0 /4000\t:  0.1605532169342041 \t0.16269659996032715 \t0.9135761260986328\n",
      "Batch:\t1 /4000\t:  0.16209173202514648 \t0.32626938819885254 \t1.381103515625\n",
      "Batch:\t2 /4000\t:  0.16294193267822266 \t0.4907853603363037 \t0.0\n",
      "Batch:\t3 /4000\t:  0.16613531112670898 \t0.6583473682403564 \t0.689091682434082\n",
      "Batch:\t4 /4000\t:  0.1610395908355713 \t0.8208098411560059 \t1.5941457748413086\n",
      "Batch:\t5 /4000\t:  0.1680593490600586 \t0.9903786182403564 \t0.15766048431396484\n",
      "Batch:\t6 /4000\t:  0.17931389808654785 \t1.171217679977417 \t0.0\n",
      "Batch:\t7 /4000\t:  0.17452645301818848 \t1.3473742008209229 \t0.0\n",
      "Batch:\t8 /4000\t:  0.20168256759643555 \t1.550682783126831 \t0.0\n",
      "Batch:\t9 /4000\t:  0.17723369598388672 \t1.7291843891143799 \t0.9846242666244507\n",
      "Batch:\t10 /4000\t:  0.1640033721923828 \t1.8943991661071777 \t1.324300765991211\n",
      "Batch:\t11 /4000\t:  0.1586313247680664 \t2.054464817047119 \t0.0\n",
      "Batch:\t12 /4000\t:  0.16706466674804688 \t2.223541259765625 \t0.0\n",
      "Batch:\t13 /4000\t:  0.16987204551696777 \t2.3950655460357666 \t0.5818023681640625\n",
      "Batch:\t14 /4000\t:  0.1631171703338623 \t2.5594491958618164 \t0.008191108703613281\n",
      "Batch:\t15 /4000\t:  0.15771937370300293 \t2.7187933921813965 \t0.0\n",
      "Batch:\t16 /4000\t:  0.16173291206359863 \t2.881951332092285 \t0.0\n",
      "Batch:\t17 /4000\t:  0.10013508796691895 \t2.9839189052581787 \t0.10506057739257812\n",
      "Time: 4.838201284408569\t Loss: 7.739556670188904\t Valdacc: 0.45\t Testacc: 0.43\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  138 / 200\n",
      "Batch:\t0 /4000\t:  0.18105363845825195 \t0.18306231498718262 \t0.5697574615478516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t1 /4000\t:  0.16160893440246582 \t0.3466048240661621 \t0.0\n",
      "Batch:\t2 /4000\t:  0.16335129737854004 \t0.5110580921173096 \t0.4635767936706543\n",
      "Batch:\t3 /4000\t:  0.16309666633605957 \t0.6757466793060303 \t0.0\n",
      "Batch:\t4 /4000\t:  0.16660857200622559 \t0.8440382480621338 \t0.3063631057739258\n",
      "Batch:\t5 /4000\t:  0.17994213104248047 \t1.0252251625061035 \t0.6379432678222656\n",
      "Batch:\t6 /4000\t:  0.17056512832641602 \t1.1972856521606445 \t0.0\n",
      "Batch:\t7 /4000\t:  0.17915129661560059 \t1.3784055709838867 \t0.0\n",
      "Batch:\t8 /4000\t:  0.16622328758239746 \t1.5459372997283936 \t0.31533241271972656\n",
      "Batch:\t9 /4000\t:  0.179459810256958 \t1.7270925045013428 \t0.0\n",
      "Batch:\t10 /4000\t:  0.1739494800567627 \t1.90274977684021 \t2.7680516242980957\n",
      "Batch:\t11 /4000\t:  0.16282939910888672 \t2.0669174194335938 \t0.33475494384765625\n",
      "Batch:\t12 /4000\t:  0.1634678840637207 \t2.231590986251831 \t1.0415964126586914\n",
      "Batch:\t13 /4000\t:  0.16974139213562012 \t2.4029502868652344 \t1.8677411079406738\n",
      "Batch:\t14 /4000\t:  0.16345787048339844 \t2.568164348602295 \t0.18804931640625\n",
      "Batch:\t15 /4000\t:  0.18282723426818848 \t2.7523977756500244 \t0.024446487426757812\n",
      "Batch:\t16 /4000\t:  0.16335344314575195 \t2.9177470207214355 \t1.325444221496582\n",
      "Batch:\t17 /4000\t:  0.10122919082641602 \t3.020641326904297 \t0.0\n",
      "Time: 4.93095588684082\t Loss: 9.84305715560913\t Valdacc: 0.44\t Testacc: 0.41\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  139 / 200\n",
      "Batch:\t0 /4000\t:  0.16556692123413086 \t0.1670219898223877 \t0.9381561279296875\n",
      "Batch:\t1 /4000\t:  0.1739826202392578 \t0.3487582206726074 \t1.6471195220947266\n",
      "Batch:\t2 /4000\t:  0.16320085525512695 \t0.5134017467498779 \t2.457796096801758\n",
      "Batch:\t3 /4000\t:  0.16758227348327637 \t0.6824650764465332 \t2.1029534339904785\n",
      "Batch:\t4 /4000\t:  0.16585874557495117 \t0.850229024887085 \t2.3340444564819336\n",
      "Batch:\t5 /4000\t:  0.16189908981323242 \t1.0134305953979492 \t0.0\n",
      "Batch:\t6 /4000\t:  0.16527962684631348 \t1.1806340217590332 \t0.0\n",
      "Batch:\t7 /4000\t:  0.17084455490112305 \t1.3527839183807373 \t0.0\n",
      "Batch:\t8 /4000\t:  0.16613984107971191 \t1.5199472904205322 \t1.2234668731689453\n",
      "Batch:\t9 /4000\t:  0.1613783836364746 \t1.6827576160430908 \t0.008633613586425781\n",
      "Batch:\t10 /4000\t:  0.16070818901062012 \t1.8448996543884277 \t0.16348743438720703\n",
      "Batch:\t11 /4000\t:  0.16478943824768066 \t2.011194944381714 \t2.0701494216918945\n",
      "Batch:\t12 /4000\t:  0.15947842597961426 \t2.1727066040039062 \t0.0\n",
      "Batch:\t13 /4000\t:  0.20195245742797852 \t2.3763442039489746 \t0.0\n",
      "Batch:\t14 /4000\t:  0.1599128246307373 \t2.537752628326416 \t0.0\n",
      "Batch:\t15 /4000\t:  0.1643846035003662 \t2.7035396099090576 \t0.4803614616394043\n",
      "Batch:\t16 /4000\t:  0.17006731033325195 \t2.875368356704712 \t1.246541976928711\n",
      "Batch:\t17 /4000\t:  0.10077261924743652 \t2.977416753768921 \t0.0\n",
      "Time: 4.949451923370361\t Loss: 14.672710418701172\t Valdacc: 0.42\t Testacc: 0.42\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  140 / 200\n",
      "Batch:\t0 /4000\t:  0.16391491889953613 \t0.1665782928466797 \t1.1215829849243164\n",
      "Batch:\t1 /4000\t:  0.16393709182739258 \t0.332378625869751 \t0.20457565784454346\n",
      "Batch:\t2 /4000\t:  0.1611940860748291 \t0.4950568675994873 \t1.2058734893798828\n",
      "Batch:\t3 /4000\t:  0.16352248191833496 \t0.659820556640625 \t0.10263442993164062\n",
      "Batch:\t4 /4000\t:  0.1646890640258789 \t0.8262941837310791 \t0.0\n",
      "Batch:\t5 /4000\t:  0.20131826400756836 \t1.0289828777313232 \t0.0\n",
      "Batch:\t6 /4000\t:  0.1744241714477539 \t1.2054169178009033 \t0.09618377685546875\n",
      "Batch:\t7 /4000\t:  0.18460822105407715 \t1.3922219276428223 \t6.847226142883301\n",
      "Batch:\t8 /4000\t:  0.16379070281982422 \t1.5575141906738281 \t2.8601016998291016\n",
      "Batch:\t9 /4000\t:  0.18481230735778809 \t1.7438712120056152 \t0.0\n",
      "Batch:\t10 /4000\t:  0.17907404899597168 \t1.9244143962860107 \t0.0\n",
      "Batch:\t11 /4000\t:  0.1764364242553711 \t2.102280855178833 \t0.0\n",
      "Batch:\t12 /4000\t:  0.15928149223327637 \t2.2634124755859375 \t0.0\n",
      "Batch:\t13 /4000\t:  0.16444993019104004 \t2.42940354347229 \t0.7775497436523438\n",
      "Batch:\t14 /4000\t:  0.16440629959106445 \t2.5953145027160645 \t0.0\n",
      "Batch:\t15 /4000\t:  0.16557741165161133 \t2.7622079849243164 \t5.098135948181152\n",
      "Batch:\t16 /4000\t:  0.1629645824432373 \t2.926718235015869 \t0.0\n",
      "Batch:\t17 /4000\t:  0.12406539916992188 \t3.053680658340454 \t0.0\n",
      "Time: 5.0041725635528564\t Loss: 18.31386387348175\t Valdacc: 0.44\t Testacc: 0.415\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  141 / 200\n",
      "Batch:\t0 /4000\t:  0.1816396713256836 \t0.1825118064880371 \t1.6367225646972656\n",
      "Batch:\t1 /4000\t:  0.17249131202697754 \t0.356203556060791 \t0.0\n",
      "Batch:\t2 /4000\t:  0.17022991180419922 \t0.5281429290771484 \t0.6576175689697266\n",
      "Batch:\t3 /4000\t:  0.1816883087158203 \t0.7112455368041992 \t0.45131683349609375\n",
      "Batch:\t4 /4000\t:  0.16643571853637695 \t0.8793509006500244 \t0.015878677368164062\n",
      "Batch:\t5 /4000\t:  0.18744564056396484 \t1.073927640914917 \t0.0\n",
      "Batch:\t6 /4000\t:  0.16506576538085938 \t1.240438461303711 \t0.0\n",
      "Batch:\t7 /4000\t:  0.16705918312072754 \t1.409008502960205 \t0.20260334014892578\n",
      "Batch:\t8 /4000\t:  0.1618640422821045 \t1.5726819038391113 \t0.0\n",
      "Batch:\t9 /4000\t:  0.1675422191619873 \t1.7416038513183594 \t0.0\n",
      "Batch:\t10 /4000\t:  0.2162928581237793 \t1.959824562072754 \t0.0\n",
      "Batch:\t11 /4000\t:  0.16410446166992188 \t2.125253438949585 \t1.0736503601074219\n",
      "Batch:\t12 /4000\t:  0.18611764907836914 \t2.312645196914673 \t0.0\n",
      "Batch:\t13 /4000\t:  0.1804490089416504 \t2.505537986755371 \t0.0\n",
      "Batch:\t14 /4000\t:  0.16481351852416992 \t2.672001361846924 \t0.44719982147216797\n",
      "Batch:\t15 /4000\t:  0.16442251205444336 \t2.838296890258789 \t0.380218505859375\n",
      "Batch:\t16 /4000\t:  0.16788339614868164 \t3.0088303089141846 \t0.0\n",
      "Batch:\t17 /4000\t:  0.10070013999938965 \t3.1107900142669678 \t0.0\n",
      "Time: 5.04034423828125\t Loss: 4.865207672119141\t Valdacc: 0.44\t Testacc: 0.39\n",
      " BestValidAcc: 0.48\n",
      " BestTestAcc: 0.465\n",
      "\n",
      "Epoch:  142 / 200\n",
      "Batch:\t0 /4000\t:  0.1647937297821045 \t0.1680622100830078 \t0.7556297779083252\n",
      "Batch:\t1 /4000\t:  0.16597890853881836 \t0.335892915725708 \t2.102426528930664\n",
      "Batch:\t2 /4000\t:  0.1656169891357422 \t0.5032846927642822 \t0.364349365234375\n",
      "Batch:\t3 /4000\t:  0.16076302528381348 \t0.6655974388122559 \t0.0\n",
      "Batch:\t4 /4000\t:  0.18229365348815918 \t0.8494420051574707 \t0.0\n",
      "Batch:\t5 /4000\t:  0.16221165657043457 \t1.0133299827575684 \t0.0\n",
      "Batch:\t6 /4000\t:  0.1651930809020996 \t1.1802785396575928 \t0.19211292266845703\n",
      "Batch:\t7 /4000\t:  0.1648726463317871 \t1.3465394973754883 \t0.0\n",
      "Batch:\t8 /4000\t:  0.16246628761291504 \t1.5105292797088623 \t2.9944982528686523\n",
      "Batch:\t9 /4000\t:  0.1621389389038086 \t1.6742594242095947 \t1.2038154602050781\n",
      "Batch:\t10 /4000\t:  0.17851758003234863 \t1.8546390533447266 \t2.4741058349609375\n",
      "Batch:\t11 /4000\t:  0.16734933853149414 \t2.023531675338745 \t0.5130186080932617\n",
      "Batch:\t12 /4000\t:  0.16688179969787598 \t2.192293167114258 \t0.5650224685668945\n",
      "Batch:\t13 /4000\t:  0.16144871711730957 \t2.3551504611968994 \t0.3408679962158203\n",
      "Batch:\t14 /4000\t:  0.1628711223602295 \t2.5197088718414307 \t0.0\n",
      "Batch:\t15 /4000\t:  0.18194007873535156 \t2.703212022781372 \t0.0\n",
      "Batch:\t16 /4000\t:  0.17310523986816406 \t2.877781391143799 \t0.0\n",
      "Batch:\t17 /4000\t:  0.10149335861206055 \t2.98110294342041 \t0.0\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if not pointwise:\n",
    "    loss_func = nn.MarginRankingLoss(margin=1, size_average=False)\n",
    "else:\n",
    "    loss_func = nn.BCEWithLogitsLoss()\n",
    "    training_model += '_pointwise'\n",
    "# db = training_loop(training_model = training_model,\n",
    "#                    parameter_dict = parameter_dict,\n",
    "#                    modeler = modeler,\n",
    "#                    train_loader = train_loader,\n",
    "#                    optimizer=optimizer,\n",
    "#                    loss_func=loss_func,\n",
    "#                    data=data,\n",
    "#                    dataset=parameter_dict['dataset'],\n",
    "#                    device=device,\n",
    "#                    test_every=1,\n",
    "#                    validate_every=1,\n",
    "#                     pointwise=pointwise,\n",
    "#                    problem='core_chain')\n",
    "train_loss, modeler, valid_accuracy, test_accuracy = cc.training_loop(training_model = training_model,\n",
    "                                                                           parameter_dict = parameter_dict,\n",
    "                                                                           modeler = modeler,\n",
    "                                                                           train_loader = train_loader,\n",
    "                                                                           optimizer=optimizer,\n",
    "                                                                           loss_func=loss_func,\n",
    "                                                                           data=data,\n",
    "                                                                           dataset=parameter_dict['dataset'],\n",
    "                                                                           device=device,\n",
    "                                                                           test_every=1,\n",
    "                                                                           validate_every=1,\n",
    "                                                                            pointwise=pointwise,\n",
    "                                                                           problem='core_chain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = modeler.encoder.init_hidden(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 256]), torch.Size([1, 4, 256]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h[0][0].shape, h[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29, 0.33, 0.37, 0.36, 0.38, 0.4, 0.39, 0.4, 0.4, 0.35, 0.37, 0.4, 0.4, 0.36, 0.42, 0.43, 0.44, 0.43, 0.4, 0.42, 0.43, 0.44, 0.41, 0.42, 0.47, 0.47, 0.41, 0.48, 0.43, 0.39, 0.4, 0.41, 0.39, 0.44, 0.41, 0.41, 0.44, 0.44, 0.47, 0.4, 0.4, 0.41, 0.4, 0.45, 0.38, 0.41, 0.4, 0.42, 0.41, 0.4, 0.41, 0.41, 0.42, 0.41, 0.42, 0.43, 0.41, 0.37, 0.34, 0.38, 0.35, 0.44, 0.45, 0.38, 0.35, 0.37, 0.39, 0.41, 0.41, 0.45, 0.44, 0.39, 0.38, 0.37, 0.39, 0.45, 0.42, 0.4, 0.41, 0.4, 0.39, 0.41, 0.42, 0.43, 0.42, 0.41, 0.42, 0.41, 0.39, 0.43, 0.39, 0.39, 0.4, 0.39, 0.39, 0.41, 0.41, 0.36, 0.4, 0.43, 0.41, 0.41, 0.43, 0.41, 0.4, 0.42, 0.39, 0.42, 0.42, 0.43, 0.42, 0.38, 0.43, 0.43, 0.41, 0.41, 0.41, 0.38, 0.38, 0.37, 0.38, 0.39, 0.33, 0.34, 0.4, 0.4, 0.41, 0.41, 0.43, 0.42, 0.4, 0.44, 0.4, 0.39, 0.42, 0.41, 0.43, 0.45, 0.44, 0.42, 0.44, 0.44]\n",
      "[0.24, 0.305, 0.35, 0.33, 0.365, 0.375, 0.34, 0.38, 0.39, 0.38, 0.38, 0.365, 0.395, 0.355, 0.4, 0.405, 0.38, 0.38, 0.405, 0.36, 0.395, 0.39, 0.425, 0.395, 0.41, 0.41, 0.41, 0.41, 0.4, 0.415, 0.37, 0.405, 0.425, 0.42, 0.395, 0.4, 0.395, 0.365, 0.4, 0.4, 0.385, 0.39, 0.395, 0.395, 0.4, 0.4, 0.41, 0.395, 0.395, 0.37, 0.365, 0.4, 0.4, 0.4, 0.425, 0.41, 0.39, 0.465, 0.41, 0.44, 0.415, 0.43, 0.43, 0.425, 0.415, 0.43, 0.42, 0.405, 0.395, 0.39, 0.405, 0.395, 0.385, 0.38, 0.375, 0.385, 0.385, 0.385, 0.39, 0.38, 0.385, 0.43, 0.405, 0.41, 0.37, 0.375, 0.37, 0.405, 0.39, 0.395, 0.395, 0.4, 0.385, 0.405, 0.4, 0.39, 0.38, 0.38, 0.395, 0.4, 0.39, 0.385, 0.4, 0.375, 0.395, 0.4, 0.39, 0.405, 0.415, 0.4, 0.445, 0.42, 0.415, 0.41, 0.4, 0.385, 0.375, 0.395, 0.405, 0.375, 0.405, 0.42, 0.425, 0.425, 0.4, 0.415, 0.405, 0.415, 0.415, 0.41, 0.395, 0.4, 0.425, 0.435, 0.43, 0.41, 0.43, 0.43, 0.41, 0.42, 0.415, 0.39, 0.43]\n",
      "validation accuracy is ,  0.48\n",
      "maximum test accuracy is ,  0.465\n",
      "correct test accuracy i.e test accuracy where validation is highest is  0.41\n"
     ]
    }
   ],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.show()\n",
    "plt.plot(valid_accuracy)\n",
    "plt.show()\n",
    "plt.plot(test_accuracy)\n",
    "plt.show()\n",
    "print(valid_accuracy)\n",
    "print(test_accuracy)\n",
    "print(\"validation accuracy is , \", max(valid_accuracy))\n",
    "print(\"maximum test accuracy is , \", max(test_accuracy))\n",
    "print(\"correct test accuracy i.e test accuracy where validation is highest is \", test_accuracy[valid_accuracy.index(max(valid_accuracy))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-61c7ec9257ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m encoder_mask = RNN_Encoder_Masks(vocab_sz=1000, emb_sz=400, weight_p=0,\n\u001b[0;32m----> 2\u001b[0;31m                                                  n_hid=256, n_layers=2, pad_token=0, qrnn=False).to(device)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLockedDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPoolingLinearClassifierMask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-d77e6c772449>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token, bidir, hidden_p, input_p, embed_p, weight_p, qrnn)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_sz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_hid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memb_sz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_hid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpad_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_sz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_sz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_dp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbeddingDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqrnn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;31m#Using QRNN requires cupy: https://github.com/cupy/cupy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "encoder_mask = RNN_Encoder_Masks(vocab_sz=1000, emb_sz=400, weight_p=0,\n",
    "                                                 n_hid=256, n_layers=2, pad_token=0, qrnn=False).to(device)\n",
    "dropout = LockedDropout(0.1)\n",
    "classifier = PoolingLinearClassifierMask(layers=[400*3, 50, 300], drops=[0.4, 0.1]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = RNN_Encoder(ntoken=1000, emb_sz=400, \n",
    "                                                 n_hid=256, n_layers=2, pad_token=0, qrnn=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 9\n",
    "b = 10\n",
    "x = torch.randint(0,1000,(b,s))\n",
    "# x[1,6:] = 0\n",
    "# x[2,4:] = 0\n",
    "# x[3,6:] = 0\n",
    "x_ = torch.tensor(x,dtype=torch.long,device=device)\n",
    "print(x_.shape)\n",
    "output = encoder(x_)\n",
    "# output = classifier(encoder(x))\n",
    "# out = dropout(output[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[-1][-1][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_mask = encoder_mask(x_.transpose(1,0))\n",
    "out = encoder(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,len =  pad_packed_sequence(pack_sequence(x_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(valid_accuracy)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(test_accuracy)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = output[-1][-1]\n",
    "a = output.view(output.size(0)*output.size(1), output.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[-1][-1][-1].shape)\n",
    "print(output[-1][-1].permute(1,2,0)[:,:,-1].shape)\n",
    "torch.equal(output[-1][-1][-1],output[-1][-1].permute(1,2,0)[:,:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
