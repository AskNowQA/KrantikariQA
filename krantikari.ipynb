{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook would be used to run the code on reduced dataset which can then offer finer controls over the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/priyansh/new_kranti/merity\n"
     ]
    }
   ],
   "source": [
    "cd merity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model as merity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/priyansh/new_kranti\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ULMFIT vocab and vectors from disk. Sit Tight.\n",
      "Label Cache not found. Creating a new one\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/data/priyansh/new_kranti/utils/dbpedia_interface.py\", line 133, in __init__\n",
      "    self.labels = pickle.load(open('resources/labels.pickle'))\n",
      "TypeError: a bytes-like object is required, not 'str'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT IMPLEMENTED FUNCTIONALITY NOT IMPLEMENTED FUNCTIONALITY !!!\n"
     ]
    }
   ],
   "source": [
    "# Torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import  DataLoader\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_sequence \n",
    "\n",
    "# Local imports\n",
    "import data_loader as dl\n",
    "import auxiliary as aux\n",
    "import network as net\n",
    "\n",
    "# Other libs\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from configs import config_loader as cl\n",
    "import corechain as cc\n",
    "import components as com\n",
    "import utils.tensor_utils as tu\n",
    "\n",
    "sys.path.append('/data/priyansh/conda/fastai')\n",
    "from fastai.text import *\n",
    "import fastai\n",
    "from fastai import text, core, lm_rnn\n",
    "from typing import Any, AnyStr, Callable, Collection, Dict, Hashable, Iterator, List, Mapping, NewType, Optional\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT IMPLEMENTED FUNCTIONALITY NOT IMPLEMENTED FUNCTIONALITY !!!\n"
     ]
    }
   ],
   "source": [
    "#setting up device,model name and loss types.\n",
    "device = torch.device(\"cuda\")\n",
    "training_model = 'bilstm_dot_ulmfit'\n",
    "_dataset = 'lcquad'\n",
    "_train_over_validation = False\n",
    "pointwise = False\n",
    "_train_over_validation = False\n",
    "finetune = False\n",
    "bidirectional = True\n",
    "\n",
    "#Loading relations file.\n",
    "COMMON_DATA_DIR = 'data/data/common'\n",
    "_dataset_specific_data_dir = 'data/data/%(dataset)s/' % {'dataset': _dataset}\n",
    "_inv_relations = aux.load_inverse_relation(COMMON_DATA_DIR)\n",
    "_word_to_id = aux.load_word_list(COMMON_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pointwise:\n",
    "    training_config = 'pointwise'\n",
    "else:\n",
    "    training_config = 'pairwise'\n",
    "\n",
    "if training_model == 'reldet':\n",
    "    schema = 'reldet'\n",
    "elif training_model == 'slotptr' or training_model == 'slotptr_common_encoder' or training_model == 'slotptrortho':\n",
    "    schema = 'slotptr'\n",
    "elif training_model == 'bilstm_dot_multiencoder':\n",
    "    schema = 'default'\n",
    "else:\n",
    "    schema = 'default'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 25)\n",
      "(700, 25)\n",
      "(700, 1000, 25)\n",
      "(100, 25)\n",
      "(100, 25)\n",
      "(100, 1000, 25)\n",
      "(200, 25)\n",
      "(200, 25)\n",
      "(200, 1000, 25)\n",
      "(700, 25)\n"
     ]
    }
   ],
   "source": [
    "parameter_dict = cl.corechain_parameters(dataset=_dataset,training_model=training_model,\n",
    "                                             training_config=training_config,config_file='configs/macros.cfg')\n",
    "\n",
    "parameter_dict['batch_size'] = 400\n",
    "if _dataset == 'lcquad':\n",
    "    test_every = parameter_dict['test_every']\n",
    "else:\n",
    "    test_every = False\n",
    "validate_every = parameter_dict['validate_every']\n",
    "\n",
    "\n",
    "data = aux.load_data(_dataset=_dataset, _train_over_validation = _train_over_validation,\n",
    "                     _parameter_dict=parameter_dict, _relations =  _inv_relations, _pointwise=pointwise, _device=device,k=1000)\n",
    "\n",
    "train_loader = cc.load_data(data, parameter_dict, pointwise, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy pasting the RNN core module from the fastai"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class CustomEncoder(lm_rnn.MultiBatchRNN):\n",
    "    @property\n",
    "    def layers(self):\n",
    "        return torch.nn.ModuleList([torch.nn.ModuleList([self.rnns[0], self.dropouths[0]]),\n",
    "                                    torch.nn.ModuleList([self.rnns[1], self.dropouths[1]]),\n",
    "                                    torch.nn.ModuleList([self.rnns[2], self.dropouths[2]])])\n",
    "\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "\n",
    "    # @TODO: inject comments.\n",
    "    def __init__(self, _device: torch.device, ntoken: int, dps: list, enc_wgts, _debug=False):\n",
    "        super(TextClassifier, self).__init__()\n",
    "\n",
    "        self.device = _device\n",
    "\n",
    "        # Load the pre-trained model\n",
    "        args = {'ntoken': ntoken, 'emb_sz': 400, 'n_hid': 1150,\n",
    "                'n_layers': 3, 'pad_token': 0, 'qrnn': False, 'bptt': 70, 'max_seq': 1400,\n",
    "                'dropouti': dps[0], 'wdrop': dps[1], 'dropoute': dps[2], 'dropouth': dps[3]}\n",
    "        self.encoder = CustomEncoder(**args).to(self.device)\n",
    "        self.encoder.load_state_dict(enc_wgts)\n",
    "        '''\n",
    "            Make new classifier.\n",
    "            \n",
    "            Explanation:\n",
    "                400*3 because input is [ h_T, maxpool, meanpool ]\n",
    "                50 is hidden layer dim\n",
    "                2 is n_classes\n",
    "\n",
    "                0.4, 0.1 are drops at various layers\n",
    "        '''\n",
    "#         self.linear = text.PoolingLinearClassifier(layers=[400 * 3, 50, 2], drops=[dps[4], 0.1]).to(self.device)\n",
    "        self.encoder.reset()\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        layers = [x for x in self.encoder.layers]\n",
    "#         layers += [x for x in self.linear.layers]\n",
    "        return torch.nn.ModuleList(layers)\n",
    "\n",
    "    @property\n",
    "    def layers_rev(self):\n",
    "        layers = [x for x in self.encoder.layers]\n",
    "#         layers += [x for x in self.linear.layers]\n",
    "        layers.reverse()\n",
    "        return torch.nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # inputs are S*B\n",
    "\n",
    "        # Encoding all the data\n",
    "#         op_p = self.encoder(x.transpose(1, 0))\n",
    "        # pos_batch = op_p[1][-1][-1]\n",
    "#         score = self.linear(op_p)[0]        \n",
    "#         return score\n",
    "\n",
    "        ques_batch, pos_batch, neg_batch, y_label = data['ques_batch'], data['pos_batch'], data['neg_batch'], data['y_label']\n",
    "                \n",
    "        op_q = self.encoder(ques_batch.transpose(1,0))\n",
    "        op_p = self.encoder(pos_batch.transpose(1,0))\n",
    "        op_n = self.encoder(neg_batch.transpose(1,0))\n",
    "        \n",
    "        \n",
    "            \n",
    "        ques_batch_encoded =  op_q[1][-1]\n",
    "        pos_batch_encoded =  op_p[1][-1]\n",
    "        neg_batch_encoded =  op_n[1][-1]\n",
    "        \n",
    "#         ques_batch_encoded =  op_q[-1][0][1].squeeze()\n",
    "#         pos_batch_encoded =  op_p[-1][0][1].squeeze()\n",
    "#         neg_batch_encoded =  op_n[-1][0][1].squeeze()\n",
    "        \n",
    "#         print(ques_batch_encoded.shape, pos_batch_encoded.shape, neg_batch_encoded.shape)\n",
    "#         raise IOError\n",
    "        \n",
    "        #Calculating dot score\n",
    "        pos_scores = torch.sum(ques_batch_encoded * pos_batch_encoded, -1)\n",
    "        neg_scores = torch.sum(ques_batch_encoded * neg_batch_encoded, -1)\n",
    "        \n",
    "        \n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            predicted = self.forward(x)\n",
    "            self.train()\n",
    "            return predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     15,
     95,
     225,
     246,
     321
    ]
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from fastai.imports import *\n",
    "from fastai.torch_imports import *\n",
    "from fastai.rnn_reg import LockedDropout,WeightDrop,EmbeddingDropout\n",
    "from fastai.model import Stepper\n",
    "from fastai.core import set_grad_enabled\n",
    "import collections\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pack_sequence\n",
    "from typing import Sequence, Tuple, TypeVar, Union\n",
    "# from .imports.core import *\n",
    "# from fastai.imports.core import *\n",
    "# IS_TORCH_04 = LooseVersion(torch.__version__) >= LooseVersion('0.4')\n",
    "\n",
    "def seq2seq_reg(output, xtra, loss, alpha=0, beta=0):\n",
    "    hs,dropped_hs = xtra\n",
    "    if alpha:  # Activation Regularization\n",
    "        loss = loss + (alpha * dropped_hs[-1].pow(2).mean()).sum()\n",
    "    if beta:   # Temporal Activation Regularization (slowness)\n",
    "        h = hs[-1]\n",
    "        if len(h)>1: loss = loss + (beta * (h[1:] - h[:-1]).pow(2).mean()).sum()\n",
    "    return loss\n",
    "\n",
    "def repackage_var(h):\n",
    "    \"\"\"Wraps h in new Variables, to detach them from their history.\"\"\"\n",
    "    if IS_TORCH_04: return h.detach() if type(h) == torch.Tensor else tuple(repackage_var(v) for v in h)\n",
    "    else: return Variable(h.data) if type(h) == Variable else tuple(repackage_var(v) for v in h)\n",
    "    \n",
    "# def dropout_mask(x,sz,p):\n",
    "#     \"Return a dropout mask of the same type as x, size sz, with probability p to cancel an element.\"\n",
    "#     return x.new(*sz).bernoulli_(1-p).div_(1-p)\n",
    "\n",
    "# class RNNDropout(nn.Module):\n",
    "#     \"Dropout that is consistent on the seq_len dimension.\"\n",
    "#     def __init__(self, p:float=0.5):\n",
    "#         super().__init__()\n",
    "#         self.p=p\n",
    "\n",
    "#     def forward(self, x:Tensor) -> Tensor:\n",
    "#         if not self.training or self.p == 0.: return x\n",
    "#         m = dropout_mask(x.data, (1, x.size(1), x.size(2)), self.p)\n",
    "#         return x * m\n",
    "\n",
    "# class WeightDropout(nn.Module):\n",
    "#     \"A module that warps another layer in which some weights will be replaced by 0 during training.\"\n",
    "\n",
    "#     def __init__(self, module, weight_p, layer_names=['weight_hh_l0']):\n",
    "#         super().__init__()\n",
    "#         self.collection = {}\n",
    "#         self.module,self.weight_p,self.layer_names = module,weight_p,layer_names\n",
    "#         for layer in self.layer_names:\n",
    "#             #Makes a copy of the weights of the selected layers.\n",
    "#             w = getattr(self.module, layer)\n",
    "#             self.register_parameter(f'{layer}_raw', nn.Parameter(w.data))\n",
    "\n",
    "#     def _setweights(self):\n",
    "#         \"Apply dropout to the raw weights.\"\n",
    "#         for layer in self.layer_names:\n",
    "#             raw_w = getattr(self, f'{layer}_raw')\n",
    "#             self.module._parameters[layer] = F.dropout(raw_w, p=self.weight_p, training=self.training)\n",
    "\n",
    "#     def forward(self, *args):\n",
    "#         self._setweights()\n",
    "#         with warnings.catch_warnings():\n",
    "#             #To avoid the warning that comes because the weights aren't flattened.\n",
    "#             warnings.simplefilter(\"ignore\")\n",
    "#             return self.module.forward(*args)\n",
    "\n",
    "#     def reset(self):\n",
    "#         for layer in self.layer_names:\n",
    "#             raw_w = getattr(self, f'{layer}_raw')\n",
    "#             self.module._parameters[layer] = F.dropout(raw_w, p=self.weight_p, training=False)\n",
    "#         if hasattr(self.module, 'reset'): self.module.reset()\n",
    "\n",
    "# class EmbeddingDropout(nn.Module):\n",
    "#     \"Apply dropout in the embedding layer by zeroing out some elements of the embedding vector.\"\n",
    "\n",
    "#     def __init__(self, emb, embed_p:float):\n",
    "#         super().__init__()\n",
    "#         self.collection = {}\n",
    "#         self.emb,self.embed_p = emb,embed_p\n",
    "#         self.pad_idx = self.emb.padding_idx\n",
    "#         if self.pad_idx is None: self.pad_idx = -1\n",
    "\n",
    "#     def forward(self, words:LongTensor, scale:Optional[float]=None) -> Tensor:\n",
    "#         if self.training and self.embed_p != 0:\n",
    "#             size = (self.emb.weight.size(0),1)\n",
    "#             mask = dropout_mask(self.emb.weight.data, size, self.embed_p)\n",
    "#             masked_embed = self.emb.weight * mask\n",
    "#         else: masked_embed = self.emb.weight\n",
    "#         if scale: masked_embed.mul_(scale)\n",
    "#         return F.embedding(words, masked_embed, self.pad_idx, self.emb.max_norm,\n",
    "#         self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)\n",
    "\n",
    "class RNN_Encoder(nn.Module):\n",
    "\n",
    "    \"\"\"A custom RNN encoder network that uses\n",
    "        - an embedding matrix to encode input,\n",
    "        - a stack of LSTM or QRNN layers to drive the network, and\n",
    "        - variational dropouts in the embedding and LSTM/QRNN layers\n",
    "        The architecture for this network was inspired by the work done in\n",
    "        \"Regularizing and Optimizing LSTM Language Models\".\n",
    "        (https://arxiv.org/pdf/1708.02182.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, ntoken, emb_sz, n_hid, n_layers, pad_token, bidir=False,\n",
    "                 dropouth=0.3, dropouti=0.65, dropoute=0.1, wdrop=0.5, qrnn=False):\n",
    "        \"\"\" Default constructor for the RNN_Encoder class\n",
    "            Args:\n",
    "                bs (int): batch size of input data\n",
    "                ntoken (int): number of vocabulary (or tokens) in the source dataset\n",
    "                emb_sz (int): the embedding size to use to encode each token\n",
    "                n_hid (int): number of hidden activation per LSTM layer\n",
    "                n_layers (int): number of LSTM layers to use in the architecture\n",
    "                pad_token (int): the int value used for padding text.\n",
    "                dropouth (float): dropout to apply to the activations going from one LSTM layer to another\n",
    "                dropouti (float): dropout to apply to the input layer.\n",
    "                dropoute (float): dropout to apply to the embedding layer.\n",
    "                wdrop (float): dropout used for a LSTM's internal (or hidden) recurrent weights.\n",
    "            Returns:\n",
    "                None\n",
    "          \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.ndir = 2 if bidir else 1\n",
    "        self.bs, self.qrnn = 1, qrnn\n",
    "        self.pad_token =pad_token\n",
    "        self.encoder = nn.Embedding(ntoken, emb_sz, padding_idx=pad_token)\n",
    "        self.encoder_with_dropout = EmbeddingDropout(self.encoder)\n",
    "        if self.qrnn:\n",
    "            #Using QRNN requires cupy: https://github.com/cupy/cupy\n",
    "            from .torchqrnn.qrnn import QRNNLayer\n",
    "            self.rnns = [QRNNLayer(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.ndir,\n",
    "                save_prev_x=True, zoneout=0, window=2 if l == 0 else 1, output_gate=True) for l in range(n_layers)]\n",
    "            if wdrop:\n",
    "                for rnn in self.rnns:\n",
    "                    rnn.linear = WeightDrop(rnn.linear, wdrop, weights=['weight'])\n",
    "        else:\n",
    "            self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.ndir,\n",
    "                1, bidirectional=bidir) for l in range(n_layers)]\n",
    "            if wdrop: self.rnns = [WeightDrop(rnn, wdrop) for rnn in self.rnns]\n",
    "        self.rnns = torch.nn.ModuleList(self.rnns)\n",
    "        self.encoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "\n",
    "        self.emb_sz,self.n_hid,self.n_layers,self.dropoute = emb_sz,n_hid,n_layers,dropoute\n",
    "        self.dropouti = LockedDropout(dropouti)\n",
    "        self.dropouths = nn.ModuleList([LockedDropout(dropouth) for l in range(n_layers)])\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\" Invoked during the forward propagation of the RNN_Encoder module.\n",
    "        Args:\n",
    "            input (Tensor): input of shape (sentence length x batch_size)\n",
    "        Returns:\n",
    "            raw_outputs (tuple(list (Tensor), list(Tensor)): list of tensors evaluated from each RNN layer without using\n",
    "            dropouth, list of tensors evaluated from each RNN layer using dropouth,\n",
    "        \"\"\"\n",
    "        sl,bs = input.size()\n",
    "        if bs!=self.bs:\n",
    "            self.bs=bs\n",
    "            self.reset()\n",
    "        with set_grad_enabled(self.training):\n",
    "            mask = tu.compute_mask(input.transpose(1, 0))\n",
    "            \n",
    "            emb = self.encoder_with_dropout(input, dropout=self.dropoute if self.training else 0)\n",
    "#             emb = self.encoder_with_dropout(input)\n",
    "            emb = self.dropouti(emb)\n",
    "#             print(\"emb shape is \", emb.shape)\n",
    "\n",
    "                    \n",
    "            lengths = mask.eq(1).long().sum(1)                                    # bs\n",
    "            lengths_sort, idx_sort = torch.sort(lengths, dim=0, descending=True)  # bs\n",
    "            _, idx_unsort = torch.sort(idx_sort, dim=0)                           # bs\n",
    "\n",
    "            emb_sort = emb.index_select(1, idx_sort)    # sl * bs * ninp\n",
    "            hid_sort = [(h[0].index_select(1, idx_sort), h[1].index_select(1, idx_sort)) for h in self.hidden]\n",
    "            emb_sort = torch.nn.utils.rnn.pack_padded_sequence(emb_sort, lengths_sort)\n",
    "\n",
    "            \n",
    "#             raw_output = emb\n",
    "            new_hidden,raw_outputs,outputs,raw_outputs_sorted = [],[],[],[]\n",
    "    \n",
    "            for l, (rnn,drop) in enumerate(zip(self.rnns, self.dropouths)):\n",
    "                current_input = emb_sort\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "#                     print(\"shape of raw output is\", raw_output.shape)\n",
    "                    emb_sort, new_h = rnn(emb_sort, hid_sort[l])\n",
    "\n",
    "                emb_sort, _ = torch.nn.utils.rnn.pad_packed_sequence(emb_sort)\n",
    "                new_hidden.append(new_h)\n",
    "                raw_outputs.append(emb_sort)\n",
    "                if l != self.n_layers - 1: \n",
    "                    emb_sort = drop(emb_sort)\n",
    "                    outputs.append(emb_sort)\n",
    "                \n",
    "#             self.hidden = repackage_var(new_hidden)\n",
    "            output =  self.dropouths[-1](emb_sort)\n",
    "            outputs.append(output)\n",
    "        \n",
    "            raw_outputs = [raw_output.index_select(1, idx_unsort) for raw_output in raw_outputs]\n",
    "            outputs = [output.index_select(1, idx_unsort) for output in outputs]\n",
    "            new_hidden = [(h_sort[0].index_select(1, idx_unsort), h_sort[1].index_select(1, idx_unsort)) for h_sort in new_hidden]\n",
    "    #         output = self.lockdrop(emb_sort, self.dropout)\n",
    "    #         outputs.append(output)\n",
    "\n",
    "        return raw_outputs, new_hidden, outputs\n",
    "\n",
    "    def one_hidden(self, l):\n",
    "#         print(self.bs)\n",
    "        nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz)//self.ndir\n",
    "        if IS_TORCH_04: return Variable(self.weights.new(self.ndir, self.bs, nh).zero_())\n",
    "        else: return Variable(self.weights.new(self.ndir, self.bs, nh).zero_(), volatile=not self.training)\n",
    "\n",
    "    def reset(self):\n",
    "        if self.qrnn: [r.reset() for r in self.rnns]\n",
    "        self.weights = next(self.parameters()).data\n",
    "        if self.qrnn: self.hidden = [self.one_hidden(l) for l in range(self.n_layers)]\n",
    "        else: self.hidden = [(self.one_hidden(l), self.one_hidden(l)) for l in range(self.n_layers)]\n",
    "    \n",
    "    def reset_hidden(self):\n",
    "        self.hidden = [(self.one_hidden(l), self.one_hidden(l)) for l in range(self.n_layers)]\n",
    "\n",
    "class PoolingLinearClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, layers, drops):\n",
    "        super().__init__()\n",
    "        print(\"done\")\n",
    "#         self.layers = nn.ModuleList([\n",
    "#             LinearBlock(layers[i], layers[i + 1], drops[i]) for i in range(len(layers) - 1)])\n",
    "\n",
    "    def pool(self, x, bs, is_max):\n",
    "        f = F.adaptive_max_pool1d if is_max else F.adaptive_avg_pool1d\n",
    "        return f(x.permute(1,2,0), (1,)).view(bs,-1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs = input\n",
    "        output = outputs[-1]\n",
    "        sl,bs,_ = output.size()\n",
    "        avgpool = self.pool(output, bs, False)\n",
    "        mxpool = self.pool(output, bs, True)\n",
    "        x = torch.cat([output[-1], mxpool, avgpool], 1)\n",
    "        return [output[-1]]\n",
    "    \n",
    "class RNN_Encoder_Masks(nn.Module):\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, vocab_sz:int, emb_sz:int, n_hid:int, n_layers:int, pad_token:int, bidir:bool=False,\n",
    "                 hidden_p:float=0.2, input_p:float=0.6, embed_p:float=0.1, weight_p:float=0.5, qrnn:bool=False):\n",
    "\n",
    "        super().__init__()\n",
    "        self.collection = {}\n",
    "        self.bs,self.qrnn,self.ndir = 1, qrnn,(2 if bidir else 1)\n",
    "        self.emb_sz,self.n_hid,self.n_layers,self.pad_token = emb_sz,n_hid,n_layers,pad_token\n",
    "        self.encoder = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n",
    "        self.encoder_dp = EmbeddingDropout(self.encoder, embed_p)\n",
    "        if self.qrnn:\n",
    "            #Using QRNN requires cupy: https://github.com/cupy/cupy\n",
    "            from .qrnn.qrnn import QRNNLayer\n",
    "            self.rnns = [QRNNLayer(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.ndir,\n",
    "                                   save_prev_x=True, zoneout=0, window=2 if l == 0 else 1, output_gate=True,\n",
    "                                   use_cuda=torch.cuda.is_available()) for l in range(n_layers)]\n",
    "            if weight_p != 0.:\n",
    "                for rnn in self.rnns:\n",
    "                    rnn.linear = WeightDropout(rnn.linear, weight_p, layer_names=['weight'])\n",
    "        else:\n",
    "            self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.ndir,\n",
    "                1, bidirectional=bidir) for l in range(n_layers)]\n",
    "            if weight_p != 0.: self.rnns = [WeightDropout(rnn, weight_p) for rnn in self.rnns]\n",
    "        self.rnns = torch.nn.ModuleList(self.rnns)\n",
    "        self.encoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.input_dp = RNNDropout(input_p)\n",
    "        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n",
    "\n",
    "    def forward(self, input:LongTensor):\n",
    "        if is_listy(input):\n",
    "            input, lengths = input\n",
    "        else:\n",
    "            input, lengths = pad_packed_sequence(pack_sequence(input))\n",
    "\n",
    "\n",
    "        # pack_padded can not handle zero lengths texts\n",
    "        empty_inputs = np.argwhere(lengths <= 0)\n",
    "        lengths[empty_inputs] = 1\n",
    "\n",
    "        sl, bs = input.size()\n",
    "        if bs != self.bs:\n",
    "            self.bs = bs\n",
    "            self.reset()\n",
    "\n",
    "        raw_output = self.input_dp(self.encoder_dp(input))\n",
    "        new_hidden, raw_outputs, outputs = [], [], []\n",
    "        for l, (rnn, hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
    "\n",
    "            packed_rnn_inp = pack_padded_sequence(raw_output, lengths)\n",
    "            rnn_output, new_h = rnn(packed_rnn_inp, self.hidden[l])\n",
    "            raw_output, _ = pad_packed_sequence(rnn_output)\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)\n",
    "            outputs.append(raw_output)\n",
    "#         self.hidden = _repackage_var(new_hidden)\n",
    "        return raw_outputs, outputs, lengths\n",
    "\n",
    "    def _one_hidden(self, l:int) -> Tensor:\n",
    "        \"Return one hidden state.\"\n",
    "        nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz)//self.ndir\n",
    "        return self.weights.new(self.ndir, self.bs, nh).zero_()\n",
    "\n",
    "    def reset(self):\n",
    "        \"Reset the hidden states.\"\n",
    "        [r.reset() for r in self.rnns if hasattr(r, 'reset')]\n",
    "        self.weights = next(self.parameters()).data\n",
    "        if self.qrnn: self.hidden = [self._one_hidden(l) for l in range(self.n_layers)]\n",
    "        else: self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]\n",
    "    \n",
    "    def reset_hidden(self):\n",
    "        self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]\n",
    "            \n",
    "class PoolingLinearClassifierMask(nn.Module):\n",
    "    \"Create a linear classifier with pooling.\"\n",
    "\n",
    "    def __init__(self, layers:Collection[int], drops:Collection[float]):\n",
    "        super().__init__()\n",
    "        mod_layers = []\n",
    "#         activs = [nn.ReLU(inplace=True)] * (len(layers) - 2) + [None]\n",
    "#         for n_in,n_out,p,actn in zip(layers[:-1],layers[1:], drops, activs):\n",
    "#             mod_layers += bn_drop_lin(n_in, n_out, p=p, actn=actn)\n",
    "#         self.layers = nn.Sequential(*mod_layers)\n",
    "\n",
    "    def avg_pool(self, x:Tensor, lengths:LongTensor):\n",
    "        diffs_from_max = lengths[0] - lengths\n",
    "        avg_lengths = np.minimum(x.shape[0] - diffs_from_max, lengths)\n",
    "        return torch.div(torch.sum(x, dim=0).permute(1, 0), avg_lengths.float()).permute(1, 0)\n",
    "\n",
    "    def max_pool(self, x:Tensor, bs:int):\n",
    "        \"Pool the tensor along the seq_len dimension.\"\n",
    "        return F.adaptive_max_pool1d(x.permute(1,2,0), (1,)).view(bs,-1)\n",
    "\n",
    "    def last_output(self, outputs:Tensor, lengths:Collection[float]):\n",
    "        diffs_from_max = lengths[0] - lengths\n",
    "        batch_lengths = outputs.shape[0] - diffs_from_max\n",
    "        return outputs[batch_lengths - 1, np.arange(outputs.shape[1]), :]\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor, LongTensor]) -> Tuple[Tensor,Tensor,Tensor]:\n",
    "        raw_outputs, outputs, lengths = input\n",
    "        output = outputs[-1]\n",
    "        sl,bs,_ = output.size()\n",
    "#         avgpool = self.avg_pool(output, lengths)\n",
    "#         mxpool = self.max_pool(output, bs)\n",
    "        return [self.last_output(outputs[-1].transpose(1,0), lengths)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = RNN_Encoder_Masks(vocab_sz=1000, emb_sz=400, weight_p=0.0,\n",
    "#                                                  n_hid=256, n_layers=2, pad_token=0, qrnn=False).to(device)\n",
    "# dropout = LockedDropout(0.1)\n",
    "# classifier = PoolingLinearClassifierMask(layers=[400*3, 50, 300], drops=[0.4, 0.1]).to(device)\n",
    "# s = 8\n",
    "# b = 11\n",
    "# x = torch.randint(0,1000,(b,s))\n",
    "# x = torch.tensor(x.transpose(1,0),dtype=torch.long,device=device)\n",
    "# output = classifier(encoder(x))\n",
    "# output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \"\"\"\n",
    "        Boilerplate class which helps others have some common functionality.\n",
    "        These are made with some debugging/loading and with corechains in mind\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def prepare_save(self):\n",
    "        pass\n",
    "\n",
    "    def load_from(self, location):\n",
    "        # Pull the data from disk\n",
    "        model_dump = torch.load(location)\n",
    "\n",
    "        # Load parameters\n",
    "        for key in self.prepare_save():\n",
    "            key[1].load_state_dict(model_dump[key[0]])\n",
    "\n",
    "    def get_parameter_sum(self):\n",
    "\n",
    "        sum = 0\n",
    "        for model in self.prepare_save():\n",
    "\n",
    "            model_sum = 0\n",
    "            for x in list(model[1].parameters()):\n",
    "\n",
    "                model_sum += np.sum(x.data.cpu().numpy().flatten())\n",
    "\n",
    "            sum += model_sum\n",
    "\n",
    "        return sum\n",
    "\n",
    "    def freeze_layer(self,layer):\n",
    "        for params in layer.parameters():\n",
    "            params.requires_grad = False\n",
    "\n",
    "    def unfreeze_layer(self,layer):\n",
    "        for params in layer.parameters():\n",
    "            params.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     77,
     250
    ]
   },
   "outputs": [],
   "source": [
    "class BiLstmDot_ulmfit(Model):\n",
    "\n",
    "    def __init__(self, _parameter_dict, _word_to_id, _device, _pointwise=False, _debug=False):\n",
    "\n",
    "        self.debug = _debug\n",
    "        self.parameter_dict = _parameter_dict\n",
    "        self.device = _device\n",
    "        self.pointwise = _pointwise\n",
    "        self.word_to_id = _word_to_id\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"Init Models\")\n",
    "\n",
    "        # Load the pre-trained model\n",
    "        pretrained_weights = torch.load('./ulmfit/wt103/fwd_wt103_enc.h5', map_location= lambda storage, loc: storage)\n",
    "        new_vectors = self.parameter_dict['vectors']\n",
    "        pretrained_weights['encoder.weight'] = T(new_vectors)\n",
    "        pretrained_weights.pop('encoder_with_dropout.embed.weight')\n",
    "#         pretrained_weights['encoder_with_dropout.embed.weight'] = T(np.copy(new_vectors))\n",
    "\n",
    "        # self.encoder = fastai.old.lm_rnn.RNN_Encoder(ntoken=new_vectors.shape[0], emb_sz=400, n_hid=1150, n_layers=1, pad_token=0,qrnn=False).to(self.device)\n",
    "        # self.encoder.load_state_dict(pretrained_weights)\n",
    "        # fastai.RNNTrainer\n",
    "        # self.encoder = com.NotSuchABetterEncoder(\n",
    "        #     number_of_layer=self.parameter_dict['number_of_layer'],\n",
    "        #     bidirectional=self.parameter_dict['bidirectional'],\n",
    "        #     embedding_dim=self.parameter_dict['embedding_dim'],\n",
    "        #     max_length = self.parameter_dict['max_length'],\n",
    "        #     hidden_dim=self.parameter_dict['hidden_size'],\n",
    "        #     vocab_size=self.parameter_dict['vocab_size'],\n",
    "        #     dropout=self.parameter_dict['dropout'],\n",
    "        #     vectors=self.parameter_dict['vectors'],\n",
    "        #     enable_layer_norm=False,\n",
    "        #     mode = 'LSTM',\n",
    "        #     debug = self.debug).to(self.device)\n",
    "\n",
    "#         self.encoder = RNN_Encoder(ntoken=self.parameter_dict['vectors'].shape[0], emb_sz=400,\n",
    "#                                                  n_hid=512, n_layers=1, pad_token=0, qrnn=False)\n",
    "  \n",
    "                                   #Merity\n",
    "        self.encoder = merity.RNNModel(rnn_type='LSTM', \n",
    "                ntoken=self.parameter_dict['vectors'].shape[0],\n",
    "                ninp=400,\n",
    "                nhid=1150,\n",
    "                nlayers=3,\n",
    "                dropout=0.1,\n",
    "                dropouth=0.5,\n",
    "                dropouti=0.5,\n",
    "                dropoute=0.5,\n",
    "                wdrop=0,\n",
    "                tie_weights=True)\n",
    "        self.encoder.encoder.padding_idx = 0\n",
    "        \n",
    "        \n",
    "        key_mapping = {\n",
    "            'encoder.weight' : 'encoder.weight',\n",
    "            'rnns.0.module.weight_ih_l0' : 'rnns.0.weight_ih_l0', \n",
    "            'rnns.0.module.bias_ih_l0' : 'rnns.0.bias_ih_l0', \n",
    "            'rnns.0.module.bias_hh_l0' : 'rnns.0.bias_hh_l0', \n",
    "            'rnns.0.module.weight_hh_l0_raw' : 'rnns.0.weight_hh_l0', \n",
    "            'rnns.1.module.weight_ih_l0' : 'rnns.1.weight_ih_l0', \n",
    "            'rnns.1.module.bias_ih_l0' : 'rnns.1.bias_ih_l0', \n",
    "            'rnns.1.module.bias_hh_l0': 'rnns.1.bias_hh_l0', \n",
    "            'rnns.1.module.weight_hh_l0_raw' : 'rnns.1.weight_hh_l0', \n",
    "            'rnns.2.module.weight_ih_l0' : 'rnns.2.weight_ih_l0', \n",
    "            'rnns.2.module.bias_ih_l0' : 'rnns.2.bias_ih_l0', \n",
    "            'rnns.2.module.bias_hh_l0': 'rnns.2.bias_hh_l0', \n",
    "            'rnns.2.module.weight_hh_l0_raw' : 'rnns.2.weight_hh_l0',\n",
    "        }\n",
    "        \n",
    "        \n",
    "        for k,v in key_mapping.items():\n",
    "            pretrained_weights[v] = pretrained_weights.pop(k)\n",
    "        \n",
    "        \n",
    "#         self.encoder.encoder = nn.Embedding.from_pretrained(torch.FloatTensor(self.parameter_dict['vectors']))\n",
    "        self.encoder = self.encoder.to(self.device)\n",
    "         \n",
    "        \n",
    "#         self.encoder = RNN_Encoder_Masks(vocab_sz=self.parameter_dict['vectors'].shape[0], emb_sz=400, weight_p=0,\n",
    "#                                                  n_hid=256, n_layers=1, pad_token=0, qrnn=False).to(device)\n",
    "        self.encoder.load_state_dict(pretrained_weights)\n",
    "#         self.encoder.encoder.weight = torch.tensor(self.parameter_dict['vectors'],device=self.device)\n",
    "#         self.encoder.encoder = nn.Embedding.from_pretrained(torch.FloatTensor(self.parameter_dict['vectors']))\n",
    "        \n",
    "#         self.classifier = PoolingLinearClassifier(layers=[400*3, 50, 300], drops=[0.4, 0.1]).to(device)\n",
    "#         self.classifier = self.classifier.to(device)\n",
    "        \n",
    "#         self.encoder = self.encoder.to(self.device)\n",
    "        \n",
    "        self.encoder.reset()\n",
    "    \n",
    "        # self.linear = torch.nn.Linear(1150,256).to(self.device)\n",
    "\n",
    "    def train(self, data, optimizer, loss_fn, device):\n",
    "        if self.pointwise:\n",
    "            return self._train_pointwise_(data, optimizer, loss_fn, device)\n",
    "        else:\n",
    "            return self._train_pairwise_(data, optimizer, loss_fn, device)\n",
    "\n",
    "    def _train_pointwise_(self, data, optimizer, loss_fn, device):\n",
    "        self.encoder.train()\n",
    "        self.classifier.train()        \n",
    "        optimizer.zero_grad()\n",
    "        self.encoder.reset_hidden()\n",
    "        \n",
    "        ques_batch, path_batch, y_label = data['ques_batch'], data['path_batch'], data['y_label']\n",
    "        self.encoder.reset_hidden()\n",
    "        op_q = self.encoder(ques_batch.transpose(1,0))\n",
    "        self.encoder.reset_hidden()\n",
    "        op_p = self.encoder(path_batch.transpose(1,0))\n",
    "        \n",
    "        ques_batch_encoded =  op_q\n",
    "        path_batch_encoded =  op_p\n",
    "        \n",
    "         #Calculating dot score\n",
    "        scores = torch.sum(ques_batch_encoded * path_batch_encoded, -1)\n",
    "\n",
    "        try:\n",
    "            loss = loss_fn(scores, y_label)\n",
    "        except RuntimeError:\n",
    "            traceback.print_exc()\n",
    "            print(pos_scores.shape, neg_scores.shape, y_label.shape,  ques_batch.shape, pos_batch.shape, neg_batch.shape)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss\n",
    "        \n",
    "        \n",
    "    def _train_pairwise_(self, data, optimizer, loss_fn, device):\n",
    "        '''\n",
    "            Given data, passes it through model, inited in constructor, returns loss and updates the weight\n",
    "            :params data: {batch of question, pos paths, neg paths and dummy y labels}\n",
    "            :params optimizer: torch.optim object\n",
    "            :params loss fn: torch.nn loss object\n",
    "            :params device: torch.device object\n",
    "\n",
    "            returns loss\n",
    "        '''\n",
    "        self.encoder.train()\n",
    "        # Unpacking the data and model from args\n",
    "        ques_batch, pos_batch, neg_batch, y_label = data['ques_batch'], data['pos_batch'], data['neg_batch'], data['y_label']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        #Encoding all the data\n",
    "\n",
    "#         print(f\"Before: \\t {ques_batch.shape[1]}, {pos_batch.shape[1]}, {neg_batch.shape[1]}\\n\"\n",
    "#         f\"After: \\t\\t {tu.trim(ques_batch).shape[1]}, {tu.trim(pos_batch).shape[1]}, {tu.trim(neg_batch).shape[1]}\")\n",
    "        \n",
    "        ques_batch = tu.trim(ques_batch)\n",
    "        pos_batch = tu.trim(pos_batch)\n",
    "        neg_batch = tu.trim(neg_batch)\n",
    "        \n",
    "#         print(pos_batch)\n",
    "#         print(neg_batch)\n",
    "\n",
    "        h = self.encoder.init_hidden(ques_batch.shape[0])\n",
    "        op_q = self.encoder(ques_batch.transpose(1,0), h)[1][-1][0].squeeze()\n",
    "        op_p = self.encoder(pos_batch.transpose(1,0), h)[1][-1][0].squeeze()\n",
    "        op_n = self.encoder(neg_batch.transpose(1,0), h)[1][-1][0].squeeze()\n",
    "                                   \n",
    "        \n",
    "#         self.encoder.reset_hidden()\n",
    "#         op_q = self.encoder(ques_batch.transpose(1,0))[-1][-1][-1]\n",
    "#         self.encoder.reset_hidden()\n",
    "#         op_p = self.encoder(pos_batch.transpose(1,0))[-1][-1][-1]\n",
    "#         self.encoder.reset_hidden()\n",
    "#         op_n = self.encoder(neg_batch.transpose(1,0))[-1][-1][-1]\n",
    "#         self.encoder.reset_hidden()\n",
    "#         op_q = self.encoder(ques_batch.transpose(1,0))[1][-1][0].squeeze()\n",
    "#         self.encoder.reset_hidden()\n",
    "#         op_p = self.encoder(pos_batch.transpose(1,0))[1][-1][0].squeeze()\n",
    "#         self.encoder.reset_hidden()\n",
    "#         op_n = self.encoder(neg_batch.transpose(1,0))[1][-1][0].squeeze()\n",
    "\n",
    "#         print(op_q.shape, op_p.shape, op_n.shape)\n",
    "#         raise IOError\n",
    "            \n",
    "        ques_batch_encoded =  op_q\n",
    "        pos_batch_encoded =  op_p\n",
    "        neg_batch_encoded =  op_n\n",
    "        \n",
    "#         ques_batch_encoded =  op_q[-1][0][1].squeeze()\n",
    "#         pos_batch_encoded =  op_p[-1][0][1].squeeze()\n",
    "#         neg_batch_encoded =  op_n[-1][0][1].squeeze()\n",
    "        \n",
    "#         print(ques_batch_encoded.shape, pos_batch_encoded.shape, neg_batch_encoded.shape)\n",
    "#         raise IOError\n",
    "        \n",
    "        #Calculating dot score\n",
    "        pos_scores = torch.sum(ques_batch_encoded * pos_batch_encoded, -1)\n",
    "        neg_scores = torch.sum(ques_batch_encoded * neg_batch_encoded, -1)\n",
    "#         print(pos_scores, neg_scores)\n",
    "        \n",
    "#         raise IOError\n",
    "        \n",
    "#         if True:\n",
    "#             print(\"ques_batch shape is \", ques_batch.shape)\n",
    "#             print(\"pos_batch shape is \", pos_batch.shape)\n",
    "#             print(\"neg_batch shape is \", neg_batch.shape)\n",
    "#             print(\"transposed ques bathc is \", ques_batch.transpose(1,0).shape)\n",
    "#             for o in op_p[1]:\n",
    "#                 print(\"o shape is \", o.shape)\n",
    "#             print(\"encoded pos batch shape is \", op_p[1][-1][-1].shape)\n",
    "            \n",
    "#             print(\"pos_score is\", pos_scores.shape)\n",
    "        '''\n",
    "            If `y == 1` then it assumed the first input should be ranked higher\n",
    "            (have a larger value) than the second input, and vice-versa for `y == -1`\n",
    "        '''\n",
    "#         raise ValueError\n",
    "        try:\n",
    "            loss = loss_fn(pos_scores, neg_scores, y_label)\n",
    "        except RuntimeError:\n",
    "            traceback.print_exc()\n",
    "            print(pos_scores.shape, neg_scores.shape, y_label.shape,  ques_batch.shape, pos_batch.shape, neg_batch.shape)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.encoder.parameters(), .5)\n",
    "        optimizer.step()\n",
    "        return loss\n",
    "\n",
    "    def predict(self, ques, paths, device):\n",
    "        \"\"\"\n",
    "            Same code works for both pairwise or pointwise\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "\n",
    "#             print(ques.shape, paths.shape, tu.trim(ques).shape,tu.trim(paths).shape)\n",
    "            \n",
    "#             if tu.trim(ques).shape[0] == 0:\n",
    "#                 print(ques)\n",
    "#                 print(paths)\n",
    "            \n",
    "            self.encoder.eval()\n",
    "#             h = self.encoder.reset()\n",
    "            # Encoding all the data\n",
    "            ques = tu.trim(ques)\n",
    "            paths = tu.trim(paths)\n",
    "    \n",
    "            \n",
    "            h  = self.encoder.init_hidden(ques.shape[0])\n",
    "            op_q = self.encoder(ques.transpose(1,0), h)\n",
    "            op_p = self.encoder(paths.transpose(1,0), h)\n",
    "#             self.encoder.reset_hidden()\n",
    "#             op_q = self.encoder(ques.transpose(1,0))\n",
    "#             self.encoder.reset_hidden()\n",
    "#             op_p = self.encoder(paths.transpose(1,0))\n",
    "\n",
    "#             question = op_q[1][0][-1]\n",
    "#             paths = op_p[1][0][-1]\n",
    "            question = op_q[1][-1][0].squeeze()\n",
    "            paths = op_p[1][-1][0].squeeze()\n",
    "        \n",
    "#             question = op_q[-1][-1][-1]\n",
    "#             paths = op_p[-1][-1][-1]\n",
    "        \n",
    "#             question = op_q[-1][0][1].squeeze()\n",
    "#             paths = op_p[-1][0][1].squeeze()\n",
    "\n",
    "            if self.pointwise:\n",
    "                # question = F.normalize(F.relu(question),p=1,dim=1)\n",
    "                # paths = F.normalize(F.relu(paths),p=1,dim=1)\n",
    "                # norm_ques_batch = torch.abs(torch.norm(question, dim=1, p=1))\n",
    "                # norm_pos_batch = torch.abs(torch.norm(paths, dim=1, p=1))\n",
    "                score = torch.sum(question * paths, -1)\n",
    "                # score = score.div(norm_ques_batch * norm_pos_batch).div_(2.0).add_(0.5)\n",
    "            else:\n",
    "                score = torch.sum(question * paths, -1)\n",
    "\n",
    "            self.encoder.train()\n",
    "            return score\n",
    "    \n",
    "    def prepare_save(self):\n",
    "        \"\"\"\n",
    "\n",
    "            This function is called when someone wants to save the underlying models.\n",
    "            Returns a tuple of key:model pairs which is to be interpreted within save model.\n",
    "\n",
    "        :return: [(key, model)]\n",
    "        \"\"\"\n",
    "        return [('encoder', self.encoder)]\n",
    "\n",
    "    def load_from(self, location):\n",
    "        # Pull the data from disk\n",
    "        if self.debug: print(\"loading Bilstmdot model from\", location)\n",
    "        self.encoder.load_state_dict(torch.load(location)['encoder'])\n",
    "        if self.debug: print(\"model loaded with weights ,\", self.get_parameter_sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0,
     80,
     159,
     169
    ]
   },
   "outputs": [],
   "source": [
    "class BiLstmDot(Model):\n",
    "\n",
    "    def __init__(self, _parameter_dict, _word_to_id, _device, _pointwise=False, _debug=False):\n",
    "\n",
    "        self.debug = _debug\n",
    "        self.parameter_dict = _parameter_dict\n",
    "        self.device = _device\n",
    "        self.pointwise = _pointwise\n",
    "        self.word_to_id = _word_to_id\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"Init Models\")\n",
    "\n",
    "\n",
    "        self.encoder = com.Encoder(self.parameter_dict['max_length'], self.parameter_dict['hidden_size'],\n",
    "                                   self.parameter_dict['number_of_layer'], self.parameter_dict['embedding_dim'],\n",
    "                                   self.parameter_dict['vocab_size'],\n",
    "                                   bidirectional=self.parameter_dict['bidirectional'],\n",
    "                                   vectors=self.parameter_dict['vectors']).to(self.device)            \n",
    "\n",
    "#         self.encoder = com.NotSuchABetterEncoder(\n",
    "#             number_of_layer=self.parameter_dict['number_of_layer'],\n",
    "#             bidirectional=self.parameter_dict['bidirectional'],\n",
    "#             embedding_dim=self.parameter_dict['embedding_dim'],\n",
    "#             max_length = self.parameter_dict['max_length'],\n",
    "#             hidden_dim=self.parameter_dict['hidden_size'],\n",
    "#             vocab_size=self.parameter_dict['vocab_size'],\n",
    "#             dropout=self.parameter_dict['dropout'],\n",
    "#             vectors=self.parameter_dict['vectors'],\n",
    "#             enable_layer_norm=False,\n",
    "#             mode = 'LSTM',\n",
    "#             debug = self.debug).to(self.device)\n",
    "\n",
    "    def train(self, data, optimizer, loss_fn, device):\n",
    "    #\n",
    "        if self.pointwise:\n",
    "            return self._train_pointwise_(data, optimizer, loss_fn, device)\n",
    "        else:\n",
    "            return self._train_pairwise_(data, optimizer, loss_fn, device)\n",
    "\n",
    "    def _train_pairwise_(self, data, optimizer, loss_fn, device):\n",
    "        '''\n",
    "            Given data, passes it through model, inited in constructor, returns loss and updates the weight\n",
    "            :params data: {batch of question, pos paths, neg paths and dummy y labels}\n",
    "            :params optimizer: torch.optim object\n",
    "            :params loss fn: torch.nn loss object\n",
    "            :params device: torch.device object\n",
    "\n",
    "            returns loss\n",
    "        '''\n",
    "\n",
    "        # Unpacking the data and model from args\n",
    "        ques_batch, pos_batch, neg_batch, y_label = data['ques_batch'], data['pos_batch'], data['neg_batch'], data['y_label']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        #Encoding all the data\n",
    "\n",
    "        hidden = self.encoder.init_hidden(ques_batch.shape[0],self.device)\n",
    "        ques_batch_encoded, _ = self.encoder(tu.trim(ques_batch), hidden)\n",
    "        pos_batch_encoded, _ = self.encoder(tu.trim(pos_batch), hidden)\n",
    "        neg_batch_encoded, _  = self.encoder(tu.trim(neg_batch), hidden)\n",
    "\n",
    "\n",
    "\n",
    "        #Calculating dot score\n",
    "        pos_scores = torch.sum(ques_batch_encoded[-1] * pos_batch_encoded[-1], -1)\n",
    "        neg_scores = torch.sum(ques_batch_encoded[-1] * neg_batch_encoded[-1], -1)\n",
    "        '''\n",
    "            If `y == 1` then it assumed the first input should be ranked higher\n",
    "            (have a larger value) than the second input, and vice-versa for `y == -1`\n",
    "        '''\n",
    "        try:\n",
    "            loss = loss_fn(pos_scores, neg_scores, y_label)\n",
    "        except RuntimeError:\n",
    "            print(pos_scores.shape, neg_scores.shape, y_label.shape,  ques_batch.shape, pos_batch.shape, neg_batch.shape)\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(self.encoder.parameters(), .5)\n",
    "        optimizer.step()\n",
    "        return loss\n",
    "\n",
    "    def _train_pointwise_(self, data, optimizer, loss_fn, device):\n",
    "        '''\n",
    "            Given data, passes it through model, inited in constructor, returns loss and updates the weight\n",
    "            :params data: {batch of question, paths and y labels}\n",
    "            :params models list of [models]\n",
    "            :params optimizer: torch.optim object\n",
    "            :params loss fn: torch.nn loss object\n",
    "            :params device: torch.device object\n",
    "            returrns loss\n",
    "        '''\n",
    "        # Unpacking the data and model from args\n",
    "        ques_batch, path_batch, y_label = data['ques_batch'], data['path_batch'], data['y_label']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Encoding all the data\n",
    "        hidden = self.encoder.init_hidden(ques_batch.shape[0], self.device)\n",
    "        _, ques_batch, _, _ = self.encoder(tu.trim(ques_batch), hidden)\n",
    "        _, pos_batch, _, _ = self.encoder(tu.trim(path_batch), hidden)\n",
    "\n",
    "        #\n",
    "        # norm_ques_batch = torch.abs(torch.norm(ques_batch,dim=1,p=1))\n",
    "        # norm_pos_batch = torch.abs(torch.norm(pos_batch,dim=1,p=1))\n",
    "\n",
    "        # ques_batch = F.normalize(F.relu(ques_batch),p=1,dim=1)\n",
    "        # pos_batch = F.normalize(F.relu(pos_batch),p=1,dim=1)\n",
    "        # ques_batch =(F.normalize(ques_batch,p=1,dim=1)/2) + .5\n",
    "        # pos_batch =(F.normalize(pos_batch,p=1,dim=1)/2) + .5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Calculating dot score\n",
    "        score = torch.sum(ques_batch * pos_batch, -1)\n",
    "        # score = score.div(norm_ques_batch*norm_pos_batch).div_(2.0).add_(0.5)\n",
    "            # print(\"shape of score is,\", score.shape)\n",
    "            # print(\"score is , \", score)\n",
    "            #\n",
    "            #\n",
    "            # print(\"shape of y label is \", y_label.shape)\n",
    "            # print(\"value of y label is \", y_label)\n",
    "\n",
    "        # raise ValueError\n",
    "\n",
    "        '''\n",
    "            Binary Cross Entropy loss function. @TODO: Check if we can give it 1/0 labels.\n",
    "        '''\n",
    "        loss = loss_fn(score, y_label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.encoder.parameters(), .5)\n",
    "        optimizer.step()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def predict(self, ques, paths, device):\n",
    "        \"\"\"\n",
    "            Same code works for both pairwise or pointwise\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "\n",
    "            self.encoder.eval()\n",
    "            hidden = self.encoder.init_hidden(ques.shape[0], self.device)\n",
    "\n",
    "            question, _ = self.encoder(tu.trim(ques.long()), hidden)\n",
    "            paths, _ = self.encoder(tu.trim(paths.long()), hidden)\n",
    "\n",
    "            if self.pointwise:\n",
    "                # question = F.normalize(F.relu(question),p=1,dim=1)\n",
    "                # paths = F.normalize(F.relu(paths),p=1,dim=1)\n",
    "                # norm_ques_batch = torch.abs(torch.norm(question, dim=1, p=1))\n",
    "                # norm_pos_batch = torch.abs(torch.norm(paths, dim=1, p=1))\n",
    "                score = torch.sum(question[-1] * paths[-1], -1)\n",
    "                # score = score.div(norm_ques_batch * norm_pos_batch).div_(2.0).add_(0.5)\n",
    "            else:\n",
    "                score = torch.sum(question[-1] * paths[-1], -1)\n",
    "\n",
    "            self.encoder.train()\n",
    "            return score\n",
    "\n",
    "    def prepare_save(self):\n",
    "        \"\"\"\n",
    "\n",
    "            This function is called when someone wants to save the underlying models.\n",
    "            Returns a tuple of key:model pairs which is to be interpreted within save model.\n",
    "\n",
    "        :return: [(key, model)]\n",
    "        \"\"\"\n",
    "        return [('encoder', self.encoder)]\n",
    "\n",
    "    def load_from(self, location):\n",
    "        # Pull the data from disk\n",
    "        if self.debug: print(\"loading Bilstmdot model from\", location)\n",
    "        self.encoder.load_state_dict(torch.load(location)['encoder'])\n",
    "        if self.debug: print(\"model loaded with weights ,\", self.get_parameter_sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_dict['vectors'] = data['vectors']\n",
    "parameter_dict['schema'] = schema\n",
    "\n",
    "parameter_dict['bidirectional'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_dict['batch_size'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LSTM(400, 1150), LSTM(1150, 1150), LSTM(1150, 400)]\n"
     ]
    }
   ],
   "source": [
    "modeler = BiLstmDot_ulmfit(_parameter_dict = parameter_dict,\n",
    "                    _word_to_id=_word_to_id,\n",
    "                    _device=device,\n",
    "                    _pointwise=pointwise,\n",
    "                    _debug=False)\n",
    "\n",
    "optimizer = optim.Adam(list(filter(lambda p: p.requires_grad, modeler.encoder.parameters())))\n",
    "#                       list(filter(lambda p: p.requires_grad, modeler.classifier.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def training_loop(training_model, parameter_dict,modeler,train_loader,\n",
    "                  optimizer,loss_func, data, dataset, device, test_every, validate_every , pointwise = False, problem='core_chain',curtail_padding_rel=True):\n",
    "\n",
    "    model_save_location = aux.save_location(problem, training_model, dataset)\n",
    "    aux_save_information = {\n",
    "        'epoch' : 0,\n",
    "        'test_accuracy':0.0,\n",
    "        'validation_accura\"\"cy':0.0,\n",
    "        'parameter_dict':parameter_dict\n",
    "    }\n",
    "    train_loss = []\n",
    "    valid_accuracy = []\n",
    "    test_accuracy = []\n",
    "    best_validation_accuracy = 0\n",
    "    best_test_accuracy = 0\n",
    "\n",
    "    if parameter_dict['schema'] == 'reldet':\n",
    "        parameter_dict['rel1_pad'] =  parameter_dict['relrd_pad']\n",
    "    elif parameter_dict['schema'] == 'slotptr':\n",
    "        parameter_dict['rel1_pad'] = parameter_dict['relsp_pad']\n",
    "\n",
    "        ###############\n",
    "    # Training Loop\n",
    "    ###############\n",
    "\n",
    "\n",
    "    #Makes test data of appropriate shape\n",
    "    print(\"the dataset is \", dataset)\n",
    "    if curtail_padding_rel and dataset == 'lcquad':\n",
    "        data = cc.curatail_padding(data, parameter_dict)\n",
    "        data['valid_neg_paths'] = np.zeros_like(data['valid_neg_paths'])\n",
    "        data['valid_pos_paths'] = np.zeros_like(data['valid_pos_paths'])\n",
    "        data['valid_questions'] = np.zeros_like(data['valid_questions'])\n",
    "\n",
    "    try:\n",
    "\n",
    "        for epoch in range(parameter_dict['epochs']):\n",
    "\n",
    "            # Epoch start print\n",
    "            print(\"Epoch: \", epoch, \"/\", parameter_dict['epochs'])\n",
    "\n",
    "            # Bookkeeping variables\n",
    "            epoch_loss = []\n",
    "            epoch_time = time.time()\n",
    "\n",
    "            # Loop for one batch\n",
    "            # tqdm_loop = tqdm(enumerate(train_loader))\n",
    "            for i_batch, sample_batched in enumerate(train_loader):\n",
    "\n",
    "                # Bookkeeping and data preparation\n",
    "                batch_time = time.time()\n",
    "\n",
    "                if not pointwise:\n",
    "                    ques_batch = torch.tensor(np.reshape(sample_batched[0][0], (-1, parameter_dict['max_length'])),\n",
    "                                              dtype=torch.long, device=device)\n",
    "                    pos_batch = torch.tensor(np.reshape(sample_batched[0][1], (-1, parameter_dict['max_length'])),\n",
    "                                             dtype=torch.long, device=device)\n",
    "                    neg_batch = torch.tensor(np.reshape(sample_batched[0][2], (-1, parameter_dict['max_length'])),\n",
    "                                             dtype=torch.long, device=device)\n",
    "\n",
    "                    data['dummy_y'] = torch.ones(ques_batch.shape[0], device=device)\n",
    "                    if parameter_dict['schema'] != 'default':\n",
    "                        pos_rel1_batch = torch.tensor(np.reshape(sample_batched[0][3], (-1, parameter_dict['max_length'])),\n",
    "                                                      dtype=torch.long, device=device)\n",
    "                        pos_rel2_batch = torch.tensor(np.reshape(sample_batched[0][4], (-1, parameter_dict['max_length'])),\n",
    "                                                      dtype=torch.long, device=device)\n",
    "                        neg_rel1_batch = torch.tensor(np.reshape(sample_batched[0][5], (-1, parameter_dict['max_length'])),\n",
    "                                                      dtype=torch.long, device=device)\n",
    "                        neg_rel2_batch = torch.tensor(np.reshape(sample_batched[0][6], (-1, parameter_dict['max_length'])),\n",
    "                                                      dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "                        data_batch = {\n",
    "                            'ques_batch': ques_batch,\n",
    "                            'pos_batch': pos_batch[:,:parameter_dict['rel_pad']],\n",
    "                            'neg_batch': neg_batch[:,:parameter_dict['rel_pad']],\n",
    "                            'y_label': data['dummy_y'],\n",
    "                            'pos_rel1_batch': pos_rel1_batch[:,:parameter_dict['rel1_pad']],\n",
    "                            'pos_rel2_batch':pos_rel2_batch[:,:parameter_dict['rel1_pad']],\n",
    "                            'neg_rel1_batch':neg_rel1_batch[:,:parameter_dict['rel1_pad']],\n",
    "                            'neg_rel2_batch' : neg_rel2_batch[:,:parameter_dict['rel1_pad']]\n",
    "                        }\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        data_batch = {\n",
    "                            'ques_batch': ques_batch,\n",
    "                            'pos_batch': pos_batch[:,:parameter_dict['rel_pad']],\n",
    "                            'neg_batch': neg_batch[:,:parameter_dict['rel_pad']],\n",
    "                            'y_label': data['dummy_y']}\n",
    "\n",
    "                else:\n",
    "                    ques_batch = torch.tensor(np.reshape(sample_batched[0][0], (-1, parameter_dict['max_length'])),\n",
    "                                              dtype=torch.long, device=device)\n",
    "                    path_batch = torch.tensor(np.reshape(sample_batched[0][1], (-1, parameter_dict['max_length'])),\n",
    "                                             dtype=torch.long, device=device)\n",
    "                    y = torch.tensor(sample_batched[1],dtype = torch.float,device=device).view(-1)\n",
    "\n",
    "\n",
    "                    if parameter_dict['schema'] != 'default':\n",
    "                        path_rel1_batch = torch.tensor(np.reshape(sample_batched[0][2], (-1, parameter_dict['max_length'])),\n",
    "                                                      dtype=torch.long, device=device)\n",
    "                        path_rel2_batch = torch.tensor(np.reshape(sample_batched[0][3], (-1, parameter_dict['max_length'])),\n",
    "                                                      dtype=torch.long, device=device)\n",
    "\n",
    "                        data_batch = {\n",
    "                            'ques_batch': ques_batch,\n",
    "                            'path_batch': path_batch[:,:parameter_dict['rel_pad']],\n",
    "                            'y_label': y,\n",
    "                            'path_rel1_batch': path_rel1_batch[:,:parameter_dict['rel1_pad']],\n",
    "                            'path_rel2_batch': path_rel2_batch[:,:parameter_dict['rel1_pad']]\n",
    "                        }\n",
    "                    else:\n",
    "                        data_batch = {\n",
    "                            'ques_batch': ques_batch,\n",
    "                            'path_batch': path_batch[:,:parameter_dict['rel_pad']],\n",
    "                            'y_label': y\n",
    "                        }\n",
    "\n",
    "\n",
    "                return data_batch\n",
    "                loss = modeler.train(data=data_batch,\n",
    "                                  optimizer=optimizer,\n",
    "                                  loss_fn=loss_func,\n",
    "                                  device=device)\n",
    "\n",
    "                # Bookkeep the training loss\n",
    "                epoch_loss.append(loss.item())\n",
    "\n",
    "                # tqdm_loop.desc(\"#\"+str(i_batch)+\"\\tLoss:\" + str(loss.item())[:min(5, len(str(loss.item())))])\n",
    "\n",
    "                print(\"Batch:\\t%d\" % i_batch, \"/%d\\t: \" % (parameter_dict['batch_size']),\n",
    "                      \"%s\" % (time.time() - batch_time),\n",
    "                      \"\\t%s\" % (time.time() - epoch_time),\n",
    "                      \"\\t%s\" % (str(loss.item())),\n",
    "                      end=None if i_batch + 1 == int(int(i_batch) / parameter_dict['batch_size']) else \"\\n\")\n",
    "\n",
    "            # EPOCH LEVEL\n",
    "\n",
    "            # Track training loss\n",
    "            train_loss.append(sum(epoch_loss))\n",
    "\n",
    "            # test_every = False\n",
    "            if test_every:\n",
    "                # Run on test set\n",
    "                if epoch%test_every == 0:\n",
    "                    if parameter_dict['schema'] != 'default':\n",
    "                        if parameter_dict['schema']  == 'slotptr':\n",
    "                            test_accuracy.append(aux.validation_accuracy(data['test_questions'], data['test_pos_paths'],\n",
    "                                                             data['test_neg_paths'],modeler, device,data['test_pos_paths_rel1_sp'],data['test_pos_paths_rel2_sp'],\n",
    "                                                                 data['test_neg_paths_rel1_sp'],data['test_neg_paths_rel2_sp']))\n",
    "                        else:\n",
    "                            test_accuracy.append(aux.validation_accuracy(data['test_questions'], data['test_pos_paths'],\n",
    "                                                                         data['test_neg_paths'], modeler, device,\n",
    "                                                                         data['test_pos_paths_rel1_rd'],\n",
    "                                                                         data['test_pos_paths_rel2_rd'],\n",
    "                                                                         data['test_neg_paths_rel1_rd'],\n",
    "                                                                         data['test_neg_paths_rel2_rd']))\n",
    "                    else:\n",
    "                        test_accuracy.append(aux.validation_accuracy(data['test_questions'], data['test_pos_paths'],\n",
    "                                                                     data['test_neg_paths'], modeler, device))\n",
    "                    if test_accuracy[-1] >= best_test_accuracy:\n",
    "                        best_test_accuracy = test_accuracy[-1]\n",
    "                        aux_save_information['test_accuracy'] = best_test_accuracy\n",
    "            else:\n",
    "                test_accuracy.append(0)\n",
    "                best_test_accuracy = 0\n",
    "\n",
    "            # Run on validation set\n",
    "            if validate_every:\n",
    "                if epoch%validate_every == 0:\n",
    "                    if parameter_dict['schema'] != 'default':\n",
    "                        if parameter_dict['schema'] == 'slotptr':\n",
    "                            valid_accuracy.append(aux.validation_accuracy(data['valid_questions'], data['valid_pos_paths'],\n",
    "                                                              data['valid_neg_paths'],  modeler, device, data['valid_pos_paths_rel1_sp'],data['valid_pos_paths_rel2_sp'],\n",
    "                                                                 data['valid_neg_paths_rel1_sp'],data['valid_neg_paths_rel2_sp']))\n",
    "                        else:\n",
    "                            valid_accuracy.append(aux.validation_accuracy(data['valid_questions'][:-1], data['valid_pos_paths'][:-1],\n",
    "                                                                          data['valid_neg_paths'][:-1], modeler, device,\n",
    "                                                                          data['valid_pos_paths_rel1_rd'][:-1],\n",
    "                                                                          data['valid_pos_paths_rel2_rd'][:-1],\n",
    "                                                                          data['valid_neg_paths_rel1_rd'][:-1],\n",
    "                                                                          data['valid_neg_paths_rel2_rd'][:-1]))\n",
    "                    else:\n",
    "                        valid_accuracy.append(aux.validation_accuracy(data['valid_questions'], data['valid_pos_paths'],\n",
    "                                                                      data['valid_neg_paths'], modeler, device))\n",
    "                    if valid_accuracy[-1] > best_validation_accuracy:\n",
    "                        print(\"MODEL WEIGHTS RIGHT NOW: \", modeler.get_parameter_sum())\n",
    "                        best_validation_accuracy = valid_accuracy[-1]\n",
    "                        aux_save_information['epoch'] = epoch\n",
    "                        aux_save_information['validation_accuracy'] = best_validation_accuracy\n",
    "                        aux.save_model(model_save_location, modeler, model_name='model.torch'\n",
    "                                   , epochs=epoch, optimizer=optimizer, accuracy=best_validation_accuracy, aux_save_information=aux_save_information)\n",
    "\n",
    "            # Resample new negative paths per epoch and shuffle all data\n",
    "            train_loader.dataset.shuffle()\n",
    "\n",
    "            # Epoch level prints\n",
    "            print(\"Time: %s\\t\" % (time.time() - epoch_time),\n",
    "                  \"Loss: %s\\t\" % (sum(epoch_loss)),\n",
    "                  \"Valdacc: %s\\t\" % (valid_accuracy[-1]),\n",
    "                    \"Testacc: %s\\n\" % (test_accuracy[-1]),\n",
    "                  \"BestValidAcc: %s\\n\" % (best_validation_accuracy),\n",
    "                  \"BestTestAcc: %s\\n\" % (best_test_accuracy))\n",
    "\n",
    "        return train_loss, modeler, valid_accuracy, test_accuracy\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('-' * 89)\n",
    "        return train_loss, modeler, valid_accuracy, test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/priyansh/conda/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dataset is  lcquad\n",
      "Epoch:  0 / 200\n",
      "Batch:\t0 /100\t:  0.30010533332824707 \t0.3031270503997803 \t486.80352783203125\n",
      "Batch:\t1 /100\t:  0.2783393859863281 \t0.5825595855712891 \t442.3117980957031\n",
      "Batch:\t2 /100\t:  0.2769312858581543 \t0.8614952564239502 \t477.81134033203125\n",
      "Batch:\t3 /100\t:  0.28230834007263184 \t1.144834041595459 \t444.215576171875\n",
      "Batch:\t4 /100\t:  0.28002238273620605 \t1.426368236541748 \t389.7696228027344\n",
      "Batch:\t5 /100\t:  0.2731955051422119 \t1.701167106628418 \t405.7113342285156\n",
      "Batch:\t6 /100\t:  0.276566743850708 \t1.978806495666504 \t372.45196533203125\n",
      "Batch:\t7 /100\t:  0.26964521408081055 \t2.249575138092041 \t317.3166198730469\n",
      "Batch:\t8 /100\t:  0.28888583183288574 \t2.540247917175293 \t320.3661193847656\n",
      "Batch:\t9 /100\t:  0.2774317264556885 \t2.818896532058716 \t386.9849853515625\n",
      "Batch:\t10 /100\t:  0.2817087173461914 \t3.101989507675171 \t279.6360778808594\n",
      "Batch:\t11 /100\t:  0.28110694885253906 \t3.384559154510498 \t366.0398254394531\n",
      "Batch:\t12 /100\t:  0.2835969924926758 \t3.6697349548339844 \t558.9443969726562\n",
      "Batch:\t13 /100\t:  0.2856261730194092 \t3.9566051959991455 \t323.32232666015625\n",
      "Batch:\t14 /100\t:  0.2862887382507324 \t4.245025634765625 \t344.1977844238281\n",
      "Batch:\t15 /100\t:  0.28492021560668945 \t4.531382322311401 \t185.71527099609375\n",
      "Batch:\t16 /100\t:  0.2882814407348633 \t4.821538686752319 \t344.2008972167969\n",
      "Batch:\t17 /100\t:  0.2821488380432129 \t5.1049041748046875 \t225.6701202392578\n",
      "Batch:\t18 /100\t:  0.2823832035064697 \t5.3883140087127686 \t738.3311157226562\n",
      "Batch:\t19 /100\t:  0.334197998046875 \t5.723919630050659 \t247.96823120117188\n",
      "Batch:\t20 /100\t:  0.28238463401794434 \t6.007531404495239 \t89.98472595214844\n",
      "Batch:\t21 /100\t:  0.2831897735595703 \t6.291919231414795 \t669.20654296875\n",
      "Batch:\t22 /100\t:  0.28494930267333984 \t6.578412055969238 \t90.34086608886719\n",
      "Batch:\t23 /100\t:  0.2808349132537842 \t6.860440492630005 \t109.24283599853516\n",
      "Batch:\t24 /100\t:  0.27795886993408203 \t7.139586448669434 \t355.6579895019531\n",
      "Batch:\t25 /100\t:  0.2841989994049072 \t7.4252400398254395 \t674.7059326171875\n",
      "Batch:\t26 /100\t:  0.28664660453796387 \t7.7130982875823975 \t540.0828247070312\n",
      "Batch:\t27 /100\t:  0.2735450267791748 \t7.987667798995972 \t40.91305923461914\n",
      "Batch:\t28 /100\t:  0.2837543487548828 \t8.27274775505066 \t743.0078735351562\n",
      "Batch:\t29 /100\t:  0.28615617752075195 \t8.560663938522339 \t182.2323760986328\n",
      "Batch:\t30 /100\t:  0.2770822048187256 \t8.838951587677002 \t433.25665283203125\n",
      "Batch:\t31 /100\t:  0.28185224533081055 \t9.122259378433228 \t321.52032470703125\n",
      "Batch:\t32 /100\t:  0.2797839641571045 \t9.403168201446533 \t252.54298400878906\n",
      "Batch:\t33 /100\t:  0.2841484546661377 \t9.688864469528198 \t225.2376708984375\n",
      "Batch:\t34 /100\t:  0.2844991683959961 \t9.974601984024048 \t422.6580810546875\n",
      "Batch:\t35 /100\t:  0.2845447063446045 \t10.260445356369019 \t281.33837890625\n",
      "Batch:\t36 /100\t:  0.2826974391937256 \t10.544212818145752 \t176.79074096679688\n",
      "Batch:\t37 /100\t:  0.2852761745452881 \t10.830952644348145 \t203.78317260742188\n",
      "Batch:\t38 /100\t:  0.2797684669494629 \t11.112212657928467 \t288.84716796875\n",
      "Batch:\t39 /100\t:  0.28723669052124023 \t11.400914907455444 \t134.71900939941406\n",
      "Batch:\t40 /100\t:  0.2747204303741455 \t11.676638126373291 \t176.88006591796875\n",
      "Batch:\t41 /100\t:  0.2872622013092041 \t11.965480327606201 \t95.12909698486328\n",
      "Batch:\t42 /100\t:  0.2801032066345215 \t12.246582746505737 \t675.3119506835938\n",
      "Batch:\t43 /100\t:  0.28168725967407227 \t12.529612302780151 \t464.9129333496094\n",
      "Batch:\t44 /100\t:  0.2818741798400879 \t12.8124520778656 \t400.4315490722656\n",
      "Batch:\t45 /100\t:  0.28386807441711426 \t13.097766876220703 \t818.0223999023438\n",
      "Batch:\t46 /100\t:  0.2803318500518799 \t13.379602670669556 \t136.94447326660156\n",
      "Batch:\t47 /100\t:  0.282681941986084 \t13.66363787651062 \t568.2667846679688\n",
      "Batch:\t48 /100\t:  0.2796659469604492 \t13.944573879241943 \t176.47276306152344\n",
      "Batch:\t49 /100\t:  0.2799816131591797 \t14.226538896560669 \t477.71807861328125\n",
      "Batch:\t50 /100\t:  0.2846193313598633 \t14.512659788131714 \t212.7257537841797\n",
      "Batch:\t51 /100\t:  0.28594136238098145 \t14.799830913543701 \t180.5163116455078\n",
      "Batch:\t52 /100\t:  0.28170275688171387 \t15.082916259765625 \t352.3385009765625\n",
      "Batch:\t53 /100\t:  0.28710460662841797 \t15.3715078830719 \t249.09539794921875\n",
      "Batch:\t54 /100\t:  0.2844429016113281 \t15.65787386894226 \t241.9356231689453\n",
      "Batch:\t55 /100\t:  0.2825343608856201 \t15.941480875015259 \t299.8350524902344\n",
      "Batch:\t56 /100\t:  0.2786071300506592 \t16.221193552017212 \t280.1885681152344\n",
      "Batch:\t57 /100\t:  0.2813754081726074 \t16.503761768341064 \t273.6192626953125\n",
      "Batch:\t58 /100\t:  0.2767171859741211 \t16.78196382522583 \t226.5995635986328\n",
      "Batch:\t59 /100\t:  0.28051042556762695 \t17.063469171524048 \t213.3406982421875\n",
      "Batch:\t60 /100\t:  0.2809576988220215 \t17.345893621444702 \t238.1378173828125\n",
      "Batch:\t61 /100\t:  0.28470706939697266 \t17.631619930267334 \t251.82484436035156\n",
      "Batch:\t62 /100\t:  0.27745676040649414 \t17.910282373428345 \t228.295166015625\n",
      "Batch:\t63 /100\t:  0.2852461338043213 \t18.19678258895874 \t191.267333984375\n",
      "Batch:\t64 /100\t:  0.2865297794342041 \t18.48500633239746 \t234.45352172851562\n",
      "Batch:\t65 /100\t:  0.27770161628723145 \t18.763933897018433 \t242.45004272460938\n",
      "Batch:\t66 /100\t:  0.27981996536254883 \t19.04518961906433 \t196.17742919921875\n",
      "Batch:\t67 /100\t:  0.2813398838043213 \t19.327864170074463 \t258.3844299316406\n",
      "Batch:\t68 /100\t:  0.28159213066101074 \t19.610567092895508 \t218.89376831054688\n",
      "Batch:\t69 /100\t:  0.28188157081604004 \t19.894503355026245 \t203.31080627441406\n",
      "Batch:\t70 /100\t:  0.27988147735595703 \t20.17563509941101 \t240.83694458007812\n",
      "Batch:\t71 /100\t:  0.27625370025634766 \t20.45301389694214 \t159.36668395996094\n",
      "Batch:\t72 /100\t:  0.29361820220947266 \t20.74811816215515 \t170.85276794433594\n",
      "Batch:\t73 /100\t:  0.3090839385986328 \t21.058637142181396 \t354.97796630859375\n",
      "Batch:\t74 /100\t:  0.2913784980773926 \t21.35137438774109 \t135.262451171875\n",
      "Batch:\t75 /100\t:  0.290102481842041 \t21.642733335494995 \t230.3614044189453\n",
      "Batch:\t76 /100\t:  0.2878117561340332 \t21.931949615478516 \t368.4878845214844\n",
      "Batch:\t77 /100\t:  0.2910945415496826 \t22.22949457168579 \t104.81818389892578\n",
      "Batch:\t78 /100\t:  0.28450846672058105 \t22.515366315841675 \t136.0101318359375\n",
      "Batch:\t79 /100\t:  0.28345322608947754 \t22.801012992858887 \t144.15072631835938\n",
      "Batch:\t80 /100\t:  0.28325533866882324 \t23.0857093334198 \t213.85220336914062\n",
      "Batch:\t81 /100\t:  0.2848849296569824 \t23.37199640274048 \t320.67950439453125\n",
      "Batch:\t82 /100\t:  0.28177785873413086 \t23.655208587646484 \t388.7486572265625\n",
      "Batch:\t83 /100\t:  0.2838106155395508 \t23.94054651260376 \t78.01940155029297\n",
      "Batch:\t84 /100\t:  0.28578758239746094 \t24.227742433547974 \t175.85450744628906\n",
      "Batch:\t85 /100\t:  0.2761063575744629 \t24.505099058151245 \t301.1263732910156\n",
      "Batch:\t86 /100\t:  0.27858734130859375 \t24.784758806228638 \t264.199462890625\n",
      "Batch:\t87 /100\t:  0.2771632671356201 \t25.06345844268799 \t62.8262939453125\n",
      "Batch:\t88 /100\t:  0.28402137756347656 \t25.348453760147095 \t264.7337646484375\n",
      "Batch:\t89 /100\t:  0.2778468132019043 \t25.627929210662842 \t83.3089599609375\n",
      "Batch:\t90 /100\t:  0.2773287296295166 \t25.906283617019653 \t321.3504638671875\n",
      "Batch:\t91 /100\t:  0.2860105037689209 \t26.193439483642578 \t244.59185791015625\n",
      "Batch:\t92 /100\t:  0.28220677375793457 \t26.476837158203125 \t230.96653747558594\n",
      "Batch:\t93 /100\t:  0.2816307544708252 \t26.75989317893982 \t163.84539794921875\n",
      "Batch:\t94 /100\t:  0.279888391494751 \t27.040778636932373 \t308.8375549316406\n",
      "Batch:\t95 /100\t:  0.2826383113861084 \t27.324689388275146 \t241.7313995361328\n",
      "Batch:\t96 /100\t:  0.2810811996459961 \t27.60696816444397 \t97.10310363769531\n",
      "Batch:\t97 /100\t:  0.27869415283203125 \t27.886725425720215 \t266.595458984375\n",
      "Batch:\t98 /100\t:  0.2890441417694092 \t28.177061080932617 \t201.69244384765625\n",
      "Batch:\t99 /100\t:  0.2757110595703125 \t28.454190969467163 \t344.2663879394531\n",
      "Batch:\t100 /100\t:  0.29663991928100586 \t28.752554893493652 \t186.87777709960938\n",
      "Batch:\t101 /100\t:  0.2829744815826416 \t29.037073850631714 \t148.8376007080078\n",
      "Batch:\t102 /100\t:  0.28334832191467285 \t29.321561336517334 \t121.76007843017578\n",
      "Batch:\t103 /100\t:  0.290621280670166 \t29.613296270370483 \t144.04380798339844\n",
      "Batch:\t104 /100\t:  0.2953486442565918 \t29.9099760055542 \t180.489501953125\n",
      "Batch:\t105 /100\t:  0.28019213676452637 \t30.191608905792236 \t117.84795379638672\n",
      "Batch:\t106 /100\t:  0.2852356433868408 \t30.478963375091553 \t180.33876037597656\n",
      "Batch:\t107 /100\t:  0.2768900394439697 \t30.757155418395996 \t193.45718383789062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t108 /100\t:  0.2802159786224365 \t31.038423776626587 \t238.60226440429688\n",
      "Batch:\t109 /100\t:  0.28276562690734863 \t31.322746515274048 \t205.26211547851562\n",
      "Batch:\t110 /100\t:  0.2862098217010498 \t31.610270261764526 \t346.9374084472656\n",
      "Batch:\t111 /100\t:  0.28723835945129395 \t31.898514986038208 \t286.8647766113281\n",
      "Batch:\t112 /100\t:  0.28548479080200195 \t32.1851441860199 \t209.57655334472656\n",
      "Batch:\t113 /100\t:  0.28124547004699707 \t32.46798491477966 \t240.33363342285156\n",
      "Batch:\t114 /100\t:  0.27350568771362305 \t32.74300479888916 \t220.9574737548828\n",
      "Batch:\t115 /100\t:  0.28095436096191406 \t33.02495861053467 \t153.85643005371094\n",
      "Batch:\t116 /100\t:  0.2843177318572998 \t33.310877084732056 \t273.86102294921875\n",
      "Batch:\t117 /100\t:  0.283980131149292 \t33.595834255218506 \t159.44247436523438\n",
      "Batch:\t118 /100\t:  0.28135108947753906 \t33.878387212753296 \t226.38348388671875\n",
      "Batch:\t119 /100\t:  0.2854924201965332 \t34.16508722305298 \t215.88113403320312\n",
      "Batch:\t120 /100\t:  0.28433656692504883 \t34.45069479942322 \t339.97528076171875\n",
      "Batch:\t121 /100\t:  0.28000330924987793 \t34.732152700424194 \t238.97193908691406\n",
      "Batch:\t122 /100\t:  0.28798604011535645 \t35.02136421203613 \t175.9970245361328\n",
      "Batch:\t123 /100\t:  0.28182363510131836 \t35.30468392372131 \t160.78472900390625\n",
      "Batch:\t124 /100\t:  0.279738187789917 \t35.585811614990234 \t202.35963439941406\n",
      "Batch:\t125 /100\t:  0.28516197204589844 \t35.872039794921875 \t178.63421630859375\n",
      "Batch:\t126 /100\t:  0.278322696685791 \t36.151591539382935 \t169.13966369628906\n",
      "Batch:\t127 /100\t:  0.27453017234802246 \t36.42719030380249 \t195.5484619140625\n",
      "Batch:\t128 /100\t:  0.294475793838501 \t36.72324872016907 \t204.76202392578125\n",
      "Batch:\t129 /100\t:  0.2825634479522705 \t37.006723165512085 \t203.16163635253906\n",
      "Batch:\t130 /100\t:  0.2823972702026367 \t37.290292739868164 \t147.26133728027344\n",
      "Batch:\t131 /100\t:  0.2769613265991211 \t37.56831669807434 \t184.9810333251953\n",
      "Batch:\t132 /100\t:  0.2792658805847168 \t37.84922909736633 \t174.52957153320312\n",
      "Batch:\t133 /100\t:  0.2836301326751709 \t38.13397192955017 \t186.50218200683594\n",
      "Batch:\t134 /100\t:  0.2791116237640381 \t38.414143085479736 \t176.16995239257812\n",
      "Batch:\t135 /100\t:  0.28026843070983887 \t38.695629835128784 \t172.2306671142578\n",
      "Batch:\t136 /100\t:  0.2825922966003418 \t38.979407787323 \t242.58592224121094\n",
      "Batch:\t137 /100\t:  0.2718498706817627 \t39.25330400466919 \t170.5523223876953\n",
      "Batch:\t138 /100\t:  0.2839365005493164 \t39.538634061813354 \t189.0562286376953\n",
      "Batch:\t139 /100\t:  0.2838478088378906 \t39.823899030685425 \t182.3660430908203\n",
      "Batch:\t140 /100\t:  0.2790820598602295 \t40.10407638549805 \t235.2321014404297\n",
      "Batch:\t141 /100\t:  0.28225183486938477 \t40.38782453536987 \t215.4532012939453\n",
      "Batch:\t142 /100\t:  0.28148913383483887 \t40.67034935951233 \t177.33982849121094\n",
      "Batch:\t143 /100\t:  0.281480073928833 \t40.95471405982971 \t171.6213836669922\n",
      "Batch:\t144 /100\t:  0.27997875213623047 \t41.235729455947876 \t195.07891845703125\n",
      "Batch:\t145 /100\t:  0.2863636016845703 \t41.52355742454529 \t140.33663940429688\n",
      "Batch:\t146 /100\t:  0.28174781799316406 \t41.806360721588135 \t173.84324645996094\n",
      "Batch:\t147 /100\t:  0.2830996513366699 \t42.09088158607483 \t183.56146240234375\n",
      "Batch:\t148 /100\t:  0.27860355377197266 \t42.37068819999695 \t120.2909927368164\n",
      "Batch:\t149 /100\t:  0.287128210067749 \t42.65924954414368 \t180.91513061523438\n",
      "Batch:\t150 /100\t:  0.28223681449890137 \t42.942564487457275 \t165.3004608154297\n",
      "Batch:\t151 /100\t:  0.2853415012359619 \t43.2289137840271 \t201.7639923095703\n",
      "Batch:\t152 /100\t:  0.2800471782684326 \t43.510053396224976 \t208.21328735351562\n",
      "Batch:\t153 /100\t:  0.28371429443359375 \t43.79548621177673 \t211.01669311523438\n",
      "Batch:\t154 /100\t:  0.2833888530731201 \t44.08046102523804 \t318.44110107421875\n",
      "Batch:\t155 /100\t:  0.2977418899536133 \t44.37931966781616 \t294.0631103515625\n",
      "Batch:\t156 /100\t:  0.2827310562133789 \t44.66316795349121 \t213.2764129638672\n",
      "Batch:\t157 /100\t:  0.28209733963012695 \t44.947054386138916 \t141.2684783935547\n",
      "Batch:\t158 /100\t:  0.2760181427001953 \t45.22944688796997 \t150.87445068359375\n",
      "Batch:\t159 /100\t:  0.27988409996032715 \t45.51031804084778 \t69.65933227539062\n",
      "Batch:\t160 /100\t:  0.2946479320526123 \t45.81094741821289 \t137.8641357421875\n",
      "Batch:\t161 /100\t:  0.2786874771118164 \t46.09074568748474 \t130.16482543945312\n",
      "Batch:\t162 /100\t:  0.2832469940185547 \t46.37534213066101 \t113.98133850097656\n",
      "Batch:\t163 /100\t:  0.28462862968444824 \t46.66115379333496 \t256.9561767578125\n",
      "Batch:\t164 /100\t:  0.2770512104034424 \t46.9395854473114 \t309.6263732910156\n",
      "Batch:\t165 /100\t:  0.2916262149810791 \t47.232996225357056 \t160.0601043701172\n",
      "Batch:\t166 /100\t:  0.28520679473876953 \t47.53438949584961 \t137.34898376464844\n",
      "Batch:\t167 /100\t:  0.27440547943115234 \t47.80986022949219 \t231.42974853515625\n",
      "Batch:\t168 /100\t:  0.28087830543518066 \t48.09218764305115 \t231.09002685546875\n",
      "Batch:\t169 /100\t:  0.2857990264892578 \t48.37923288345337 \t78.10921478271484\n",
      "Batch:\t170 /100\t:  0.28487730026245117 \t48.665698528289795 \t148.7837371826172\n",
      "Batch:\t171 /100\t:  0.28389668464660645 \t48.950772762298584 \t162.35415649414062\n",
      "Batch:\t172 /100\t:  0.2840390205383301 \t49.23721790313721 \t305.11572265625\n",
      "Batch:\t173 /100\t:  0.27625536918640137 \t49.514814138412476 \t198.65353393554688\n",
      "Batch:\t174 /100\t:  0.2786123752593994 \t49.79475498199463 \t216.3719940185547\n",
      "MODEL WEIGHTS RIGHT NOW:  6493.170593261719\n",
      "model with accuracy  0.31 stored at data/models/core_chain/bilstm_dot_ulmfit/lcquad/115/model.torch\n",
      "Time: 78.9053201675415\t Loss: 43829.4146156311\t Valdacc: 0.31\t Testacc: 0.26\n",
      " BestValidAcc: 0.31\n",
      " BestTestAcc: 0.26\n",
      "\n",
      "Epoch:  1 / 200\n",
      "Batch:\t0 /100\t:  0.2847630977630615 \t0.2864069938659668 \t85.8399429321289\n",
      "Batch:\t1 /100\t:  0.2815084457397461 \t0.5689706802368164 \t176.50738525390625\n",
      "Batch:\t2 /100\t:  0.2877542972564697 \t0.8582417964935303 \t280.685546875\n",
      "Batch:\t3 /100\t:  0.28600358963012695 \t1.1455764770507812 \t254.2830810546875\n",
      "Batch:\t4 /100\t:  0.27829694747924805 \t1.4252634048461914 \t237.0341796875\n",
      "Batch:\t5 /100\t:  0.2827270030975342 \t1.7092986106872559 \t178.50613403320312\n",
      "Batch:\t6 /100\t:  0.2782883644104004 \t1.9888272285461426 \t168.8313446044922\n",
      "Batch:\t7 /100\t:  0.28656721115112305 \t2.276505947113037 \t215.14480590820312\n",
      "Batch:\t8 /100\t:  0.28298115730285645 \t2.5609612464904785 \t205.0191650390625\n",
      "Batch:\t9 /100\t:  0.28815460205078125 \t2.8502187728881836 \t134.63795471191406\n",
      "Batch:\t10 /100\t:  0.2858846187591553 \t3.137270927429199 \t165.03562927246094\n",
      "Batch:\t11 /100\t:  0.28726768493652344 \t3.4259393215179443 \t182.833740234375\n",
      "Batch:\t12 /100\t:  0.2860910892486572 \t3.713312864303589 \t159.43063354492188\n",
      "Batch:\t13 /100\t:  0.28234291076660156 \t3.996893882751465 \t156.88612365722656\n",
      "Batch:\t14 /100\t:  0.2822842597961426 \t4.280477523803711 \t132.45970153808594\n",
      "Batch:\t15 /100\t:  0.2862727642059326 \t4.568249702453613 \t160.04100036621094\n",
      "Batch:\t16 /100\t:  0.2830631732940674 \t4.852444171905518 \t230.0210418701172\n",
      "Batch:\t17 /100\t:  0.283489465713501 \t5.137025594711304 \t157.15560913085938\n",
      "Batch:\t18 /100\t:  0.28678178787231445 \t5.425048589706421 \t159.13864135742188\n",
      "Batch:\t19 /100\t:  0.2827610969543457 \t5.708963394165039 \t217.743896484375\n",
      "Batch:\t20 /100\t:  0.28499317169189453 \t5.99498987197876 \t113.76445770263672\n",
      "Batch:\t21 /100\t:  0.29527711868286133 \t6.291723012924194 \t229.94851684570312\n",
      "Batch:\t22 /100\t:  0.2795119285583496 \t6.572255373001099 \t83.4818115234375\n",
      "Batch:\t23 /100\t:  0.28081345558166504 \t6.854527711868286 \t138.01609802246094\n",
      "Batch:\t24 /100\t:  0.2866556644439697 \t7.142377138137817 \t243.4352569580078\n",
      "Batch:\t25 /100\t:  0.2842092514038086 \t7.427663087844849 \t185.6806182861328\n",
      "Batch:\t26 /100\t:  0.2872779369354248 \t7.7160749435424805 \t188.1807861328125\n",
      "Batch:\t27 /100\t:  0.2882804870605469 \t8.005828142166138 \t146.8013458251953\n",
      "Batch:\t28 /100\t:  0.2853264808654785 \t8.29374098777771 \t145.7733917236328\n",
      "Batch:\t29 /100\t:  0.281815767288208 \t8.5766441822052 \t140.75086975097656\n",
      "Batch:\t30 /100\t:  0.2797229290008545 \t8.862934827804565 \t124.74260711669922\n",
      "Batch:\t31 /100\t:  0.28702807426452637 \t9.151475191116333 \t169.51951599121094\n",
      "Batch:\t32 /100\t:  0.2864511013031006 \t9.43907380104065 \t147.1150360107422\n",
      "Batch:\t33 /100\t:  0.2898678779602051 \t9.730044841766357 \t214.14956665039062\n",
      "Batch:\t34 /100\t:  0.28763484954833984 \t10.01892876625061 \t202.85195922851562\n",
      "Batch:\t35 /100\t:  0.28473734855651855 \t10.305089712142944 \t180.4180145263672\n",
      "Batch:\t36 /100\t:  0.27732372283935547 \t10.583628177642822 \t110.79518127441406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t37 /100\t:  0.2810359001159668 \t10.865610599517822 \t129.95530700683594\n",
      "Batch:\t38 /100\t:  0.2778646945953369 \t11.145063877105713 \t178.83370971679688\n",
      "Batch:\t39 /100\t:  0.2879598140716553 \t11.434070348739624 \t149.1237030029297\n",
      "Batch:\t40 /100\t:  0.31560683250427246 \t11.751316547393799 \t220.88011169433594\n",
      "Batch:\t41 /100\t:  0.28202176094055176 \t12.03441333770752 \t191.55474853515625\n",
      "Batch:\t42 /100\t:  0.28351902961730957 \t12.31952977180481 \t126.9154281616211\n",
      "Batch:\t43 /100\t:  0.2863035202026367 \t12.606808185577393 \t142.00152587890625\n",
      "Batch:\t44 /100\t:  0.29629993438720703 \t12.904948234558105 \t117.78494262695312\n",
      "Batch:\t45 /100\t:  0.28913331031799316 \t13.195143699645996 \t132.32952880859375\n",
      "Batch:\t46 /100\t:  0.2876136302947998 \t13.484155416488647 \t108.631103515625\n",
      "Batch:\t47 /100\t:  0.2788584232330322 \t13.76409387588501 \t207.93338012695312\n",
      "Batch:\t48 /100\t:  0.2863280773162842 \t14.051645994186401 \t230.4327850341797\n",
      "Batch:\t49 /100\t:  0.2895810604095459 \t14.34233546257019 \t115.39971160888672\n",
      "Batch:\t50 /100\t:  0.28090667724609375 \t14.624735355377197 \t114.399658203125\n",
      "Batch:\t51 /100\t:  0.2845923900604248 \t14.910326719284058 \t145.148193359375\n",
      "Batch:\t52 /100\t:  0.2837398052215576 \t15.195124626159668 \t149.43496704101562\n",
      "Batch:\t53 /100\t:  0.2823457717895508 \t15.478498697280884 \t239.4911651611328\n",
      "Batch:\t54 /100\t:  0.28288984298706055 \t15.762873888015747 \t182.70791625976562\n",
      "Batch:\t55 /100\t:  0.28345465660095215 \t16.047873497009277 \t98.82817077636719\n",
      "Batch:\t56 /100\t:  0.2788856029510498 \t16.328278064727783 \t160.46084594726562\n",
      "Batch:\t57 /100\t:  0.2812964916229248 \t16.61101746559143 \t137.18910217285156\n",
      "Batch:\t58 /100\t:  0.285259485244751 \t16.897818565368652 \t158.37847900390625\n",
      "Batch:\t59 /100\t:  0.2839338779449463 \t17.183220386505127 \t140.0819854736328\n",
      "Batch:\t60 /100\t:  0.27878642082214355 \t17.463043451309204 \t199.80010986328125\n",
      "Batch:\t61 /100\t:  0.29004812240600586 \t17.754487991333008 \t182.92559814453125\n",
      "Batch:\t62 /100\t:  0.2819364070892334 \t18.03794026374817 \t300.611328125\n",
      "Batch:\t63 /100\t:  0.28824305534362793 \t18.32722306251526 \t214.54173278808594\n",
      "Batch:\t64 /100\t:  0.2773733139038086 \t18.606507301330566 \t159.29518127441406\n",
      "Batch:\t65 /100\t:  0.2804279327392578 \t18.8884756565094 \t136.86334228515625\n",
      "Batch:\t66 /100\t:  0.2836618423461914 \t19.173557996749878 \t166.00277709960938\n",
      "Batch:\t67 /100\t:  0.28254270553588867 \t19.457139015197754 \t165.76223754882812\n",
      "Batch:\t68 /100\t:  0.29144906997680664 \t19.74966549873352 \t209.19651794433594\n",
      "Batch:\t69 /100\t:  0.28589415550231934 \t20.036762237548828 \t151.77041625976562\n",
      "Batch:\t70 /100\t:  0.2810215950012207 \t20.319400310516357 \t101.65266418457031\n",
      "Batch:\t71 /100\t:  0.2762460708618164 \t20.596643924713135 \t119.2283935546875\n",
      "Batch:\t72 /100\t:  0.2800760269165039 \t20.87804102897644 \t179.63743591308594\n",
      "Batch:\t73 /100\t:  0.2752547264099121 \t21.154247045516968 \t174.69796752929688\n",
      "Batch:\t74 /100\t:  0.2848074436187744 \t21.440200090408325 \t100.74160766601562\n",
      "Batch:\t75 /100\t:  0.2910020351409912 \t21.732227087020874 \t161.82742309570312\n",
      "Batch:\t76 /100\t:  0.27644944190979004 \t22.01009774208069 \t198.38648986816406\n",
      "Batch:\t77 /100\t:  0.28623414039611816 \t22.297570943832397 \t172.59507751464844\n",
      "Batch:\t78 /100\t:  0.2838761806488037 \t22.582660913467407 \t133.8779296875\n",
      "Batch:\t79 /100\t:  0.2855093479156494 \t22.869141817092896 \t93.16576385498047\n",
      "Batch:\t80 /100\t:  0.29332804679870605 \t23.16351056098938 \t160.47157287597656\n",
      "Batch:\t81 /100\t:  0.2853214740753174 \t23.45017099380493 \t361.69903564453125\n",
      "Batch:\t82 /100\t:  0.28361988067626953 \t23.734869480133057 \t136.95584106445312\n",
      "Batch:\t83 /100\t:  0.2936263084411621 \t24.03371787071228 \t148.544189453125\n",
      "Batch:\t84 /100\t:  0.286179780960083 \t24.32116460800171 \t185.34710693359375\n",
      "Batch:\t85 /100\t:  0.280930757522583 \t24.60315442085266 \t317.20623779296875\n",
      "Batch:\t86 /100\t:  0.28328609466552734 \t24.887882471084595 \t121.20093536376953\n",
      "Batch:\t87 /100\t:  0.2847013473510742 \t25.173701524734497 \t314.2797546386719\n",
      "Batch:\t88 /100\t:  0.28374505043029785 \t25.459036350250244 \t205.04150390625\n",
      "Batch:\t89 /100\t:  0.2845907211303711 \t25.745097875595093 \t99.32920837402344\n",
      "Batch:\t90 /100\t:  0.28601694107055664 \t26.03222966194153 \t171.6070098876953\n",
      "Batch:\t91 /100\t:  0.28265810012817383 \t26.31609010696411 \t129.2314910888672\n",
      "Batch:\t92 /100\t:  0.28488779067993164 \t26.602587938308716 \t163.20950317382812\n",
      "Batch:\t93 /100\t:  0.28049492835998535 \t26.884570121765137 \t147.9705352783203\n",
      "Batch:\t94 /100\t:  0.2857186794281006 \t27.171268463134766 \t139.03915405273438\n",
      "Batch:\t95 /100\t:  0.2792692184448242 \t27.452012062072754 \t110.55027770996094\n",
      "Batch:\t96 /100\t:  0.2866504192352295 \t27.739677667617798 \t137.53199768066406\n",
      "Batch:\t97 /100\t:  0.28145527839660645 \t28.022781372070312 \t143.0321044921875\n",
      "Batch:\t98 /100\t:  0.2843616008758545 \t28.308416843414307 \t99.38580322265625\n",
      "Batch:\t99 /100\t:  0.2841312885284424 \t28.59387707710266 \t149.53468322753906\n",
      "Batch:\t100 /100\t:  0.29233789443969727 \t28.887782335281372 \t127.29790496826172\n",
      "Batch:\t101 /100\t:  0.28365397453308105 \t29.17301630973816 \t127.47875213623047\n",
      "Batch:\t102 /100\t:  0.2802422046661377 \t29.45448398590088 \t147.62815856933594\n",
      "Batch:\t103 /100\t:  0.28038477897644043 \t29.73626732826233 \t186.26333618164062\n",
      "Batch:\t104 /100\t:  0.2834506034851074 \t30.021000385284424 \t142.92311096191406\n",
      "Batch:\t105 /100\t:  0.30153942108154297 \t30.323660850524902 \t189.44281005859375\n",
      "Batch:\t106 /100\t:  0.28861522674560547 \t30.613855838775635 \t128.20443725585938\n",
      "Batch:\t107 /100\t:  0.2857656478881836 \t30.900973558425903 \t161.247802734375\n",
      "Batch:\t108 /100\t:  0.2823803424835205 \t31.184457063674927 \t113.1235122680664\n",
      "Batch:\t109 /100\t:  0.29366040229797363 \t31.479374170303345 \t148.37380981445312\n",
      "Batch:\t110 /100\t:  0.29297518730163574 \t31.773331880569458 \t167.2764892578125\n",
      "Batch:\t111 /100\t:  0.28224754333496094 \t32.05686902999878 \t230.52166748046875\n",
      "Batch:\t112 /100\t:  0.28894829750061035 \t32.34683442115784 \t179.30101013183594\n",
      "Batch:\t113 /100\t:  0.27898716926574707 \t32.627065658569336 \t182.02825927734375\n",
      "Batch:\t114 /100\t:  0.28169751167297363 \t32.90993785858154 \t168.7002410888672\n",
      "Batch:\t115 /100\t:  0.2804734706878662 \t33.19173192977905 \t169.85386657714844\n",
      "Batch:\t116 /100\t:  0.29108381271362305 \t33.48426961898804 \t226.11038208007812\n",
      "Batch:\t117 /100\t:  0.28185129165649414 \t33.76760792732239 \t194.4067840576172\n",
      "Batch:\t118 /100\t:  0.28565502166748047 \t34.05470633506775 \t174.27084350585938\n",
      "Batch:\t119 /100\t:  0.28539562225341797 \t34.34129190444946 \t137.43739318847656\n",
      "Batch:\t120 /100\t:  0.2885422706604004 \t34.63111114501953 \t172.61856079101562\n",
      "Batch:\t121 /100\t:  0.2830195426940918 \t34.915149450302124 \t176.10137939453125\n",
      "Batch:\t122 /100\t:  0.2780497074127197 \t35.19424390792847 \t146.63978576660156\n",
      "Batch:\t123 /100\t:  0.2852954864501953 \t35.480607748031616 \t130.70021057128906\n",
      "Batch:\t124 /100\t:  0.2770414352416992 \t35.75919723510742 \t153.67735290527344\n",
      "Batch:\t125 /100\t:  0.2833843231201172 \t36.04366445541382 \t166.80291748046875\n",
      "Batch:\t126 /100\t:  0.2858436107635498 \t36.331209659576416 \t165.62631225585938\n",
      "Batch:\t127 /100\t:  0.2834508419036865 \t36.615837812423706 \t189.32583618164062\n",
      "Batch:\t128 /100\t:  0.28119325637817383 \t36.898112297058105 \t125.67822265625\n",
      "Batch:\t129 /100\t:  0.29250168800354004 \t37.194135665893555 \t116.9959945678711\n",
      "Batch:\t130 /100\t:  0.2882993221282959 \t37.4839608669281 \t125.5835189819336\n",
      "Batch:\t131 /100\t:  0.28498387336730957 \t37.77023434638977 \t134.17149353027344\n",
      "Batch:\t132 /100\t:  0.2831239700317383 \t38.054693937301636 \t120.54029846191406\n",
      "Batch:\t133 /100\t:  0.28452134132385254 \t38.34239912033081 \t130.99505615234375\n",
      "Batch:\t134 /100\t:  0.28406858444213867 \t38.62798309326172 \t147.87989807128906\n",
      "Batch:\t135 /100\t:  0.2863905429840088 \t38.91605830192566 \t111.7803955078125\n",
      "Batch:\t136 /100\t:  0.28999805450439453 \t39.20751881599426 \t189.04429626464844\n",
      "Batch:\t137 /100\t:  0.2898564338684082 \t39.49846529960632 \t208.49371337890625\n",
      "Batch:\t138 /100\t:  0.28723931312561035 \t39.78674077987671 \t112.86687469482422\n",
      "Batch:\t139 /100\t:  0.2788398265838623 \t40.06700944900513 \t141.99066162109375\n",
      "Batch:\t140 /100\t:  0.2789480686187744 \t40.34713935852051 \t137.6151580810547\n",
      "Batch:\t141 /100\t:  0.28491806983947754 \t40.63492703437805 \t120.84917449951172\n",
      "Batch:\t142 /100\t:  0.27701377868652344 \t40.91326856613159 \t101.30317687988281\n",
      "Batch:\t143 /100\t:  0.28919363021850586 \t41.20393419265747 \t81.97935485839844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t144 /100\t:  0.28217196464538574 \t41.48791289329529 \t89.84453582763672\n",
      "Batch:\t145 /100\t:  0.28656721115112305 \t41.77557349205017 \t145.39210510253906\n",
      "Batch:\t146 /100\t:  0.290424108505249 \t42.0671706199646 \t338.5005798339844\n",
      "Batch:\t147 /100\t:  0.2816345691680908 \t42.35083556175232 \t173.00035095214844\n",
      "Batch:\t148 /100\t:  0.28731870651245117 \t42.639689207077026 \t76.1186752319336\n",
      "Batch:\t149 /100\t:  0.28518152236938477 \t42.92657494544983 \t408.4632873535156\n",
      "Batch:\t150 /100\t:  0.2824409008026123 \t43.210031270980835 \t73.583251953125\n",
      "Batch:\t151 /100\t:  0.2798268795013428 \t43.49136543273926 \t75.49170684814453\n",
      "Batch:\t152 /100\t:  0.2822723388671875 \t43.775062561035156 \t126.57853698730469\n",
      "Batch:\t153 /100\t:  0.2846214771270752 \t44.06110167503357 \t89.33523559570312\n",
      "Batch:\t154 /100\t:  0.27750492095947266 \t44.33958339691162 \t161.671142578125\n",
      "Batch:\t155 /100\t:  0.2801227569580078 \t44.62152934074402 \t333.8701171875\n",
      "Batch:\t156 /100\t:  0.28505444526672363 \t44.90749931335449 \t116.73599243164062\n",
      "Batch:\t157 /100\t:  0.28102993965148926 \t45.18989634513855 \t291.1376953125\n",
      "Batch:\t158 /100\t:  0.2837684154510498 \t45.47937631607056 \t116.65471649169922\n",
      "Batch:\t159 /100\t:  0.2969217300415039 \t45.7777099609375 \t221.2144317626953\n",
      "Batch:\t160 /100\t:  0.2767055034637451 \t46.055511474609375 \t102.87784576416016\n",
      "Batch:\t161 /100\t:  0.28621554374694824 \t46.34280347824097 \t112.70443725585938\n",
      "Batch:\t162 /100\t:  0.29217076301574707 \t46.63650298118591 \t129.73497009277344\n",
      "Batch:\t163 /100\t:  0.2797994613647461 \t46.91772532463074 \t128.2445526123047\n",
      "Batch:\t164 /100\t:  0.28319573402404785 \t47.20225739479065 \t103.47647094726562\n",
      "Batch:\t165 /100\t:  0.2797245979309082 \t47.48343229293823 \t141.24143981933594\n",
      "Batch:\t166 /100\t:  0.29039859771728516 \t47.77516150474548 \t129.58253479003906\n",
      "Batch:\t167 /100\t:  0.2829422950744629 \t48.05964255332947 \t154.7397003173828\n",
      "Batch:\t168 /100\t:  0.3005383014678955 \t48.36674237251282 \t81.33068084716797\n",
      "Batch:\t169 /100\t:  0.2831597328186035 \t48.65201234817505 \t173.48233032226562\n",
      "Batch:\t170 /100\t:  0.28818607330322266 \t48.941853761672974 \t71.76535034179688\n",
      "Batch:\t171 /100\t:  0.2798030376434326 \t49.22316861152649 \t176.93057250976562\n",
      "Batch:\t172 /100\t:  0.29065561294555664 \t49.51504898071289 \t136.1240997314453\n",
      "Batch:\t173 /100\t:  0.2828037738800049 \t49.79902505874634 \t158.822265625\n",
      "Batch:\t174 /100\t:  0.27894020080566406 \t50.079158544540405 \t116.97147369384766\n",
      "Time: 77.51596522331238\t Loss: 28231.076301574707\t Valdacc: 0.3\t Testacc: 0.26\n",
      " BestValidAcc: 0.31\n",
      " BestTestAcc: 0.26\n",
      "\n",
      "Epoch:  2 / 200\n",
      "Batch:\t0 /100\t:  0.2728383541107178 \t0.2745554447174072 \t233.5332794189453\n",
      "Batch:\t1 /100\t:  0.27876877784729004 \t0.554694414138794 \t66.28840637207031\n",
      "Batch:\t2 /100\t:  0.27961134910583496 \t0.835578203201294 \t219.4159393310547\n",
      "Batch:\t3 /100\t:  0.27540063858032227 \t1.112067461013794 \t143.88882446289062\n",
      "Batch:\t4 /100\t:  0.28954219818115234 \t1.4025728702545166 \t104.41706085205078\n",
      "Batch:\t5 /100\t:  0.2838480472564697 \t1.687795877456665 \t58.085716247558594\n",
      "Batch:\t6 /100\t:  0.2851595878601074 \t1.9743564128875732 \t211.41131591796875\n",
      "Batch:\t7 /100\t:  0.2896409034729004 \t2.2654547691345215 \t78.41039276123047\n",
      "Batch:\t8 /100\t:  0.2871229648590088 \t2.5540661811828613 \t169.23867797851562\n",
      "Batch:\t9 /100\t:  0.2825477123260498 \t2.8380348682403564 \t122.537841796875\n",
      "Batch:\t10 /100\t:  0.2913858890533447 \t3.1311211585998535 \t195.59629821777344\n",
      "Batch:\t11 /100\t:  0.2916748523712158 \t3.424215793609619 \t146.508544921875\n",
      "Batch:\t12 /100\t:  0.2826566696166992 \t3.708345890045166 \t119.65380859375\n",
      "Batch:\t13 /100\t:  0.2843596935272217 \t3.993971109390259 \t187.19680786132812\n",
      "Batch:\t14 /100\t:  0.28003954887390137 \t4.275251388549805 \t116.97886657714844\n",
      "Batch:\t15 /100\t:  0.28769588470458984 \t4.564426422119141 \t122.53972625732422\n",
      "Batch:\t16 /100\t:  0.28042101860046387 \t4.8463287353515625 \t87.09649658203125\n",
      "Batch:\t17 /100\t:  0.28502774238586426 \t5.132917165756226 \t127.06925201416016\n",
      "Batch:\t18 /100\t:  0.281536340713501 \t5.41569972038269 \t154.3729705810547\n",
      "Batch:\t19 /100\t:  0.2780442237854004 \t5.6951212882995605 \t101.31012725830078\n",
      "Batch:\t20 /100\t:  0.2835073471069336 \t5.9798548221588135 \t176.7168426513672\n",
      "Batch:\t21 /100\t:  0.29578304290771484 \t6.276139497756958 \t144.1918487548828\n",
      "Batch:\t22 /100\t:  0.28861498832702637 \t6.566140413284302 \t184.1014404296875\n",
      "Batch:\t23 /100\t:  0.3029356002807617 \t6.8765199184417725 \t116.33283233642578\n",
      "Batch:\t24 /100\t:  0.28725266456604004 \t7.1651787757873535 \t158.08322143554688\n",
      "Batch:\t25 /100\t:  0.28037500381469727 \t7.451502323150635 \t132.6643524169922\n",
      "Batch:\t26 /100\t:  0.282930850982666 \t7.735953092575073 \t117.51810455322266\n",
      "Batch:\t27 /100\t:  0.2915842533111572 \t8.028718948364258 \t130.8058319091797\n",
      "Batch:\t28 /100\t:  0.289506196975708 \t8.319307565689087 \t131.9849395751953\n",
      "Batch:\t29 /100\t:  0.2867588996887207 \t8.60734510421753 \t132.00717163085938\n",
      "Batch:\t30 /100\t:  0.28094935417175293 \t8.890136003494263 \t133.0574188232422\n",
      "Batch:\t31 /100\t:  0.30657100677490234 \t9.203419208526611 \t133.35140991210938\n",
      "Batch:\t32 /100\t:  0.290679931640625 \t9.495817422866821 \t123.32217407226562\n",
      "Batch:\t33 /100\t:  0.27436113357543945 \t9.771486282348633 \t136.03286743164062\n",
      "Batch:\t34 /100\t:  0.2884373664855957 \t10.061015367507935 \t136.26309204101562\n",
      "Batch:\t35 /100\t:  0.28342628479003906 \t10.346264600753784 \t139.79859924316406\n",
      "Batch:\t36 /100\t:  0.28083372116088867 \t10.628172397613525 \t126.97337341308594\n",
      "Batch:\t37 /100\t:  0.27953433990478516 \t10.908889770507812 \t115.95983123779297\n",
      "Batch:\t38 /100\t:  0.27878522872924805 \t11.188664197921753 \t147.36094665527344\n",
      "Batch:\t39 /100\t:  0.277667760848999 \t11.467807531356812 \t132.5747833251953\n",
      "Batch:\t40 /100\t:  0.29517173767089844 \t11.76421308517456 \t168.19210815429688\n",
      "Batch:\t41 /100\t:  0.2886345386505127 \t12.05435848236084 \t89.58699798583984\n",
      "Batch:\t42 /100\t:  0.2848246097564697 \t12.340466976165771 \t131.59425354003906\n",
      "Batch:\t43 /100\t:  0.2843785285949707 \t12.627001523971558 \t133.21144104003906\n",
      "Batch:\t44 /100\t:  0.30145764350891113 \t12.929678678512573 \t108.62694549560547\n",
      "Batch:\t45 /100\t:  0.29991579055786133 \t13.231389999389648 \t103.44274139404297\n",
      "Batch:\t46 /100\t:  0.28978729248046875 \t13.522268533706665 \t120.31266021728516\n",
      "Batch:\t47 /100\t:  0.2885596752166748 \t13.811985731124878 \t149.3836669921875\n",
      "Batch:\t48 /100\t:  0.2873382568359375 \t14.100282430648804 \t119.26969909667969\n",
      "Batch:\t49 /100\t:  0.2824716567993164 \t14.384255170822144 \t128.3607177734375\n",
      "Batch:\t50 /100\t:  0.2839634418487549 \t14.669214248657227 \t129.28469848632812\n",
      "Batch:\t51 /100\t:  0.276684045791626 \t14.947102308273315 \t84.58987426757812\n",
      "Batch:\t52 /100\t:  0.2818267345428467 \t15.23021650314331 \t198.109375\n",
      "Batch:\t53 /100\t:  0.2850766181945801 \t15.516303300857544 \t86.43278503417969\n",
      "Batch:\t54 /100\t:  0.2928745746612549 \t15.810603618621826 \t152.41265869140625\n",
      "Batch:\t55 /100\t:  0.2862818241119385 \t16.09835934638977 \t171.7509765625\n",
      "Batch:\t56 /100\t:  0.2853519916534424 \t16.38477659225464 \t73.59288024902344\n",
      "Batch:\t57 /100\t:  0.2828347682952881 \t16.6688711643219 \t147.64144897460938\n",
      "Batch:\t58 /100\t:  0.27694153785705566 \t16.947082996368408 \t132.59991455078125\n",
      "Batch:\t59 /100\t:  0.30016517639160156 \t17.248312950134277 \t114.04460906982422\n",
      "Batch:\t60 /100\t:  0.27812838554382324 \t17.527840852737427 \t74.92550659179688\n",
      "Batch:\t61 /100\t:  0.28151726722717285 \t17.81041121482849 \t60.77656936645508\n",
      "Batch:\t62 /100\t:  0.27799081802368164 \t18.0894775390625 \t182.38070678710938\n",
      "Batch:\t63 /100\t:  0.27829599380493164 \t18.369182586669922 \t62.902381896972656\n",
      "Batch:\t64 /100\t:  0.2865931987762451 \t18.657343864440918 \t175.8616943359375\n",
      "Batch:\t65 /100\t:  0.2795889377593994 \t18.93796968460083 \t123.38011932373047\n",
      "Batch:\t66 /100\t:  0.2844576835632324 \t19.223819971084595 \t50.00376510620117\n",
      "Batch:\t67 /100\t:  0.2937593460083008 \t19.518596172332764 \t260.23321533203125\n",
      "Batch:\t68 /100\t:  0.2740027904510498 \t19.79417872428894 \t152.54173278808594\n",
      "Batch:\t69 /100\t:  0.2876276969909668 \t20.082817316055298 \t193.5218505859375\n",
      "Batch:\t70 /100\t:  0.28804945945739746 \t20.372337579727173 \t51.31200408935547\n",
      "Batch:\t71 /100\t:  0.2792201042175293 \t20.652726650238037 \t153.04541015625\n",
      "Batch:\t72 /100\t:  0.28734731674194336 \t20.94151473045349 \t265.9871826171875\n",
      "Batch:\t73 /100\t:  0.29103827476501465 \t21.233882188796997 \t89.24908447265625\n",
      "Batch:\t74 /100\t:  0.2874031066894531 \t21.5227689743042 \t150.17431640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t75 /100\t:  0.2846245765686035 \t21.80852174758911 \t212.3553466796875\n",
      "Batch:\t76 /100\t:  0.28105688095092773 \t22.090779542922974 \t167.1796417236328\n",
      "Batch:\t77 /100\t:  0.2805142402648926 \t22.372512102127075 \t182.4715118408203\n",
      "Batch:\t78 /100\t:  0.2904679775238037 \t22.664756536483765 \t175.59471130371094\n",
      "Batch:\t79 /100\t:  0.28753185272216797 \t22.95338249206543 \t136.40565490722656\n",
      "Batch:\t80 /100\t:  0.28493356704711914 \t23.239824056625366 \t129.38865661621094\n",
      "Batch:\t81 /100\t:  0.2937440872192383 \t23.539946794509888 \t123.75284576416016\n",
      "Batch:\t82 /100\t:  0.2855210304260254 \t23.826537370681763 \t123.21477508544922\n",
      "Batch:\t83 /100\t:  0.2998957633972168 \t24.13312864303589 \t98.86343383789062\n",
      "Batch:\t84 /100\t:  0.2868926525115967 \t24.421079397201538 \t119.58230590820312\n",
      "Batch:\t85 /100\t:  0.2759549617767334 \t24.69799542427063 \t123.48590087890625\n",
      "Batch:\t86 /100\t:  0.2881448268890381 \t24.987552881240845 \t104.91847229003906\n",
      "Batch:\t87 /100\t:  0.2811269760131836 \t25.269713163375854 \t121.51792907714844\n",
      "Batch:\t88 /100\t:  0.28792333602905273 \t25.558998107910156 \t120.76365661621094\n",
      "Batch:\t89 /100\t:  0.2868006229400635 \t25.847071170806885 \t109.95756530761719\n",
      "Batch:\t90 /100\t:  0.2971200942993164 \t26.146142721176147 \t160.49046325683594\n",
      "Batch:\t91 /100\t:  0.2872626781463623 \t26.435279607772827 \t90.14148712158203\n",
      "Batch:\t92 /100\t:  0.2898731231689453 \t26.727351427078247 \t160.89808654785156\n",
      "Batch:\t93 /100\t:  0.2945249080657959 \t27.028530597686768 \t137.66384887695312\n",
      "Batch:\t94 /100\t:  0.28390073776245117 \t27.313875436782837 \t67.29048156738281\n",
      "Batch:\t95 /100\t:  0.28342175483703613 \t27.598432064056396 \t132.15826416015625\n",
      "Batch:\t96 /100\t:  0.28882336616516113 \t27.88847327232361 \t102.90532684326172\n",
      "Batch:\t97 /100\t:  0.3168213367462158 \t28.211700677871704 \t104.77647399902344\n",
      "Batch:\t98 /100\t:  0.2849123477935791 \t28.498204469680786 \t113.39484405517578\n",
      "Batch:\t99 /100\t:  0.2835559844970703 \t28.782776594161987 \t116.96724700927734\n",
      "Batch:\t100 /100\t:  0.2886645793914795 \t29.07258105278015 \t104.76670837402344\n",
      "Batch:\t101 /100\t:  0.27730250358581543 \t29.351112365722656 \t189.51364135742188\n",
      "Batch:\t102 /100\t:  0.28624844551086426 \t29.638877391815186 \t98.4601058959961\n",
      "Batch:\t103 /100\t:  0.3002941608428955 \t29.94021964073181 \t77.43461608886719\n",
      "Batch:\t104 /100\t:  0.28072166442871094 \t30.2222797870636 \t139.4708709716797\n",
      "Batch:\t105 /100\t:  0.28086042404174805 \t30.504266262054443 \t135.11163330078125\n",
      "Batch:\t106 /100\t:  0.2803356647491455 \t30.785770416259766 \t120.27184295654297\n",
      "Batch:\t107 /100\t:  0.27895164489746094 \t31.06616711616516 \t66.51922607421875\n",
      "Batch:\t108 /100\t:  0.2806978225708008 \t31.348029613494873 \t65.73735046386719\n",
      "Batch:\t109 /100\t:  0.2818174362182617 \t31.631272554397583 \t56.87162399291992\n",
      "Batch:\t110 /100\t:  0.2871284484863281 \t31.91947293281555 \t134.2716064453125\n",
      "Batch:\t111 /100\t:  0.2825024127960205 \t32.20366311073303 \t44.940216064453125\n",
      "Batch:\t112 /100\t:  0.2851240634918213 \t32.489869832992554 \t195.72425842285156\n",
      "Batch:\t113 /100\t:  0.2843017578125 \t32.7754340171814 \t152.49139404296875\n",
      "Batch:\t114 /100\t:  0.28070878982543945 \t33.05711555480957 \t34.843265533447266\n",
      "Batch:\t115 /100\t:  0.2849111557006836 \t33.34376358985901 \t54.91291046142578\n",
      "Batch:\t116 /100\t:  0.2828962802886963 \t33.627758264541626 \t70.92729187011719\n",
      "Batch:\t117 /100\t:  0.28711676597595215 \t33.91641116142273 \t614.224365234375\n",
      "Batch:\t118 /100\t:  0.28362059593200684 \t34.20133137702942 \t126.03593444824219\n",
      "Batch:\t119 /100\t:  0.28787899017333984 \t34.49060869216919 \t254.714111328125\n",
      "Batch:\t120 /100\t:  0.28731703758239746 \t34.779133558273315 \t101.71668243408203\n",
      "Batch:\t121 /100\t:  0.2828688621520996 \t35.0632119178772 \t313.0249938964844\n",
      "Batch:\t122 /100\t:  0.2811927795410156 \t35.345412731170654 \t98.77225494384766\n",
      "Batch:\t123 /100\t:  0.2873566150665283 \t35.6344108581543 \t82.2496109008789\n",
      "Batch:\t124 /100\t:  0.27724146842956543 \t35.912899017333984 \t74.13909149169922\n",
      "Batch:\t125 /100\t:  0.284257173538208 \t36.198707818984985 \t171.57647705078125\n",
      "Batch:\t126 /100\t:  0.29054832458496094 \t36.49030256271362 \t59.5860710144043\n",
      "Batch:\t127 /100\t:  0.28247618675231934 \t36.77421569824219 \t191.373291015625\n",
      "Batch:\t128 /100\t:  0.28145766258239746 \t37.05682706832886 \t67.81854248046875\n",
      "Batch:\t129 /100\t:  0.2837188243865967 \t37.34163546562195 \t108.81273651123047\n",
      "Batch:\t130 /100\t:  0.27913331985473633 \t37.621923208236694 \t57.061676025390625\n",
      "Batch:\t131 /100\t:  0.293137788772583 \t37.91655492782593 \t107.60511779785156\n",
      "Batch:\t132 /100\t:  0.2846086025238037 \t38.2136435508728 \t118.02870178222656\n",
      "Batch:\t133 /100\t:  0.2950143814086914 \t38.50973701477051 \t97.9919662475586\n",
      "Batch:\t134 /100\t:  0.28568458557128906 \t38.79673910140991 \t118.56809997558594\n",
      "Batch:\t135 /100\t:  0.31435370445251465 \t39.11260986328125 \t95.63050079345703\n",
      "Batch:\t136 /100\t:  0.28934693336486816 \t39.40311551094055 \t135.73707580566406\n",
      "Batch:\t137 /100\t:  0.2999274730682373 \t39.70477867126465 \t130.9528350830078\n",
      "Batch:\t138 /100\t:  0.2838001251220703 \t39.989723443984985 \t91.50152587890625\n",
      "Batch:\t139 /100\t:  0.2831251621246338 \t40.274601459503174 \t99.22465515136719\n",
      "Batch:\t140 /100\t:  0.28525590896606445 \t40.56081748008728 \t128.2409210205078\n",
      "Batch:\t141 /100\t:  0.2906198501586914 \t40.85302519798279 \t75.14775848388672\n",
      "Batch:\t142 /100\t:  0.2826211452484131 \t41.13684153556824 \t153.39649963378906\n",
      "Batch:\t143 /100\t:  0.2935338020324707 \t41.43195843696594 \t77.89496612548828\n",
      "Batch:\t144 /100\t:  0.2795600891113281 \t41.714388370513916 \t340.0090637207031\n",
      "Batch:\t145 /100\t:  0.2750122547149658 \t41.9906370639801 \t136.4669952392578\n",
      "Batch:\t146 /100\t:  0.27681446075439453 \t42.26865243911743 \t259.7616882324219\n",
      "Batch:\t147 /100\t:  0.29567384719848633 \t42.565828800201416 \t59.4017448425293\n",
      "Batch:\t148 /100\t:  0.28145670890808105 \t42.848881483078 \t326.8078308105469\n",
      "Batch:\t149 /100\t:  0.29187726974487305 \t43.14244294166565 \t83.11489868164062\n",
      "Batch:\t150 /100\t:  0.28778529167175293 \t43.431745290756226 \t141.72113037109375\n",
      "Batch:\t151 /100\t:  0.27376818656921387 \t43.70651888847351 \t114.68535614013672\n",
      "Batch:\t152 /100\t:  0.2734665870666504 \t43.981260538101196 \t91.02188110351562\n",
      "Batch:\t153 /100\t:  0.28029298782348633 \t44.262770891189575 \t41.71623611450195\n",
      "Batch:\t154 /100\t:  0.28392744064331055 \t44.5485143661499 \t274.6120910644531\n",
      "Batch:\t155 /100\t:  0.2813143730163574 \t44.830857276916504 \t214.7286834716797\n",
      "Batch:\t156 /100\t:  0.2880723476409912 \t45.120052099227905 \t196.744873046875\n",
      "Batch:\t157 /100\t:  0.2872617244720459 \t45.408390045166016 \t50.969722747802734\n",
      "Batch:\t158 /100\t:  0.28757596015930176 \t45.697163343429565 \t117.87120056152344\n",
      "Batch:\t159 /100\t:  0.28229427337646484 \t45.98058366775513 \t107.72968292236328\n",
      "Batch:\t160 /100\t:  0.2817540168762207 \t46.263548612594604 \t133.91444396972656\n",
      "Batch:\t161 /100\t:  0.2814962863922119 \t46.54609274864197 \t127.36823272705078\n",
      "Batch:\t162 /100\t:  0.2934563159942627 \t46.84087085723877 \t85.51829528808594\n",
      "Batch:\t163 /100\t:  0.28548097610473633 \t47.12698531150818 \t105.1257553100586\n",
      "Batch:\t164 /100\t:  0.28292250633239746 \t47.41157126426697 \t118.54085540771484\n",
      "Batch:\t165 /100\t:  0.2873096466064453 \t47.704041481018066 \t100.78657531738281\n",
      "Batch:\t166 /100\t:  0.28317832946777344 \t47.98835277557373 \t117.61754608154297\n",
      "Batch:\t167 /100\t:  0.2832159996032715 \t48.27286958694458 \t63.53744125366211\n",
      "Batch:\t168 /100\t:  0.28955864906311035 \t48.56400489807129 \t76.40977478027344\n",
      "Batch:\t169 /100\t:  0.2831687927246094 \t48.84922957420349 \t120.59765625\n",
      "Batch:\t170 /100\t:  0.2797353267669678 \t49.13008785247803 \t55.29692840576172\n",
      "Batch:\t171 /100\t:  0.2890279293060303 \t49.42064332962036 \t170.28872680664062\n",
      "Batch:\t172 /100\t:  0.289989709854126 \t49.71160626411438 \t71.32168579101562\n",
      "Batch:\t173 /100\t:  0.2798149585723877 \t49.99254536628723 \t84.44578552246094\n",
      "Batch:\t174 /100\t:  0.28841733932495117 \t50.282254219055176 \t26.934877395629883\n",
      "Time: 77.73650527000427\t Loss: 22847.889043807983\t Valdacc: 0.27\t Testacc: 0.295\n",
      " BestValidAcc: 0.31\n",
      " BestTestAcc: 0.295\n",
      "\n",
      "Epoch:  3 / 200\n",
      "Batch:\t0 /100\t:  0.28647923469543457 \t0.2890138626098633 \t321.8665466308594\n",
      "Batch:\t1 /100\t:  0.2764441967010498 \t0.5670790672302246 \t127.97347259521484\n",
      "Batch:\t2 /100\t:  0.28095507621765137 \t0.8493225574493408 \t101.40763092041016\n",
      "Batch:\t3 /100\t:  0.2821500301361084 \t1.1325054168701172 \t186.7841033935547\n",
      "Batch:\t4 /100\t:  0.2863156795501709 \t1.420253038406372 \t204.61802673339844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t5 /100\t:  0.27889418601989746 \t1.7014398574829102 \t91.96903991699219\n",
      "Batch:\t6 /100\t:  0.2877521514892578 \t1.9910519123077393 \t61.223567962646484\n",
      "Batch:\t7 /100\t:  0.27820396423339844 \t2.270217180252075 \t247.03826904296875\n",
      "Batch:\t8 /100\t:  0.28078770637512207 \t2.552351474761963 \t191.69256591796875\n",
      "Batch:\t9 /100\t:  0.28439950942993164 \t2.8377110958099365 \t173.66026306152344\n",
      "Batch:\t10 /100\t:  0.2819075584411621 \t3.1210086345672607 \t142.5013427734375\n",
      "Batch:\t11 /100\t:  0.28806352615356445 \t3.4102203845977783 \t66.82835388183594\n",
      "Batch:\t12 /100\t:  0.2893376350402832 \t3.700857400894165 \t58.21564865112305\n",
      "Batch:\t13 /100\t:  0.2849721908569336 \t3.9871461391448975 \t38.938133239746094\n",
      "Batch:\t14 /100\t:  0.3144979476928711 \t4.303140640258789 \t84.09174346923828\n",
      "Batch:\t15 /100\t:  0.2800295352935791 \t4.584228277206421 \t291.6002197265625\n",
      "Batch:\t16 /100\t:  0.28209638595581055 \t4.867342472076416 \t133.32205200195312\n",
      "Batch:\t17 /100\t:  0.282285213470459 \t5.150603294372559 \t111.04190063476562\n",
      "Batch:\t18 /100\t:  0.28768491744995117 \t5.439558029174805 \t123.72623443603516\n",
      "Batch:\t19 /100\t:  0.286207914352417 \t5.726903676986694 \t117.99895477294922\n",
      "Batch:\t20 /100\t:  0.28925085067749023 \t6.0176026821136475 \t99.84831237792969\n",
      "Batch:\t21 /100\t:  0.2852048873901367 \t6.305308818817139 \t64.33647918701172\n",
      "Batch:\t22 /100\t:  0.2850518226623535 \t6.591467618942261 \t151.5913543701172\n",
      "Batch:\t23 /100\t:  0.3097057342529297 \t6.902973890304565 \t185.3059539794922\n",
      "Batch:\t24 /100\t:  0.28684401512145996 \t7.191277027130127 \t143.04566955566406\n",
      "Batch:\t25 /100\t:  0.28487682342529297 \t7.477155923843384 \t82.16938018798828\n",
      "Batch:\t26 /100\t:  0.2881200313568115 \t7.766969203948975 \t92.40771484375\n",
      "Batch:\t27 /100\t:  0.2978830337524414 \t8.065903902053833 \t121.60646057128906\n",
      "Batch:\t28 /100\t:  0.2844398021697998 \t8.35168170928955 \t180.45675659179688\n",
      "Batch:\t29 /100\t:  0.2775876522064209 \t8.630543947219849 \t93.99211883544922\n",
      "Batch:\t30 /100\t:  0.2846970558166504 \t8.916436672210693 \t97.82893371582031\n",
      "Batch:\t31 /100\t:  0.28395891189575195 \t9.201932668685913 \t105.88414764404297\n",
      "Batch:\t32 /100\t:  0.28343963623046875 \t9.48667573928833 \t72.35247802734375\n",
      "Batch:\t33 /100\t:  0.2828695774078369 \t9.7725350856781 \t143.97735595703125\n",
      "Batch:\t34 /100\t:  0.2846860885620117 \t10.058570623397827 \t148.0454864501953\n",
      "Batch:\t35 /100\t:  0.3043031692504883 \t10.364300966262817 \t127.23220825195312\n",
      "Batch:\t36 /100\t:  0.28502607345581055 \t10.650387287139893 \t143.9017791748047\n",
      "Batch:\t37 /100\t:  0.2832348346710205 \t10.934950113296509 \t90.51010131835938\n",
      "Batch:\t38 /100\t:  0.28896164894104004 \t11.22485876083374 \t113.1009292602539\n",
      "Batch:\t39 /100\t:  0.28826212882995605 \t11.51481318473816 \t114.53924560546875\n",
      "Batch:\t40 /100\t:  0.2783017158508301 \t11.79422402381897 \t118.34163665771484\n",
      "Batch:\t41 /100\t:  0.28104090690612793 \t12.078580379486084 \t66.57608032226562\n",
      "Batch:\t42 /100\t:  0.28239917755126953 \t12.362322807312012 \t85.65037536621094\n",
      "Batch:\t43 /100\t:  0.30274367332458496 \t12.666358709335327 \t126.13888549804688\n",
      "Batch:\t44 /100\t:  0.28366565704345703 \t12.951459169387817 \t129.4546356201172\n",
      "Batch:\t45 /100\t:  0.29590368270874023 \t13.248624801635742 \t94.61082458496094\n",
      "Batch:\t46 /100\t:  0.2850759029388428 \t13.53478479385376 \t102.56884765625\n",
      "Batch:\t47 /100\t:  0.2869257926940918 \t13.823579549789429 \t102.70685577392578\n",
      "Batch:\t48 /100\t:  0.28644680976867676 \t14.111009120941162 \t56.40085983276367\n",
      "Batch:\t49 /100\t:  0.27686238288879395 \t14.389315366744995 \t108.92381286621094\n",
      "Batch:\t50 /100\t:  0.287447452545166 \t14.677896738052368 \t109.81712341308594\n",
      "Batch:\t51 /100\t:  0.2743418216705322 \t14.95357632637024 \t48.92670822143555\n",
      "Batch:\t52 /100\t:  0.2778208255767822 \t15.249890327453613 \t196.739013671875\n",
      "Batch:\t53 /100\t:  0.28070950508117676 \t15.531873226165771 \t171.32034301757812\n",
      "Batch:\t54 /100\t:  0.27826428413391113 \t15.812143325805664 \t162.29383850097656\n",
      "Batch:\t55 /100\t:  0.2865293025970459 \t16.099982023239136 \t46.73174285888672\n",
      "Batch:\t56 /100\t:  0.27552032470703125 \t16.376580953598022 \t88.56380462646484\n",
      "Batch:\t57 /100\t:  0.28624558448791504 \t16.66407084465027 \t147.34397888183594\n",
      "Batch:\t58 /100\t:  0.28749942779541016 \t16.952728271484375 \t39.1855583190918\n",
      "Batch:\t59 /100\t:  0.27741456031799316 \t17.23137640953064 \t213.4904327392578\n",
      "Batch:\t60 /100\t:  0.2878429889678955 \t17.520575046539307 \t142.117431640625\n",
      "Batch:\t61 /100\t:  0.2821066379547119 \t17.803725481033325 \t63.34907150268555\n",
      "Batch:\t62 /100\t:  0.28165340423583984 \t18.086637496948242 \t65.63040161132812\n",
      "Batch:\t63 /100\t:  0.28495359420776367 \t18.37298011779785 \t129.4362030029297\n",
      "Batch:\t64 /100\t:  0.2785189151763916 \t18.65294098854065 \t81.78633880615234\n",
      "Batch:\t65 /100\t:  0.2825202941894531 \t18.936989784240723 \t129.84854125976562\n",
      "Batch:\t66 /100\t:  0.29615116119384766 \t19.234530925750732 \t116.08788299560547\n",
      "Batch:\t67 /100\t:  0.3050503730773926 \t19.540672063827515 \t76.8380126953125\n",
      "Batch:\t68 /100\t:  0.3001377582550049 \t19.842710494995117 \t133.6554412841797\n",
      "Batch:\t69 /100\t:  0.28435277938842773 \t20.12803554534912 \t98.18207550048828\n",
      "Batch:\t70 /100\t:  0.28040242195129395 \t20.409521341323853 \t88.91290283203125\n",
      "Batch:\t71 /100\t:  0.28423547744750977 \t20.695050954818726 \t224.5389404296875\n",
      "Batch:\t72 /100\t:  0.28295469284057617 \t20.979554176330566 \t76.68071746826172\n",
      "Batch:\t73 /100\t:  0.28157687187194824 \t21.26214075088501 \t127.43228149414062\n",
      "Batch:\t74 /100\t:  0.2892322540283203 \t21.554280519485474 \t97.80651092529297\n",
      "Batch:\t75 /100\t:  0.2790255546569824 \t21.834674835205078 \t97.23909759521484\n",
      "Batch:\t76 /100\t:  0.30249643325805664 \t22.138767957687378 \t46.428462982177734\n",
      "Batch:\t77 /100\t:  0.28553247451782227 \t22.42582893371582 \t59.77886962890625\n",
      "Batch:\t78 /100\t:  0.2986018657684326 \t22.726162672042847 \t204.7729949951172\n",
      "Batch:\t79 /100\t:  0.2830486297607422 \t23.010271787643433 \t84.99663543701172\n",
      "Batch:\t80 /100\t:  0.3028907775878906 \t23.314736366271973 \t42.41804122924805\n",
      "Batch:\t81 /100\t:  0.2846057415008545 \t23.600406646728516 \t105.93062591552734\n",
      "Batch:\t82 /100\t:  0.288499116897583 \t23.890645027160645 \t89.22640991210938\n",
      "Batch:\t83 /100\t:  0.2770991325378418 \t24.168837785720825 \t137.95913696289062\n",
      "Batch:\t84 /100\t:  0.2836289405822754 \t24.45355248451233 \t112.06790924072266\n",
      "Batch:\t85 /100\t:  0.2789945602416992 \t24.733555793762207 \t108.8709487915039\n",
      "Batch:\t86 /100\t:  0.2817513942718506 \t25.016388654708862 \t89.93590545654297\n",
      "Batch:\t87 /100\t:  0.27724671363830566 \t25.294671297073364 \t138.83958435058594\n",
      "Batch:\t88 /100\t:  0.2874453067779541 \t25.58364248275757 \t102.92389678955078\n",
      "Batch:\t89 /100\t:  0.28159332275390625 \t25.86621642112732 \t100.50740051269531\n",
      "Batch:\t90 /100\t:  0.2784583568572998 \t26.14613437652588 \t122.0009765625\n",
      "Batch:\t91 /100\t:  0.28619933128356934 \t26.433326721191406 \t138.44102478027344\n",
      "Batch:\t92 /100\t:  0.2872045040130615 \t26.72188115119934 \t105.83544921875\n",
      "Batch:\t93 /100\t:  0.28212952613830566 \t27.00500249862671 \t96.17620086669922\n",
      "Batch:\t94 /100\t:  0.2705421447753906 \t27.276808261871338 \t90.95415496826172\n",
      "Batch:\t95 /100\t:  0.28426456451416016 \t27.56204342842102 \t103.81794738769531\n",
      "Batch:\t96 /100\t:  0.28728723526000977 \t27.850731372833252 \t96.11297607421875\n",
      "Batch:\t97 /100\t:  0.2859079837799072 \t28.137636184692383 \t89.51123046875\n",
      "Batch:\t98 /100\t:  0.28873491287231445 \t28.428110122680664 \t63.12541961669922\n",
      "Batch:\t99 /100\t:  0.27837562561035156 \t28.707576036453247 \t156.52752685546875\n",
      "Batch:\t100 /100\t:  0.2831532955169678 \t28.991791248321533 \t105.0806655883789\n",
      "Batch:\t101 /100\t:  0.2826848030090332 \t29.276472091674805 \t162.0582733154297\n",
      "Batch:\t102 /100\t:  0.2857675552368164 \t29.563608646392822 \t154.4520721435547\n",
      "Batch:\t103 /100\t:  0.2824726104736328 \t29.84732413291931 \t78.36412811279297\n",
      "Batch:\t104 /100\t:  0.28682422637939453 \t30.135327577590942 \t103.8514633178711\n",
      "Batch:\t105 /100\t:  0.28709983825683594 \t30.42379665374756 \t91.51763916015625\n",
      "Batch:\t106 /100\t:  0.28051114082336426 \t30.70562505722046 \t94.98831176757812\n",
      "Batch:\t107 /100\t:  0.29091501235961914 \t30.997706413269043 \t167.16494750976562\n",
      "Batch:\t108 /100\t:  0.28743505477905273 \t31.286520957946777 \t83.10456848144531\n",
      "Batch:\t109 /100\t:  0.29796481132507324 \t31.585963487625122 \t128.5587921142578\n",
      "Batch:\t110 /100\t:  0.28650403022766113 \t31.873802185058594 \t110.114013671875\n",
      "Batch:\t111 /100\t:  0.28609657287597656 \t32.16128373146057 \t128.377197265625\n",
      "Batch:\t112 /100\t:  0.28571653366088867 \t32.44820523262024 \t100.85215759277344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t113 /100\t:  0.287686824798584 \t32.73762488365173 \t71.1641616821289\n",
      "Batch:\t114 /100\t:  0.28522276878356934 \t33.024033069610596 \t89.36113739013672\n",
      "Batch:\t115 /100\t:  0.3405892848968506 \t33.368163108825684 \t84.55937957763672\n",
      "Batch:\t116 /100\t:  0.2850637435913086 \t33.6547532081604 \t65.16570281982422\n",
      "Batch:\t117 /100\t:  0.28533339500427246 \t33.94190788269043 \t137.4446563720703\n",
      "Batch:\t118 /100\t:  0.2815992832183838 \t34.224796533584595 \t124.6165542602539\n",
      "Batch:\t119 /100\t:  0.2828707695007324 \t34.509021520614624 \t105.99451446533203\n",
      "Batch:\t120 /100\t:  0.28531432151794434 \t34.795573711395264 \t128.30728149414062\n",
      "Batch:\t121 /100\t:  0.27881669998168945 \t35.07575440406799 \t125.33222198486328\n",
      "Batch:\t122 /100\t:  0.28905820846557617 \t35.36620497703552 \t105.5141372680664\n",
      "Batch:\t123 /100\t:  0.2826104164123535 \t35.650397539138794 \t112.0228500366211\n",
      "Batch:\t124 /100\t:  0.27625441551208496 \t35.92790985107422 \t75.01600646972656\n",
      "Batch:\t125 /100\t:  0.28357481956481934 \t36.213305711746216 \t143.6195526123047\n",
      "Batch:\t126 /100\t:  0.28678011894226074 \t36.50152611732483 \t114.80233001708984\n",
      "Batch:\t127 /100\t:  0.27724432945251465 \t36.78059768676758 \t111.15611267089844\n",
      "Batch:\t128 /100\t:  0.28519344329833984 \t37.06735157966614 \t61.19629669189453\n",
      "Batch:\t129 /100\t:  0.28745555877685547 \t37.356133460998535 \t168.53807067871094\n",
      "Batch:\t130 /100\t:  0.2879045009613037 \t37.64546608924866 \t59.360382080078125\n",
      "Batch:\t131 /100\t:  0.27939462661743164 \t37.926339864730835 \t99.2518310546875\n",
      "Batch:\t132 /100\t:  0.2773253917694092 \t38.20509719848633 \t113.24105834960938\n",
      "Batch:\t133 /100\t:  0.289414644241333 \t38.49589228630066 \t122.87277221679688\n",
      "Batch:\t134 /100\t:  0.2839484214782715 \t38.78084683418274 \t78.78459930419922\n",
      "Batch:\t135 /100\t:  0.2846972942352295 \t39.06707215309143 \t162.0552520751953\n",
      "Batch:\t136 /100\t:  0.2833113670349121 \t39.351409912109375 \t94.04566192626953\n",
      "Batch:\t137 /100\t:  0.28454041481018066 \t39.637091636657715 \t73.47216796875\n",
      "Batch:\t138 /100\t:  0.2873225212097168 \t39.92554068565369 \t92.42967987060547\n",
      "Batch:\t139 /100\t:  0.284743070602417 \t40.21163535118103 \t139.77415466308594\n",
      "Batch:\t140 /100\t:  0.2865891456604004 \t40.50460362434387 \t105.52793884277344\n",
      "Batch:\t141 /100\t:  0.2857961654663086 \t40.79144263267517 \t111.64505004882812\n",
      "Batch:\t142 /100\t:  0.28780364990234375 \t41.080384254455566 \t104.53773498535156\n",
      "Batch:\t143 /100\t:  0.28200697898864746 \t41.36370825767517 \t158.1725616455078\n",
      "Batch:\t144 /100\t:  0.2909078598022461 \t41.655691146850586 \t179.65162658691406\n",
      "Batch:\t145 /100\t:  0.2846546173095703 \t41.9418580532074 \t85.68961334228516\n",
      "Batch:\t146 /100\t:  0.2792232036590576 \t42.222283124923706 \t106.61992645263672\n",
      "Batch:\t147 /100\t:  0.2797849178314209 \t42.50310492515564 \t120.78641510009766\n",
      "Batch:\t148 /100\t:  0.28545284271240234 \t42.78969669342041 \t119.35595703125\n",
      "Batch:\t149 /100\t:  0.27889585494995117 \t43.07007646560669 \t98.27075958251953\n",
      "Batch:\t150 /100\t:  0.28245043754577637 \t43.354599714279175 \t82.7386474609375\n",
      "Batch:\t151 /100\t:  0.2811453342437744 \t43.63673663139343 \t154.59344482421875\n",
      "Batch:\t152 /100\t:  0.2752082347869873 \t43.91313028335571 \t83.88138580322266\n",
      "Batch:\t153 /100\t:  0.28304195404052734 \t44.19724774360657 \t77.56794738769531\n",
      "Batch:\t154 /100\t:  0.2896995544433594 \t44.48843717575073 \t125.36785125732422\n",
      "Batch:\t155 /100\t:  0.2899930477142334 \t44.77950692176819 \t125.90142822265625\n",
      "Batch:\t156 /100\t:  0.2923762798309326 \t45.07317852973938 \t111.05571746826172\n",
      "Batch:\t157 /100\t:  0.2808403968811035 \t45.35530662536621 \t87.86397552490234\n",
      "Batch:\t158 /100\t:  0.2895026206970215 \t45.64642834663391 \t109.39628601074219\n",
      "Batch:\t159 /100\t:  0.2866401672363281 \t45.93429493904114 \t91.78841400146484\n",
      "Batch:\t160 /100\t:  0.291886568069458 \t46.22804522514343 \t101.35295104980469\n",
      "Batch:\t161 /100\t:  0.2877812385559082 \t46.51691770553589 \t92.09722137451172\n",
      "Batch:\t162 /100\t:  0.28677988052368164 \t46.80518317222595 \t112.02471923828125\n",
      "Batch:\t163 /100\t:  0.27512526512145996 \t47.08173990249634 \t124.28617858886719\n",
      "Batch:\t164 /100\t:  0.28776073455810547 \t47.37091088294983 \t80.60740661621094\n",
      "Batch:\t165 /100\t:  0.28502821922302246 \t47.65741324424744 \t117.62242889404297\n",
      "Batch:\t166 /100\t:  0.2822904586791992 \t47.94282364845276 \t88.445068359375\n",
      "Batch:\t167 /100\t:  0.29331278800964355 \t48.23736357688904 \t92.05352783203125\n",
      "Batch:\t168 /100\t:  0.2814445495605469 \t48.52014970779419 \t142.7034149169922\n",
      "Batch:\t169 /100\t:  0.2822742462158203 \t48.80396604537964 \t92.29225158691406\n",
      "Batch:\t170 /100\t:  0.28960084915161133 \t49.09449791908264 \t90.13248443603516\n",
      "Batch:\t171 /100\t:  0.2841026782989502 \t49.379746198654175 \t92.30750274658203\n",
      "Batch:\t172 /100\t:  0.280087947845459 \t49.66083908081055 \t73.56189727783203\n",
      "Batch:\t173 /100\t:  0.284102201461792 \t49.94632911682129 \t97.59687805175781\n",
      "Batch:\t174 /100\t:  0.2837541103363037 \t50.23394799232483 \t109.32624053955078\n",
      "MODEL WEIGHTS RIGHT NOW:  6605.155418395996\n",
      "model with accuracy  0.33 stored at data/models/core_chain/bilstm_dot_ulmfit/lcquad/115/model.torch\n",
      "in model save, no vectors were found.\n",
      "Time: 80.73683452606201\t Loss: 19815.523483276367\t Valdacc: 0.33\t Testacc: 0.335\n",
      " BestValidAcc: 0.33\n",
      " BestTestAcc: 0.335\n",
      "\n",
      "Epoch:  4 / 200\n",
      "Batch:\t0 /100\t:  0.30569982528686523 \t0.3073866367340088 \t64.29025268554688\n",
      "Batch:\t1 /100\t:  0.28406310081481934 \t0.5927343368530273 \t74.5202407836914\n",
      "Batch:\t2 /100\t:  0.28033947944641113 \t0.8740525245666504 \t109.07707214355469\n",
      "Batch:\t3 /100\t:  0.2752225399017334 \t1.150575876235962 \t61.23875427246094\n",
      "Batch:\t4 /100\t:  0.28635573387145996 \t1.4383063316345215 \t117.30261993408203\n",
      "Batch:\t5 /100\t:  0.2930717468261719 \t1.7330715656280518 \t83.67866516113281\n",
      "Batch:\t6 /100\t:  0.2856721878051758 \t2.0198161602020264 \t99.03755187988281\n",
      "Batch:\t7 /100\t:  0.2869110107421875 \t2.3085954189300537 \t119.80821990966797\n",
      "Batch:\t8 /100\t:  0.28252649307250977 \t2.592207193374634 \t72.4932861328125\n",
      "Batch:\t9 /100\t:  0.2857654094696045 \t2.8797199726104736 \t136.3130645751953\n",
      "Batch:\t10 /100\t:  0.2799210548400879 \t3.160623788833618 \t88.56007385253906\n",
      "Batch:\t11 /100\t:  0.27925705909729004 \t3.441371440887451 \t84.28081512451172\n",
      "Batch:\t12 /100\t:  0.2808239459991455 \t3.7231762409210205 \t80.23126983642578\n",
      "Batch:\t13 /100\t:  0.2898411750793457 \t4.014073610305786 \t135.85171508789062\n",
      "Batch:\t14 /100\t:  0.2848043441772461 \t4.299877405166626 \t73.41381072998047\n",
      "Batch:\t15 /100\t:  0.2721123695373535 \t4.573549747467041 \t97.37010192871094\n",
      "Batch:\t16 /100\t:  0.2863786220550537 \t4.864630222320557 \t70.78690338134766\n",
      "Batch:\t17 /100\t:  0.277219295501709 \t5.143001079559326 \t49.44288635253906\n",
      "Batch:\t18 /100\t:  0.27881646156311035 \t5.423083066940308 \t90.5095443725586\n",
      "Batch:\t19 /100\t:  0.2823467254638672 \t5.706693172454834 \t131.6565704345703\n",
      "Batch:\t20 /100\t:  0.2857048511505127 \t5.993677377700806 \t138.94837951660156\n",
      "Batch:\t21 /100\t:  0.30066418647766113 \t6.296152353286743 \t121.58182525634766\n",
      "Batch:\t22 /100\t:  0.3122069835662842 \t6.609923362731934 \t81.69273376464844\n",
      "Batch:\t23 /100\t:  0.2830030918121338 \t6.8944056034088135 \t36.87044906616211\n",
      "Batch:\t24 /100\t:  0.28727006912231445 \t7.182911396026611 \t70.54269409179688\n",
      "Batch:\t25 /100\t:  0.27856993675231934 \t7.462567090988159 \t169.1715087890625\n",
      "Batch:\t26 /100\t:  0.28469228744506836 \t7.748542547225952 \t121.11793518066406\n",
      "Batch:\t27 /100\t:  0.278961181640625 \t8.028552770614624 \t90.51068878173828\n",
      "Batch:\t28 /100\t:  0.282912015914917 \t8.312873601913452 \t181.70584106445312\n",
      "Batch:\t29 /100\t:  0.28766655921936035 \t8.601594686508179 \t111.70220947265625\n",
      "Batch:\t30 /100\t:  0.2777433395385742 \t8.880525588989258 \t103.7311019897461\n",
      "Batch:\t31 /100\t:  0.2819795608520508 \t9.163598537445068 \t77.54449462890625\n",
      "Batch:\t32 /100\t:  0.2841939926147461 \t9.448885202407837 \t81.22018432617188\n",
      "Batch:\t33 /100\t:  0.27459216117858887 \t9.724709749221802 \t70.01883697509766\n",
      "Batch:\t34 /100\t:  0.28563451766967773 \t10.011844158172607 \t173.9352264404297\n",
      "Batch:\t35 /100\t:  0.29228758811950684 \t10.305306673049927 \t81.92996978759766\n",
      "Batch:\t36 /100\t:  0.2877976894378662 \t10.59464430809021 \t51.52232360839844\n",
      "Batch:\t37 /100\t:  0.28789305686950684 \t10.883816719055176 \t71.21758270263672\n",
      "Batch:\t38 /100\t:  0.28488731384277344 \t11.170248031616211 \t125.84699249267578\n",
      "Batch:\t39 /100\t:  0.2789428234100342 \t11.450440406799316 \t149.24864196777344\n",
      "Batch:\t40 /100\t:  0.28482532501220703 \t11.736291408538818 \t61.775821685791016\n",
      "Batch:\t41 /100\t:  0.28829431533813477 \t12.025704383850098 \t69.07864379882812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t42 /100\t:  0.28235411643981934 \t12.309567213058472 \t118.67301177978516\n",
      "Batch:\t43 /100\t:  0.28759336471557617 \t12.598316669464111 \t67.47325897216797\n",
      "Batch:\t44 /100\t:  0.2840590476989746 \t12.884153604507446 \t56.108673095703125\n",
      "Batch:\t45 /100\t:  0.28110790252685547 \t13.166410446166992 \t97.39665985107422\n",
      "Batch:\t46 /100\t:  0.28095030784606934 \t13.448912620544434 \t85.00922393798828\n",
      "Batch:\t47 /100\t:  0.28129100799560547 \t13.731199502944946 \t225.37632751464844\n",
      "Batch:\t48 /100\t:  0.2841958999633789 \t14.016849994659424 \t210.6260528564453\n",
      "Batch:\t49 /100\t:  0.28212976455688477 \t14.299957275390625 \t70.35845184326172\n",
      "Batch:\t50 /100\t:  0.28951287269592285 \t14.59107255935669 \t89.4571762084961\n",
      "Batch:\t51 /100\t:  0.28716492652893066 \t14.87921929359436 \t63.45964050292969\n",
      "Batch:\t52 /100\t:  0.2897524833679199 \t15.170016050338745 \t132.7263641357422\n",
      "Batch:\t53 /100\t:  0.2854282855987549 \t15.456417083740234 \t109.76377868652344\n",
      "Batch:\t54 /100\t:  0.2853844165802002 \t15.743288516998291 \t63.73508071899414\n",
      "Batch:\t55 /100\t:  0.28545355796813965 \t16.029740810394287 \t70.76972961425781\n",
      "Batch:\t56 /100\t:  0.2795443534851074 \t16.310352087020874 \t78.07142639160156\n",
      "Batch:\t57 /100\t:  0.27982497215270996 \t16.591336727142334 \t109.94465637207031\n",
      "Batch:\t58 /100\t:  0.2846717834472656 \t16.877505779266357 \t143.27589416503906\n",
      "Batch:\t59 /100\t:  0.2829170227050781 \t17.16241765022278 \t77.40091705322266\n",
      "Batch:\t60 /100\t:  0.2810540199279785 \t17.444873094558716 \t107.78761291503906\n",
      "Batch:\t61 /100\t:  0.286379337310791 \t17.732433795928955 \t123.53923797607422\n",
      "Batch:\t62 /100\t:  0.28632235527038574 \t18.019916534423828 \t66.07923889160156\n",
      "Batch:\t63 /100\t:  0.27701401710510254 \t18.298071146011353 \t110.84334564208984\n",
      "Batch:\t64 /100\t:  0.2913639545440674 \t18.590622901916504 \t70.4300537109375\n",
      "Batch:\t65 /100\t:  0.28719067573547363 \t18.88469386100769 \t109.49034881591797\n",
      "Batch:\t66 /100\t:  0.28620219230651855 \t19.172401428222656 \t49.30805206298828\n",
      "Batch:\t67 /100\t:  0.29218459129333496 \t19.466065168380737 \t67.21680450439453\n",
      "Batch:\t68 /100\t:  0.28594160079956055 \t19.754154920578003 \t167.02410888671875\n",
      "Batch:\t69 /100\t:  0.27886271476745605 \t20.03424072265625 \t49.19721603393555\n",
      "Batch:\t70 /100\t:  0.28298425674438477 \t20.31829571723938 \t32.594276428222656\n",
      "Batch:\t71 /100\t:  0.2872164249420166 \t20.606964349746704 \t146.49661254882812\n",
      "Batch:\t72 /100\t:  0.2855567932128906 \t20.89356827735901 \t105.96302795410156\n",
      "Batch:\t73 /100\t:  0.27762770652770996 \t21.172457933425903 \t142.9672088623047\n",
      "Batch:\t74 /100\t:  0.28723978996276855 \t21.460862398147583 \t142.3670196533203\n",
      "Batch:\t75 /100\t:  0.2920973300933838 \t21.765150785446167 \t60.966922760009766\n",
      "Batch:\t76 /100\t:  0.28620386123657227 \t22.052830696105957 \t57.929012298583984\n",
      "Batch:\t77 /100\t:  0.2869069576263428 \t22.340898275375366 \t128.21849060058594\n",
      "Batch:\t78 /100\t:  0.2929084300994873 \t22.635031938552856 \t57.49209976196289\n",
      "Batch:\t79 /100\t:  0.32140088081359863 \t22.957668781280518 \t84.3438491821289\n",
      "Batch:\t80 /100\t:  0.2817842960357666 \t23.240976333618164 \t94.4499282836914\n",
      "Batch:\t81 /100\t:  0.2888016700744629 \t23.53096890449524 \t106.62136840820312\n",
      "Batch:\t82 /100\t:  0.28464651107788086 \t23.81686806678772 \t110.46815490722656\n",
      "Batch:\t83 /100\t:  0.28232431411743164 \t24.1005756855011 \t53.577091217041016\n",
      "Batch:\t84 /100\t:  0.28226161003112793 \t24.38427448272705 \t85.56861877441406\n",
      "Batch:\t85 /100\t:  0.2901651859283447 \t24.67561626434326 \t149.79879760742188\n",
      "Batch:\t86 /100\t:  0.2854430675506592 \t24.96239185333252 \t188.45838928222656\n",
      "Batch:\t87 /100\t:  0.29047679901123047 \t25.25486421585083 \t114.29430389404297\n",
      "Batch:\t88 /100\t:  0.29142212867736816 \t25.54736328125 \t88.78363037109375\n",
      "Batch:\t89 /100\t:  0.29410839080810547 \t25.84348964691162 \t78.16162109375\n",
      "Batch:\t90 /100\t:  0.2879757881164551 \t26.13252305984497 \t96.75527954101562\n",
      "Batch:\t91 /100\t:  0.28868603706359863 \t26.422683715820312 \t127.42819213867188\n",
      "Batch:\t92 /100\t:  0.28758907318115234 \t26.711594581604004 \t81.37801361083984\n",
      "Batch:\t93 /100\t:  0.275998592376709 \t26.989069938659668 \t99.6976547241211\n",
      "Batch:\t94 /100\t:  0.28826379776000977 \t27.278265953063965 \t91.83651733398438\n",
      "Batch:\t95 /100\t:  0.2852475643157959 \t27.564846515655518 \t83.38783264160156\n",
      "Batch:\t96 /100\t:  0.2837941646575928 \t27.84959578514099 \t88.79457092285156\n",
      "Batch:\t97 /100\t:  0.27848339080810547 \t28.129112482070923 \t71.82548522949219\n",
      "Batch:\t98 /100\t:  0.2834925651550293 \t28.413679599761963 \t87.28998565673828\n",
      "Batch:\t99 /100\t:  0.28372955322265625 \t28.698686838150024 \t106.39745330810547\n",
      "Batch:\t100 /100\t:  0.28380680084228516 \t28.983511686325073 \t140.6173095703125\n",
      "Batch:\t101 /100\t:  0.28588128089904785 \t29.270582914352417 \t98.88691711425781\n",
      "Batch:\t102 /100\t:  0.2848680019378662 \t29.55650305747986 \t64.29472351074219\n",
      "Batch:\t103 /100\t:  0.31185245513916016 \t29.86980652809143 \t97.0779037475586\n",
      "Batch:\t104 /100\t:  0.30391764640808105 \t30.174795150756836 \t93.110595703125\n",
      "Batch:\t105 /100\t:  0.2877020835876465 \t30.463554620742798 \t75.66558837890625\n",
      "Batch:\t106 /100\t:  0.2924971580505371 \t30.757560968399048 \t106.98812103271484\n",
      "Batch:\t107 /100\t:  0.28383779525756836 \t31.042588233947754 \t91.29261779785156\n",
      "Batch:\t108 /100\t:  0.2823364734649658 \t31.325989723205566 \t68.45641326904297\n",
      "Batch:\t109 /100\t:  0.28063106536865234 \t31.607957363128662 \t97.84046936035156\n",
      "Batch:\t110 /100\t:  0.28545212745666504 \t31.894471883773804 \t39.204349517822266\n",
      "Batch:\t111 /100\t:  0.3132174015045166 \t32.20893478393555 \t123.94341278076172\n",
      "Batch:\t112 /100\t:  0.2834174633026123 \t32.493589639663696 \t92.97306823730469\n",
      "Batch:\t113 /100\t:  0.28254175186157227 \t32.7778160572052 \t124.7625961303711\n",
      "Batch:\t114 /100\t:  0.2784395217895508 \t33.05764627456665 \t158.90164184570312\n",
      "Batch:\t115 /100\t:  0.2980632781982422 \t33.35737061500549 \t145.4180145263672\n",
      "Batch:\t116 /100\t:  0.27463507652282715 \t33.6334753036499 \t94.67646026611328\n",
      "Batch:\t117 /100\t:  0.2949061393737793 \t33.93131494522095 \t162.10748291015625\n",
      "Batch:\t118 /100\t:  0.280383825302124 \t34.213008642196655 \t111.19125366210938\n",
      "Batch:\t119 /100\t:  0.286191463470459 \t34.50098705291748 \t40.767822265625\n",
      "Batch:\t120 /100\t:  0.28833723068237305 \t34.79045343399048 \t81.94844818115234\n",
      "Batch:\t121 /100\t:  0.2865457534790039 \t35.07840085029602 \t67.20576477050781\n",
      "Batch:\t122 /100\t:  0.27940940856933594 \t35.35895013809204 \t79.0029067993164\n",
      "Batch:\t123 /100\t:  0.27830076217651367 \t35.638599157333374 \t73.1420669555664\n",
      "Batch:\t124 /100\t:  0.28415894508361816 \t35.92375683784485 \t103.27340698242188\n",
      "Batch:\t125 /100\t:  0.2795116901397705 \t36.20479154586792 \t75.08148956298828\n",
      "Batch:\t126 /100\t:  0.2819375991821289 \t36.487793922424316 \t104.68403625488281\n",
      "Batch:\t127 /100\t:  0.29507994651794434 \t36.78409790992737 \t84.98745727539062\n",
      "Batch:\t128 /100\t:  0.28366684913635254 \t37.068851470947266 \t95.65348815917969\n",
      "Batch:\t129 /100\t:  0.2852599620819092 \t37.355639934539795 \t59.51017761230469\n",
      "Batch:\t130 /100\t:  0.2803013324737549 \t37.637051820755005 \t70.22261810302734\n",
      "Batch:\t131 /100\t:  0.2878105640411377 \t37.92662262916565 \t63.385929107666016\n",
      "Batch:\t132 /100\t:  0.28380727767944336 \t38.2117133140564 \t76.44142150878906\n",
      "Batch:\t133 /100\t:  0.29161763191223145 \t38.504849910736084 \t59.62421798706055\n",
      "Batch:\t134 /100\t:  0.2801830768585205 \t38.78601837158203 \t105.9102554321289\n",
      "Batch:\t135 /100\t:  0.291363000869751 \t39.07912278175354 \t89.51748657226562\n",
      "Batch:\t136 /100\t:  0.28917789459228516 \t39.374874114990234 \t105.69609069824219\n",
      "Batch:\t137 /100\t:  0.2825186252593994 \t39.658974170684814 \t99.20591735839844\n",
      "Batch:\t138 /100\t:  0.28014469146728516 \t39.94028854370117 \t138.7416534423828\n",
      "Batch:\t139 /100\t:  0.28234004974365234 \t40.22412371635437 \t65.55096435546875\n",
      "Batch:\t140 /100\t:  0.2848634719848633 \t40.510823249816895 \t75.79234313964844\n",
      "Batch:\t141 /100\t:  0.2849397659301758 \t40.79728078842163 \t80.50540924072266\n",
      "Batch:\t142 /100\t:  0.28504371643066406 \t41.08364987373352 \t79.15042877197266\n",
      "Batch:\t143 /100\t:  0.2928144931793213 \t41.37758255004883 \t96.82821655273438\n",
      "Batch:\t144 /100\t:  0.28292131423950195 \t41.66250014305115 \t111.32621765136719\n",
      "Batch:\t145 /100\t:  0.2817957401275635 \t41.94546461105347 \t74.67942810058594\n",
      "Batch:\t146 /100\t:  0.2804553508758545 \t42.227158546447754 \t113.92959594726562\n",
      "Batch:\t147 /100\t:  0.27959108352661133 \t42.507861614227295 \t87.60775756835938\n",
      "Batch:\t148 /100\t:  0.3098902702331543 \t42.81953239440918 \t97.18988800048828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t149 /100\t:  0.28398966789245605 \t43.104522466659546 \t103.7620849609375\n",
      "Batch:\t150 /100\t:  0.284726619720459 \t43.3909125328064 \t87.00450897216797\n",
      "Batch:\t151 /100\t:  0.2762794494628906 \t43.668394804000854 \t129.5108642578125\n",
      "Batch:\t152 /100\t:  0.28727245330810547 \t43.95707583427429 \t54.47871780395508\n",
      "Batch:\t153 /100\t:  0.28612732887268066 \t44.24497175216675 \t66.72395324707031\n",
      "Batch:\t154 /100\t:  0.2836785316467285 \t44.53045654296875 \t73.89005279541016\n",
      "Batch:\t155 /100\t:  0.28339624404907227 \t44.81532549858093 \t89.38955688476562\n",
      "Batch:\t156 /100\t:  0.28037309646606445 \t45.09705114364624 \t155.76463317871094\n",
      "Batch:\t157 /100\t:  0.2805948257446289 \t45.378865242004395 \t120.76791381835938\n",
      "Batch:\t158 /100\t:  0.28336548805236816 \t45.663652181625366 \t77.0235595703125\n",
      "Batch:\t159 /100\t:  0.289264440536499 \t45.95396614074707 \t132.5420684814453\n",
      "Batch:\t160 /100\t:  0.2871890068054199 \t46.242701053619385 \t117.29647064208984\n",
      "Batch:\t161 /100\t:  0.28595590591430664 \t46.52989196777344 \t81.84172058105469\n",
      "Batch:\t162 /100\t:  0.2882208824157715 \t46.819735050201416 \t72.67372131347656\n",
      "Batch:\t163 /100\t:  0.2913684844970703 \t47.11224341392517 \t81.19180297851562\n",
      "Batch:\t164 /100\t:  0.2854630947113037 \t47.39899015426636 \t88.10807037353516\n",
      "Batch:\t165 /100\t:  0.28414154052734375 \t47.684205293655396 \t89.92680358886719\n",
      "Batch:\t166 /100\t:  0.28600072860717773 \t47.97160220146179 \t61.10945510864258\n",
      "Batch:\t167 /100\t:  0.27921414375305176 \t48.252562522888184 \t73.7293472290039\n",
      "Batch:\t168 /100\t:  0.280658483505249 \t48.534393072128296 \t93.7957992553711\n",
      "Batch:\t169 /100\t:  0.2877640724182129 \t48.82343816757202 \t95.64545440673828\n",
      "Batch:\t170 /100\t:  0.2875797748565674 \t49.112706422805786 \t64.5609130859375\n",
      "Batch:\t171 /100\t:  0.29235363006591797 \t49.40678405761719 \t81.68836212158203\n",
      "Batch:\t172 /100\t:  0.28139495849609375 \t49.689436197280884 \t76.4688949584961\n",
      "Batch:\t173 /100\t:  0.2912757396697998 \t49.98200702667236 \t61.04489517211914\n",
      "Batch:\t174 /100\t:  0.2854740619659424 \t50.26823353767395 \t107.36410522460938\n",
      "Time: 77.7384717464447\t Loss: 16716.906829833984\t Valdacc: 0.33\t Testacc: 0.325\n",
      " BestValidAcc: 0.33\n",
      " BestTestAcc: 0.335\n",
      "\n",
      "Epoch:  5 / 200\n",
      "Batch:\t0 /100\t:  0.28955602645874023 \t0.2935934066772461 \t112.12294006347656\n",
      "Batch:\t1 /100\t:  0.28903913497924805 \t0.5845541954040527 \t89.95445251464844\n",
      "Batch:\t2 /100\t:  0.2858564853668213 \t0.8713836669921875 \t107.67215728759766\n",
      "Batch:\t3 /100\t:  0.3169996738433838 \t1.1902036666870117 \t99.25225830078125\n",
      "Batch:\t4 /100\t:  0.2876701354980469 \t1.4789342880249023 \t67.34280395507812\n",
      "Batch:\t5 /100\t:  0.276658296585083 \t1.757218599319458 \t89.42233276367188\n",
      "Batch:\t6 /100\t:  0.28499889373779297 \t2.0436644554138184 \t63.199581146240234\n",
      "Batch:\t7 /100\t:  0.2776496410369873 \t2.323674201965332 \t106.45774841308594\n",
      "Batch:\t8 /100\t:  0.28743648529052734 \t2.612210988998413 \t102.0156021118164\n",
      "Batch:\t9 /100\t:  0.2949485778808594 \t2.908740520477295 \t84.43050384521484\n",
      "Batch:\t10 /100\t:  0.27800536155700684 \t3.187826633453369 \t110.10774993896484\n",
      "Batch:\t11 /100\t:  0.3020918369293213 \t3.4911866188049316 \t183.08335876464844\n",
      "Batch:\t12 /100\t:  0.2827601432800293 \t3.7752881050109863 \t87.52725982666016\n",
      "Batch:\t13 /100\t:  0.2898094654083252 \t4.066371202468872 \t108.48755645751953\n",
      "Batch:\t14 /100\t:  0.2802128791809082 \t4.347787857055664 \t100.7216796875\n",
      "Batch:\t15 /100\t:  0.2797994613647461 \t4.629366874694824 \t67.35496520996094\n",
      "Batch:\t16 /100\t:  0.2821846008300781 \t4.913045167922974 \t84.31795501708984\n",
      "Batch:\t17 /100\t:  0.27721524238586426 \t5.191570520401001 \t45.143707275390625\n",
      "Batch:\t18 /100\t:  0.2925081253051758 \t5.485507249832153 \t45.549835205078125\n",
      "Batch:\t19 /100\t:  0.2857339382171631 \t5.773043155670166 \t80.76350402832031\n",
      "Batch:\t20 /100\t:  0.28635406494140625 \t6.060735702514648 \t74.4369125366211\n",
      "Batch:\t21 /100\t:  0.2836425304412842 \t6.34597110748291 \t123.63908386230469\n",
      "Batch:\t22 /100\t:  0.29108166694641113 \t6.638385772705078 \t79.03399658203125\n",
      "Batch:\t23 /100\t:  0.27719664573669434 \t6.917012691497803 \t120.74195098876953\n",
      "Batch:\t24 /100\t:  0.2822597026824951 \t7.200249433517456 \t68.4786605834961\n",
      "Batch:\t25 /100\t:  0.2828850746154785 \t7.484118700027466 \t126.32543182373047\n",
      "Batch:\t26 /100\t:  0.28341102600097656 \t7.768507242202759 \t125.47220611572266\n",
      "Batch:\t27 /100\t:  0.28737568855285645 \t8.057061910629272 \t80.79588317871094\n",
      "Batch:\t28 /100\t:  0.287153959274292 \t8.34947419166565 \t73.43724060058594\n",
      "Batch:\t29 /100\t:  0.2791745662689209 \t8.629913806915283 \t84.9984130859375\n",
      "Batch:\t30 /100\t:  0.30447888374328613 \t8.940916061401367 \t97.54370880126953\n",
      "Batch:\t31 /100\t:  0.28919339179992676 \t9.231570720672607 \t75.47129821777344\n",
      "Batch:\t32 /100\t:  0.284102201461792 \t9.517116785049438 \t53.331539154052734\n",
      "Batch:\t33 /100\t:  0.2839970588684082 \t9.802515983581543 \t108.49620819091797\n",
      "Batch:\t34 /100\t:  0.2930428981781006 \t10.096975564956665 \t85.75037384033203\n",
      "Batch:\t35 /100\t:  0.2837843894958496 \t10.382248401641846 \t64.13568878173828\n",
      "Batch:\t36 /100\t:  0.27794718742370605 \t10.661253690719604 \t89.81238555908203\n",
      "Batch:\t37 /100\t:  0.29094624519348145 \t10.95370078086853 \t112.05176544189453\n",
      "Batch:\t38 /100\t:  0.28790879249572754 \t11.242965936660767 \t73.31426239013672\n",
      "Batch:\t39 /100\t:  0.28639912605285645 \t11.530369281768799 \t70.11885070800781\n",
      "Batch:\t40 /100\t:  0.28339338302612305 \t11.815169095993042 \t68.45623016357422\n",
      "Batch:\t41 /100\t:  0.2945070266723633 \t12.111626863479614 \t72.38057708740234\n",
      "Batch:\t42 /100\t:  0.2889847755432129 \t12.402129173278809 \t99.08710479736328\n",
      "Batch:\t43 /100\t:  0.2856016159057617 \t12.69029974937439 \t154.12777709960938\n",
      "Batch:\t44 /100\t:  0.2865931987762451 \t12.978473424911499 \t99.02799987792969\n",
      "Batch:\t45 /100\t:  0.28162622451782227 \t13.261317014694214 \t83.3709945678711\n",
      "Batch:\t46 /100\t:  0.28090715408325195 \t13.543324708938599 \t82.83786010742188\n",
      "Batch:\t47 /100\t:  0.2796823978424072 \t13.824006795883179 \t95.69766998291016\n",
      "Batch:\t48 /100\t:  0.28588128089904785 \t14.11094856262207 \t80.52264404296875\n",
      "Batch:\t49 /100\t:  0.30722522735595703 \t14.419659852981567 \t84.62792205810547\n",
      "Batch:\t50 /100\t:  0.3042302131652832 \t14.72498607635498 \t87.43363952636719\n",
      "Batch:\t51 /100\t:  0.28068065643310547 \t15.006874799728394 \t85.38433837890625\n",
      "Batch:\t52 /100\t:  0.287905216217041 \t15.296252727508545 \t107.60702514648438\n",
      "Batch:\t53 /100\t:  0.27861690521240234 \t15.57630443572998 \t71.3458023071289\n",
      "Batch:\t54 /100\t:  0.28081345558166504 \t15.858513593673706 \t79.89370727539062\n",
      "Batch:\t55 /100\t:  0.28507089614868164 \t16.145060062408447 \t73.26282501220703\n",
      "Batch:\t56 /100\t:  0.28475046157836914 \t16.43077802658081 \t84.38204956054688\n",
      "Batch:\t57 /100\t:  0.2926499843597412 \t16.724956274032593 \t83.12544250488281\n",
      "Batch:\t58 /100\t:  0.2817399501800537 \t17.00825548171997 \t76.58841705322266\n",
      "Batch:\t59 /100\t:  0.28411269187927246 \t17.293917655944824 \t100.05604553222656\n",
      "Batch:\t60 /100\t:  0.2824978828430176 \t17.577452182769775 \t77.29652404785156\n",
      "Batch:\t61 /100\t:  0.292248010635376 \t17.871066331863403 \t101.67261505126953\n",
      "Batch:\t62 /100\t:  0.28284263610839844 \t18.1549551486969 \t88.23318481445312\n",
      "Batch:\t63 /100\t:  0.2913217544555664 \t18.447930335998535 \t69.8111343383789\n",
      "Batch:\t64 /100\t:  0.2792496681213379 \t18.72818398475647 \t56.311927795410156\n",
      "Batch:\t65 /100\t:  0.2826683521270752 \t19.011969089508057 \t136.94003295898438\n",
      "Batch:\t66 /100\t:  0.2898752689361572 \t19.302901029586792 \t59.214759826660156\n",
      "Batch:\t67 /100\t:  0.2873654365539551 \t19.592036485671997 \t81.78938293457031\n",
      "Batch:\t68 /100\t:  0.28352928161621094 \t19.876564025878906 \t126.5251235961914\n",
      "Batch:\t69 /100\t:  0.2811605930328369 \t20.159008741378784 \t86.66578674316406\n",
      "Batch:\t70 /100\t:  0.28389620780944824 \t20.443840265274048 \t83.57268524169922\n",
      "Batch:\t71 /100\t:  0.28888964653015137 \t20.733933925628662 \t101.33055877685547\n",
      "Batch:\t72 /100\t:  0.2808971405029297 \t21.016053915023804 \t56.154945373535156\n",
      "Batch:\t73 /100\t:  0.28558778762817383 \t21.303109407424927 \t71.57536315917969\n",
      "Batch:\t74 /100\t:  0.28509974479675293 \t21.591098308563232 \t89.11268615722656\n",
      "Batch:\t75 /100\t:  0.2905259132385254 \t21.88288116455078 \t34.823360443115234\n",
      "Batch:\t76 /100\t:  0.28376007080078125 \t22.168195486068726 \t56.86408996582031\n",
      "Batch:\t77 /100\t:  0.298229455947876 \t22.467779636383057 \t89.78319549560547\n",
      "Batch:\t78 /100\t:  0.28273987770080566 \t22.757219791412354 \t54.750579833984375\n",
      "Batch:\t79 /100\t:  0.28786277770996094 \t23.046705722808838 \t54.430511474609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t80 /100\t:  0.28598761558532715 \t23.333865880966187 \t125.3939437866211\n",
      "Batch:\t81 /100\t:  0.2804267406463623 \t23.61560082435608 \t62.14924621582031\n",
      "Batch:\t82 /100\t:  0.28045058250427246 \t23.898257732391357 \t103.91008758544922\n",
      "Batch:\t83 /100\t:  0.2856738567352295 \t24.18548011779785 \t73.37602996826172\n",
      "Batch:\t84 /100\t:  0.2889134883880615 \t24.475658893585205 \t54.07710266113281\n",
      "Batch:\t85 /100\t:  0.2868778705596924 \t24.764184713363647 \t105.18498992919922\n",
      "Batch:\t86 /100\t:  0.2804298400878906 \t25.04598307609558 \t69.46707916259766\n",
      "Batch:\t87 /100\t:  0.285797119140625 \t25.333202838897705 \t77.98259735107422\n",
      "Batch:\t88 /100\t:  0.286759614944458 \t25.621427536010742 \t72.1031494140625\n",
      "Batch:\t89 /100\t:  0.2861766815185547 \t25.908990383148193 \t94.52405548095703\n",
      "Batch:\t90 /100\t:  0.2952420711517334 \t26.206372022628784 \t75.16038513183594\n",
      "Batch:\t91 /100\t:  0.2900075912475586 \t26.497451305389404 \t101.90608215332031\n",
      "Batch:\t92 /100\t:  0.2883620262145996 \t26.787691593170166 \t63.59376525878906\n",
      "Batch:\t93 /100\t:  0.2895824909210205 \t27.07837986946106 \t89.37342834472656\n",
      "Batch:\t94 /100\t:  0.3008005619049072 \t27.39491891860962 \t60.255027770996094\n",
      "Batch:\t95 /100\t:  0.28039002418518066 \t27.676433086395264 \t104.91970825195312\n",
      "Batch:\t96 /100\t:  0.29035329818725586 \t27.96840524673462 \t98.68030548095703\n",
      "Batch:\t97 /100\t:  0.2884490489959717 \t28.25781536102295 \t87.48951721191406\n",
      "Batch:\t98 /100\t:  0.28072547912597656 \t28.540019035339355 \t49.81479263305664\n",
      "Batch:\t99 /100\t:  0.28008413314819336 \t28.821579456329346 \t53.95646286010742\n",
      "Batch:\t100 /100\t:  0.28642940521240234 \t29.109772205352783 \t90.25044250488281\n",
      "Batch:\t101 /100\t:  0.28385138511657715 \t29.394789457321167 \t74.22989654541016\n",
      "Batch:\t102 /100\t:  0.2761349678039551 \t29.67227077484131 \t79.79415893554688\n",
      "Batch:\t103 /100\t:  0.2803037166595459 \t29.953584909439087 \t97.6092300415039\n",
      "Batch:\t104 /100\t:  0.28287172317504883 \t30.23784613609314 \t84.31985473632812\n",
      "Batch:\t105 /100\t:  0.2852904796600342 \t30.524128437042236 \t64.74339294433594\n",
      "Batch:\t106 /100\t:  0.283109188079834 \t30.80854558944702 \t101.24539947509766\n",
      "Batch:\t107 /100\t:  0.287334680557251 \t31.0970139503479 \t52.842098236083984\n",
      "Batch:\t108 /100\t:  0.2732717990875244 \t31.37127184867859 \t50.698368072509766\n",
      "Batch:\t109 /100\t:  0.284942626953125 \t31.657371282577515 \t79.4640884399414\n",
      "Batch:\t110 /100\t:  0.28873181343078613 \t31.947317600250244 \t162.24932861328125\n",
      "Batch:\t111 /100\t:  0.28140878677368164 \t32.230019330978394 \t73.63214111328125\n",
      "Batch:\t112 /100\t:  0.28510046005249023 \t32.51668095588684 \t120.27587127685547\n",
      "Batch:\t113 /100\t:  0.27685117721557617 \t32.79474425315857 \t103.03094482421875\n",
      "Batch:\t114 /100\t:  0.29059314727783203 \t33.08648085594177 \t130.06173706054688\n",
      "Batch:\t115 /100\t:  0.28655076026916504 \t33.37454605102539 \t53.20475769042969\n",
      "Batch:\t116 /100\t:  0.2831428050994873 \t33.65916919708252 \t79.04927062988281\n",
      "Batch:\t117 /100\t:  0.3177351951599121 \t33.97914242744446 \t124.11490631103516\n",
      "Batch:\t118 /100\t:  0.28566503524780273 \t34.26652455329895 \t79.6358871459961\n",
      "Batch:\t119 /100\t:  0.28110337257385254 \t34.54903721809387 \t76.01829528808594\n",
      "Batch:\t120 /100\t:  0.28211379051208496 \t34.83222985267639 \t65.57528686523438\n",
      "Batch:\t121 /100\t:  0.2849085330963135 \t35.11870980262756 \t82.22637939453125\n",
      "Batch:\t122 /100\t:  0.28882646560668945 \t35.408596992492676 \t81.97432708740234\n",
      "Batch:\t123 /100\t:  0.28169703483581543 \t35.69189429283142 \t90.3204574584961\n",
      "Batch:\t124 /100\t:  0.2839372158050537 \t35.977375745773315 \t92.31112670898438\n",
      "Batch:\t125 /100\t:  0.28535962104797363 \t36.264453172683716 \t96.68460083007812\n",
      "Batch:\t126 /100\t:  0.28444933891296387 \t36.550052881240845 \t59.88429641723633\n",
      "Batch:\t127 /100\t:  0.2854926586151123 \t36.83676719665527 \t66.51222229003906\n",
      "Batch:\t128 /100\t:  0.2788379192352295 \t37.11655235290527 \t78.68877410888672\n",
      "Batch:\t129 /100\t:  0.2783503532409668 \t37.395989418029785 \t74.8263931274414\n",
      "Batch:\t130 /100\t:  0.28457164764404297 \t37.681551933288574 \t105.59913635253906\n",
      "Batch:\t131 /100\t:  0.28659844398498535 \t37.969610929489136 \t110.98426818847656\n",
      "Batch:\t132 /100\t:  0.28104400634765625 \t38.25170040130615 \t119.0086669921875\n",
      "Batch:\t133 /100\t:  0.29275989532470703 \t38.54606008529663 \t69.8391342163086\n",
      "Batch:\t134 /100\t:  0.28224730491638184 \t38.82963228225708 \t61.967926025390625\n",
      "Batch:\t135 /100\t:  0.2859926223754883 \t39.116976261138916 \t103.91632080078125\n",
      "Batch:\t136 /100\t:  0.27585482597351074 \t39.39395260810852 \t84.2852554321289\n",
      "Batch:\t137 /100\t:  0.283130407333374 \t39.67802834510803 \t75.847900390625\n",
      "Batch:\t138 /100\t:  0.28737568855285645 \t39.96662878990173 \t77.182373046875\n",
      "Batch:\t139 /100\t:  0.2836151123046875 \t40.25153946876526 \t73.89024353027344\n",
      "Batch:\t140 /100\t:  0.29367756843566895 \t40.54651665687561 \t139.12185668945312\n",
      "Batch:\t141 /100\t:  0.29483938217163086 \t40.842721700668335 \t55.057125091552734\n",
      "Batch:\t142 /100\t:  0.29764294624328613 \t41.14164996147156 \t103.49681091308594\n",
      "Batch:\t143 /100\t:  0.28366899490356445 \t41.42682218551636 \t65.33683013916016\n",
      "Batch:\t144 /100\t:  0.2848019599914551 \t41.71284770965576 \t69.04783630371094\n",
      "Batch:\t145 /100\t:  0.29010915756225586 \t42.00403332710266 \t97.07176971435547\n",
      "Batch:\t146 /100\t:  0.28608059883117676 \t42.291706562042236 \t94.2531509399414\n",
      "Batch:\t147 /100\t:  0.27992701530456543 \t42.57277035713196 \t98.40328216552734\n",
      "Batch:\t148 /100\t:  0.30639052391052246 \t42.88089203834534 \t85.62539672851562\n",
      "Batch:\t149 /100\t:  0.2833566665649414 \t43.16563534736633 \t81.03909301757812\n",
      "Batch:\t150 /100\t:  0.2899138927459717 \t43.45730757713318 \t52.93617248535156\n",
      "Batch:\t151 /100\t:  0.2824838161468506 \t43.7411847114563 \t67.78620910644531\n",
      "Batch:\t152 /100\t:  0.2820756435394287 \t44.02473449707031 \t114.69810485839844\n",
      "Batch:\t153 /100\t:  0.28499341011047363 \t44.311145305633545 \t97.40210723876953\n",
      "Batch:\t154 /100\t:  0.2826659679412842 \t44.595247745513916 \t79.13079071044922\n",
      "Batch:\t155 /100\t:  0.2914466857910156 \t44.88806700706482 \t81.80736541748047\n",
      "Batch:\t156 /100\t:  0.2847886085510254 \t45.1743323802948 \t77.35543060302734\n",
      "Batch:\t157 /100\t:  0.28571319580078125 \t45.461477756500244 \t66.47835540771484\n",
      "Batch:\t158 /100\t:  0.28070998191833496 \t45.74352478981018 \t80.8613052368164\n",
      "Batch:\t159 /100\t:  0.2857036590576172 \t46.03025269508362 \t63.29436492919922\n",
      "Batch:\t160 /100\t:  0.2848062515258789 \t46.3164918422699 \t109.2802963256836\n",
      "Batch:\t161 /100\t:  0.27838683128356934 \t46.595911502838135 \t115.00961303710938\n",
      "Batch:\t162 /100\t:  0.283583402633667 \t46.88084697723389 \t122.99702453613281\n",
      "Batch:\t163 /100\t:  0.2888917922973633 \t47.17073106765747 \t74.61096954345703\n",
      "Batch:\t164 /100\t:  0.2829132080078125 \t47.455318450927734 \t91.34242248535156\n",
      "Batch:\t165 /100\t:  0.2857215404510498 \t47.74794244766235 \t87.62088012695312\n",
      "Batch:\t166 /100\t:  0.2803347110748291 \t48.02964210510254 \t92.12104034423828\n",
      "Batch:\t167 /100\t:  0.2775900363922119 \t48.308387994766235 \t89.19865417480469\n",
      "Batch:\t168 /100\t:  0.2910044193267822 \t48.60055208206177 \t73.19963073730469\n",
      "Batch:\t169 /100\t:  0.28332972526550293 \t48.885339975357056 \t83.64743041992188\n",
      "Batch:\t170 /100\t:  0.28606724739074707 \t49.172975063323975 \t48.44417190551758\n",
      "Batch:\t171 /100\t:  0.28279662132263184 \t49.45738101005554 \t67.47122955322266\n",
      "Batch:\t172 /100\t:  0.28127527236938477 \t49.73986029624939 \t67.62445068359375\n",
      "Batch:\t173 /100\t:  0.31045103073120117 \t50.05845832824707 \t67.52944946289062\n",
      "Batch:\t174 /100\t:  0.28704261779785156 \t50.346941232681274 \t90.2406005859375\n",
      "Time: 77.83348560333252\t Loss: 14990.242469787598\t Valdacc: 0.31\t Testacc: 0.305\n",
      " BestValidAcc: 0.33\n",
      " BestTestAcc: 0.335\n",
      "\n",
      "Epoch:  6 / 200\n",
      "Batch:\t0 /100\t:  0.28603196144104004 \t0.2886536121368408 \t32.63227844238281\n",
      "Batch:\t1 /100\t:  0.289963960647583 \t0.5813395977020264 \t52.544986724853516\n",
      "Batch:\t2 /100\t:  0.27785515785217285 \t0.8604071140289307 \t39.91956329345703\n",
      "Batch:\t3 /100\t:  0.2828817367553711 \t1.1447908878326416 \t43.45229721069336\n",
      "Batch:\t4 /100\t:  0.28594017028808594 \t1.4318397045135498 \t184.06863403320312\n",
      "Batch:\t5 /100\t:  0.28674769401550293 \t1.7204468250274658 \t101.18354034423828\n",
      "Batch:\t6 /100\t:  0.29170727729797363 \t2.013641595840454 \t28.127206802368164\n",
      "Batch:\t7 /100\t:  0.3360724449157715 \t2.351693630218506 \t85.05109405517578\n",
      "Batch:\t8 /100\t:  0.2902991771697998 \t2.6433920860290527 \t40.463218688964844\n",
      "Batch:\t9 /100\t:  0.28136277198791504 \t2.926987886428833 \t47.818931579589844\n",
      "Batch:\t10 /100\t:  0.2866075038909912 \t3.214965343475342 \t87.49046325683594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t11 /100\t:  0.291184663772583 \t3.508209228515625 \t84.07793426513672\n",
      "Batch:\t12 /100\t:  0.2812981605529785 \t3.7905611991882324 \t34.1467170715332\n",
      "Batch:\t13 /100\t:  0.28768014907836914 \t4.080246925354004 \t34.78572463989258\n",
      "Batch:\t14 /100\t:  0.2911524772644043 \t4.372530221939087 \t83.8360824584961\n",
      "Batch:\t15 /100\t:  0.29094552993774414 \t4.664862871170044 \t137.85198974609375\n",
      "Batch:\t16 /100\t:  0.28064870834350586 \t4.947214365005493 \t92.00542449951172\n",
      "Batch:\t17 /100\t:  0.27951622009277344 \t5.228087425231934 \t107.07905578613281\n",
      "Batch:\t18 /100\t:  0.2884335517883301 \t5.517568349838257 \t96.82917785644531\n",
      "Batch:\t19 /100\t:  0.27945685386657715 \t5.798168897628784 \t125.61341094970703\n",
      "Batch:\t20 /100\t:  0.2825779914855957 \t6.0817625522613525 \t30.940025329589844\n",
      "Batch:\t21 /100\t:  0.2860379219055176 \t6.36931037902832 \t32.4068717956543\n",
      "Batch:\t22 /100\t:  0.2854123115539551 \t6.65591287612915 \t82.45346069335938\n",
      "Batch:\t23 /100\t:  0.2810394763946533 \t6.938101530075073 \t152.0740509033203\n",
      "Batch:\t24 /100\t:  0.2820732593536377 \t7.221333026885986 \t157.63473510742188\n",
      "Batch:\t25 /100\t:  0.2809932231903076 \t7.503759860992432 \t129.53778076171875\n",
      "Batch:\t26 /100\t:  0.28449440002441406 \t7.790212631225586 \t136.47640991210938\n",
      "Batch:\t27 /100\t:  0.3031792640686035 \t8.094521284103394 \t122.51155853271484\n",
      "Batch:\t28 /100\t:  0.28993821144104004 \t8.385546684265137 \t47.91197204589844\n",
      "Batch:\t29 /100\t:  0.2816760540008545 \t8.66856050491333 \t109.97893524169922\n",
      "Batch:\t30 /100\t:  0.271686315536499 \t8.941606998443604 \t60.27314376831055\n",
      "Batch:\t31 /100\t:  0.29363369941711426 \t9.236767053604126 \t92.41232299804688\n",
      "Batch:\t32 /100\t:  0.3078041076660156 \t9.546679019927979 \t72.2748031616211\n",
      "Batch:\t33 /100\t:  0.2881896495819092 \t9.83644723892212 \t60.4990348815918\n",
      "Batch:\t34 /100\t:  0.2792391777038574 \t10.116935968399048 \t81.69683837890625\n",
      "Batch:\t35 /100\t:  0.2866971492767334 \t10.404721975326538 \t67.40193939208984\n",
      "Batch:\t36 /100\t:  0.2846713066101074 \t10.690601110458374 \t89.61593627929688\n",
      "Batch:\t37 /100\t:  0.28936076164245605 \t10.981106996536255 \t99.1731948852539\n",
      "Batch:\t38 /100\t:  0.281383752822876 \t11.263768672943115 \t85.18611907958984\n",
      "Batch:\t39 /100\t:  0.28165531158447266 \t11.546518087387085 \t78.57014465332031\n",
      "Batch:\t40 /100\t:  0.2962915897369385 \t11.844525575637817 \t88.53494262695312\n",
      "Batch:\t41 /100\t:  0.28630542755126953 \t12.132198095321655 \t118.94857788085938\n",
      "Batch:\t42 /100\t:  0.2898247241973877 \t12.423527479171753 \t70.08234405517578\n",
      "Batch:\t43 /100\t:  0.28252124786376953 \t12.707150220870972 \t53.224571228027344\n",
      "Batch:\t44 /100\t:  0.28481268882751465 \t12.993068218231201 \t56.22228240966797\n",
      "Batch:\t45 /100\t:  0.2804872989654541 \t13.275034666061401 \t60.85063552856445\n",
      "Batch:\t46 /100\t:  0.27918219566345215 \t13.555800676345825 \t45.642459869384766\n",
      "Batch:\t47 /100\t:  0.286726713180542 \t13.843580722808838 \t112.06474304199219\n",
      "Batch:\t48 /100\t:  0.29343724250793457 \t14.13852834701538 \t80.64935302734375\n",
      "Batch:\t49 /100\t:  0.2858285903930664 \t14.425368785858154 \t93.14517211914062\n",
      "Batch:\t50 /100\t:  0.2853546142578125 \t14.71226167678833 \t74.8487777709961\n",
      "Batch:\t51 /100\t:  0.28357386589050293 \t14.996903419494629 \t108.95634460449219\n",
      "Batch:\t52 /100\t:  0.28933072090148926 \t15.287228107452393 \t81.72559356689453\n",
      "Batch:\t53 /100\t:  0.2827596664428711 \t15.571045875549316 \t50.158939361572266\n",
      "Batch:\t54 /100\t:  0.3095860481262207 \t15.882139444351196 \t67.07527160644531\n",
      "Batch:\t55 /100\t:  0.27904486656188965 \t16.162203788757324 \t75.41497802734375\n",
      "Batch:\t56 /100\t:  0.2818794250488281 \t16.445298671722412 \t57.833980560302734\n",
      "Batch:\t57 /100\t:  0.2842381000518799 \t16.730642080307007 \t105.72415161132812\n",
      "Batch:\t58 /100\t:  0.2857658863067627 \t17.01799511909485 \t57.680355072021484\n",
      "Batch:\t59 /100\t:  0.27686285972595215 \t17.296056270599365 \t100.5184326171875\n",
      "Batch:\t60 /100\t:  0.27800512313842773 \t17.57524275779724 \t88.66313934326172\n",
      "Batch:\t61 /100\t:  0.3049929141998291 \t17.88154649734497 \t74.79203796386719\n",
      "Batch:\t62 /100\t:  0.28809452056884766 \t18.171079874038696 \t61.41246032714844\n",
      "Batch:\t63 /100\t:  0.28406834602355957 \t18.46456503868103 \t50.604854583740234\n",
      "Batch:\t64 /100\t:  0.28298163414001465 \t18.7491934299469 \t59.34328842163086\n",
      "Batch:\t65 /100\t:  0.28322577476501465 \t19.033684730529785 \t82.17916870117188\n",
      "Batch:\t66 /100\t:  0.28606510162353516 \t19.32131838798523 \t149.36354064941406\n",
      "Batch:\t67 /100\t:  0.28087592124938965 \t19.60324764251709 \t81.83155822753906\n",
      "Batch:\t68 /100\t:  0.2855823040008545 \t19.88995909690857 \t63.61079788208008\n",
      "Batch:\t69 /100\t:  0.28298258781433105 \t20.17398738861084 \t72.30455780029297\n",
      "Batch:\t70 /100\t:  0.2863638401031494 \t20.461759567260742 \t64.1516342163086\n",
      "Batch:\t71 /100\t:  0.28037023544311523 \t20.743305206298828 \t117.28157043457031\n",
      "Batch:\t72 /100\t:  0.28225016593933105 \t21.027151107788086 \t123.55191040039062\n",
      "Batch:\t73 /100\t:  0.28444838523864746 \t21.31301212310791 \t66.55562591552734\n",
      "Batch:\t74 /100\t:  0.2909226417541504 \t21.605338096618652 \t79.47016143798828\n",
      "Batch:\t75 /100\t:  0.3342602252960205 \t21.941096544265747 \t55.07146072387695\n",
      "Batch:\t76 /100\t:  0.28412485122680664 \t22.226998805999756 \t90.7924575805664\n",
      "Batch:\t77 /100\t:  0.2827019691467285 \t22.510769605636597 \t69.30648803710938\n",
      "Batch:\t78 /100\t:  0.27751588821411133 \t22.789410829544067 \t102.74185180664062\n",
      "Batch:\t79 /100\t:  0.2844829559326172 \t23.07541036605835 \t103.29129791259766\n",
      "Batch:\t80 /100\t:  0.2802309989929199 \t23.35674476623535 \t67.01535034179688\n",
      "Batch:\t81 /100\t:  0.28728199005126953 \t23.645726680755615 \t60.173160552978516\n",
      "Batch:\t82 /100\t:  0.29288244247436523 \t23.93965721130371 \t71.33350372314453\n",
      "Batch:\t83 /100\t:  0.3217456340789795 \t24.26322913169861 \t91.1240234375\n",
      "Batch:\t84 /100\t:  0.2883274555206299 \t24.552883625030518 \t65.25431823730469\n",
      "Batch:\t85 /100\t:  0.27761363983154297 \t24.832620859146118 \t55.95214080810547\n",
      "Batch:\t86 /100\t:  0.2847580909729004 \t25.118593454360962 \t56.880470275878906\n",
      "Batch:\t87 /100\t:  0.2918660640716553 \t25.412150382995605 \t62.218570709228516\n",
      "Batch:\t88 /100\t:  0.27878904342651367 \t25.692011833190918 \t80.55706024169922\n",
      "Batch:\t89 /100\t:  0.2910318374633789 \t25.984227895736694 \t68.15057373046875\n",
      "Batch:\t90 /100\t:  0.28958845138549805 \t26.275498628616333 \t97.48675537109375\n",
      "Batch:\t91 /100\t:  0.2835049629211426 \t26.56072211265564 \t49.84141159057617\n",
      "Batch:\t92 /100\t:  0.28862714767456055 \t26.85081386566162 \t77.68067932128906\n",
      "Batch:\t93 /100\t:  0.2820730209350586 \t27.134527683258057 \t51.30963897705078\n",
      "Batch:\t94 /100\t:  0.28212523460388184 \t27.41804528236389 \t72.22559356689453\n",
      "Batch:\t95 /100\t:  0.28701353073120117 \t27.706602811813354 \t78.42121887207031\n",
      "Batch:\t96 /100\t:  0.28287363052368164 \t27.990943670272827 \t59.4501838684082\n",
      "Batch:\t97 /100\t:  0.2820894718170166 \t28.274598360061646 \t83.9051284790039\n",
      "Batch:\t98 /100\t:  0.2845337390899658 \t28.560202598571777 \t83.55724334716797\n",
      "Batch:\t99 /100\t:  0.28136110305786133 \t28.843409776687622 \t74.27656555175781\n",
      "Batch:\t100 /100\t:  0.28775548934936523 \t29.13998246192932 \t91.64411926269531\n",
      "Batch:\t101 /100\t:  0.281909704208374 \t29.42346167564392 \t74.13688659667969\n",
      "Batch:\t102 /100\t:  0.29869699478149414 \t29.723419427871704 \t62.225189208984375\n",
      "Batch:\t103 /100\t:  0.28849196434020996 \t30.013571977615356 \t63.461158752441406\n",
      "Batch:\t104 /100\t:  0.2844381332397461 \t30.29925537109375 \t91.89518737792969\n",
      "Batch:\t105 /100\t:  0.28598785400390625 \t30.586627960205078 \t75.92573547363281\n",
      "Batch:\t106 /100\t:  0.2812068462371826 \t30.869213342666626 \t63.17827606201172\n",
      "Batch:\t107 /100\t:  0.28117823600769043 \t31.151691675186157 \t78.36560821533203\n",
      "Batch:\t108 /100\t:  0.2893552780151367 \t31.442206621170044 \t51.11167907714844\n",
      "Batch:\t109 /100\t:  0.2867095470428467 \t31.73026204109192 \t57.374755859375\n",
      "Batch:\t110 /100\t:  0.3086967468261719 \t32.045238971710205 \t72.47296905517578\n",
      "Batch:\t111 /100\t:  0.28307533264160156 \t32.32964301109314 \t87.70403289794922\n",
      "Batch:\t112 /100\t:  0.2853412628173828 \t32.61625099182129 \t58.47808074951172\n",
      "Batch:\t113 /100\t:  0.28656482696533203 \t32.90398693084717 \t68.42813873291016\n",
      "Batch:\t114 /100\t:  0.28128933906555176 \t33.18651270866394 \t70.8772201538086\n",
      "Batch:\t115 /100\t:  0.28830981254577637 \t33.47592782974243 \t50.41377258300781\n",
      "Batch:\t116 /100\t:  0.280498743057251 \t33.75816535949707 \t73.67603302001953\n",
      "Batch:\t117 /100\t:  0.2892723083496094 \t34.04870414733887 \t108.65121459960938\n",
      "Batch:\t118 /100\t:  0.2905576229095459 \t34.3407347202301 \t65.32662200927734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t119 /100\t:  0.29419374465942383 \t34.635998249053955 \t85.51302337646484\n",
      "Batch:\t120 /100\t:  0.2881960868835449 \t34.92573404312134 \t77.2669906616211\n",
      "Batch:\t121 /100\t:  0.30396437644958496 \t35.23115944862366 \t121.01502227783203\n",
      "Batch:\t122 /100\t:  0.27478790283203125 \t35.507895708084106 \t80.04347229003906\n",
      "Batch:\t123 /100\t:  0.2808094024658203 \t35.78970956802368 \t46.65058898925781\n",
      "Batch:\t124 /100\t:  0.28383493423461914 \t36.07486295700073 \t71.5881576538086\n",
      "Batch:\t125 /100\t:  0.2941861152648926 \t36.37051558494568 \t64.17329406738281\n",
      "Batch:\t126 /100\t:  0.28370189666748047 \t36.65557408332825 \t62.124629974365234\n",
      "Batch:\t127 /100\t:  0.2875862121582031 \t36.94459319114685 \t64.9560775756836\n",
      "Batch:\t128 /100\t:  0.2742142677307129 \t37.22104215621948 \t70.76834869384766\n",
      "Batch:\t129 /100\t:  0.28097081184387207 \t37.50312852859497 \t108.86109161376953\n",
      "Batch:\t130 /100\t:  0.28462910652160645 \t37.78896427154541 \t27.093549728393555\n",
      "Batch:\t131 /100\t:  0.28511548042297363 \t38.075167655944824 \t37.82993698120117\n",
      "Batch:\t132 /100\t:  0.286151647567749 \t38.362863063812256 \t65.98884582519531\n",
      "Batch:\t133 /100\t:  0.28310179710388184 \t38.647444009780884 \t58.137752532958984\n",
      "Batch:\t134 /100\t:  0.2890908718109131 \t38.93784141540527 \t58.93813705444336\n",
      "Batch:\t135 /100\t:  0.28757262229919434 \t39.226433992385864 \t50.101104736328125\n",
      "Batch:\t136 /100\t:  0.28553080558776855 \t39.51369094848633 \t26.79291534423828\n",
      "Batch:\t137 /100\t:  0.2838108539581299 \t39.79857039451599 \t72.04473876953125\n",
      "Batch:\t138 /100\t:  0.28586483001708984 \t40.08575773239136 \t46.65237808227539\n",
      "Batch:\t139 /100\t:  0.283405065536499 \t40.37049436569214 \t86.1139907836914\n",
      "Batch:\t140 /100\t:  0.279987096786499 \t40.65165376663208 \t51.19233703613281\n",
      "Batch:\t141 /100\t:  0.2840118408203125 \t40.93709969520569 \t116.83775329589844\n",
      "Batch:\t142 /100\t:  0.28940510749816895 \t41.22807168960571 \t61.032371520996094\n",
      "Batch:\t143 /100\t:  0.28616809844970703 \t41.51535940170288 \t25.20648765563965\n",
      "Batch:\t144 /100\t:  0.28115272521972656 \t41.79767894744873 \t107.55904388427734\n",
      "Batch:\t145 /100\t:  0.2911243438720703 \t42.08993911743164 \t83.00991821289062\n",
      "Batch:\t146 /100\t:  0.27951955795288086 \t42.370683431625366 \t66.54685974121094\n",
      "Batch:\t147 /100\t:  0.27883005142211914 \t42.650604486465454 \t116.24687194824219\n",
      "Batch:\t148 /100\t:  0.2864971160888672 \t42.93830394744873 \t140.34771728515625\n",
      "Batch:\t149 /100\t:  0.28852224349975586 \t43.22848606109619 \t75.58747863769531\n",
      "Batch:\t150 /100\t:  0.2803783416748047 \t43.510658979415894 \t54.82433319091797\n",
      "Batch:\t151 /100\t:  0.2860727310180664 \t43.79797172546387 \t71.64708709716797\n",
      "Batch:\t152 /100\t:  0.2848031520843506 \t44.08382248878479 \t96.46255493164062\n",
      "Batch:\t153 /100\t:  0.2955636978149414 \t44.386696577072144 \t70.2093505859375\n",
      "Batch:\t154 /100\t:  0.2828540802001953 \t44.670870780944824 \t57.20568084716797\n",
      "Batch:\t155 /100\t:  0.27968406677246094 \t44.95227384567261 \t77.78394317626953\n",
      "Batch:\t156 /100\t:  0.2758772373199463 \t45.22923016548157 \t63.58089828491211\n",
      "Batch:\t157 /100\t:  0.28055453300476074 \t45.511377573013306 \t61.84144973754883\n",
      "Batch:\t158 /100\t:  0.2864046096801758 \t45.798802614212036 \t48.34219741821289\n",
      "Batch:\t159 /100\t:  0.28952932357788086 \t46.089420318603516 \t80.09553527832031\n",
      "Batch:\t160 /100\t:  0.28345513343811035 \t46.37335181236267 \t71.208984375\n",
      "Batch:\t161 /100\t:  0.27655458450317383 \t46.65135169029236 \t61.44545364379883\n",
      "Batch:\t162 /100\t:  0.2804441452026367 \t46.9328875541687 \t43.86062240600586\n",
      "Batch:\t163 /100\t:  0.29307055473327637 \t47.22709655761719 \t68.1029281616211\n",
      "Batch:\t164 /100\t:  0.28007936477661133 \t47.508230209350586 \t54.53770446777344\n",
      "Batch:\t165 /100\t:  0.28264403343200684 \t47.79289197921753 \t55.75517272949219\n",
      "Batch:\t166 /100\t:  0.28026509284973145 \t48.07418489456177 \t62.399803161621094\n",
      "Batch:\t167 /100\t:  0.2814817428588867 \t48.35677218437195 \t52.29583740234375\n",
      "Batch:\t168 /100\t:  0.2874588966369629 \t48.65046405792236 \t64.80661010742188\n",
      "Batch:\t169 /100\t:  0.2776174545288086 \t48.929707765579224 \t93.13948059082031\n",
      "Batch:\t170 /100\t:  0.29041314125061035 \t49.22120809555054 \t95.990966796875\n",
      "Batch:\t171 /100\t:  0.285830020904541 \t49.50816106796265 \t62.55001449584961\n",
      "Batch:\t172 /100\t:  0.2892417907714844 \t49.798563957214355 \t96.8990707397461\n",
      "Batch:\t173 /100\t:  0.2813451290130615 \t50.08139729499817 \t50.730796813964844\n",
      "Batch:\t174 /100\t:  0.3025805950164795 \t50.391098499298096 \t40.887969970703125\n",
      "MODEL WEIGHTS RIGHT NOW:  6783.52645111084\n",
      "model with accuracy  0.39 stored at data/models/core_chain/bilstm_dot_ulmfit/lcquad/115/model.torch\n",
      "in model save, no vectors were found.\n",
      "Time: 80.9204089641571\t Loss: 13198.614381790161\t Valdacc: 0.39\t Testacc: 0.315\n",
      " BestValidAcc: 0.39\n",
      " BestTestAcc: 0.335\n",
      "\n",
      "Epoch:  7 / 200\n",
      "Batch:\t0 /100\t:  0.2910163402557373 \t0.2915656566619873 \t65.10183715820312\n",
      "Batch:\t1 /100\t:  0.28390026092529297 \t0.57657790184021 \t45.90948486328125\n",
      "Batch:\t2 /100\t:  0.285616397857666 \t0.8632757663726807 \t65.63935852050781\n",
      "Batch:\t3 /100\t:  0.28173017501831055 \t1.1460528373718262 \t59.47880554199219\n",
      "Batch:\t4 /100\t:  0.28784799575805664 \t1.4351370334625244 \t100.82490539550781\n",
      "Batch:\t5 /100\t:  0.2925589084625244 \t1.729278564453125 \t80.56421661376953\n",
      "Batch:\t6 /100\t:  0.2796950340270996 \t2.011312246322632 \t64.23396301269531\n",
      "Batch:\t7 /100\t:  0.2819647789001465 \t2.2946279048919678 \t62.172332763671875\n",
      "Batch:\t8 /100\t:  0.28838276863098145 \t2.5841054916381836 \t80.31253051757812\n",
      "Batch:\t9 /100\t:  0.28784608840942383 \t2.8730831146240234 \t62.70067596435547\n",
      "Batch:\t10 /100\t:  0.2876708507537842 \t3.1622047424316406 \t77.56026458740234\n",
      "Batch:\t11 /100\t:  0.28713274002075195 \t3.450509548187256 \t115.45329284667969\n",
      "Batch:\t12 /100\t:  0.28403711318969727 \t3.7367966175079346 \t41.599761962890625\n",
      "Batch:\t13 /100\t:  0.2859206199645996 \t4.023813962936401 \t38.513954162597656\n",
      "Batch:\t14 /100\t:  0.29737329483032227 \t4.322893381118774 \t93.38294982910156\n",
      "Batch:\t15 /100\t:  0.2916836738586426 \t4.615738391876221 \t96.80496978759766\n",
      "Batch:\t16 /100\t:  0.28949999809265137 \t4.9071044921875 \t78.90044403076172\n",
      "Batch:\t17 /100\t:  0.2833430767059326 \t5.1918182373046875 \t47.16830062866211\n",
      "Batch:\t18 /100\t:  0.32633280754089355 \t5.526960849761963 \t125.12533569335938\n",
      "Batch:\t19 /100\t:  0.29581570625305176 \t5.824392557144165 \t98.4052963256836\n",
      "Batch:\t20 /100\t:  0.2770242691040039 \t6.102464199066162 \t98.98606872558594\n",
      "Batch:\t21 /100\t:  0.287644624710083 \t6.391745567321777 \t89.92762756347656\n",
      "Batch:\t22 /100\t:  0.2913186550140381 \t6.684312105178833 \t91.11382293701172\n",
      "Batch:\t23 /100\t:  0.28803205490112305 \t6.973391056060791 \t88.6070327758789\n",
      "Batch:\t24 /100\t:  0.2834138870239258 \t7.25799036026001 \t44.229618072509766\n",
      "Batch:\t25 /100\t:  0.2914423942565918 \t7.55070948600769 \t82.75141906738281\n",
      "Batch:\t26 /100\t:  0.2816810607910156 \t7.833651781082153 \t54.78951644897461\n",
      "Batch:\t27 /100\t:  0.28896403312683105 \t8.123836994171143 \t54.617286682128906\n",
      "Batch:\t28 /100\t:  0.3293583393096924 \t8.462940692901611 \t71.20773315429688\n",
      "Batch:\t29 /100\t:  0.28333115577697754 \t8.747355222702026 \t49.693946838378906\n",
      "Batch:\t30 /100\t:  0.301647424697876 \t9.055647850036621 \t64.74047088623047\n",
      "Batch:\t31 /100\t:  0.28728461265563965 \t9.344249248504639 \t95.91850280761719\n",
      "Batch:\t32 /100\t:  0.27556419372558594 \t9.620982885360718 \t50.83434295654297\n",
      "Batch:\t33 /100\t:  0.276857852935791 \t9.898757457733154 \t81.55436706542969\n",
      "Batch:\t34 /100\t:  0.28147244453430176 \t10.181817293167114 \t85.35147094726562\n",
      "Batch:\t35 /100\t:  0.28031301498413086 \t10.463296175003052 \t63.05389404296875\n",
      "Batch:\t36 /100\t:  0.2847750186920166 \t10.749360084533691 \t80.8095474243164\n",
      "Batch:\t37 /100\t:  0.284773588180542 \t11.035386323928833 \t49.3391227722168\n",
      "Batch:\t38 /100\t:  0.28281426429748535 \t11.319658994674683 \t93.88270568847656\n",
      "Batch:\t39 /100\t:  0.2824389934539795 \t11.603147745132446 \t90.73825073242188\n",
      "Batch:\t40 /100\t:  0.2868785858154297 \t11.89116907119751 \t39.81498718261719\n",
      "Batch:\t41 /100\t:  0.2818107604980469 \t12.173987865447998 \t49.06163787841797\n",
      "Batch:\t42 /100\t:  0.28521156311035156 \t12.460695266723633 \t86.9092788696289\n",
      "Batch:\t43 /100\t:  0.2865920066833496 \t12.748298168182373 \t66.3265380859375\n",
      "Batch:\t44 /100\t:  0.28142833709716797 \t13.03071665763855 \t68.47798156738281\n",
      "Batch:\t45 /100\t:  0.28374314308166504 \t13.315534830093384 \t57.46418380737305\n",
      "Batch:\t46 /100\t:  0.2832460403442383 \t13.599821329116821 \t69.94873809814453\n",
      "Batch:\t47 /100\t:  0.2846567630767822 \t13.88563585281372 \t72.5401840209961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t48 /100\t:  0.28295421600341797 \t14.169642210006714 \t77.67452239990234\n",
      "Batch:\t49 /100\t:  0.27768397331237793 \t14.448346614837646 \t64.975830078125\n",
      "Batch:\t50 /100\t:  0.2928147315979004 \t14.742938995361328 \t75.04146575927734\n",
      "Batch:\t51 /100\t:  0.282900333404541 \t15.027007579803467 \t50.52314376831055\n",
      "Batch:\t52 /100\t:  0.2815992832183838 \t15.309986114501953 \t68.63565826416016\n",
      "Batch:\t53 /100\t:  0.2890942096710205 \t15.600533723831177 \t74.09783172607422\n",
      "Batch:\t54 /100\t:  0.28086185455322266 \t15.883001327514648 \t70.86946868896484\n",
      "Batch:\t55 /100\t:  0.27417898178100586 \t16.15845489501953 \t56.71770095825195\n",
      "Batch:\t56 /100\t:  0.2859947681427002 \t16.445712566375732 \t77.19869232177734\n",
      "Batch:\t57 /100\t:  0.28807663917541504 \t16.735318899154663 \t53.05147933959961\n",
      "Batch:\t58 /100\t:  0.2858302593231201 \t17.023595094680786 \t92.30790710449219\n",
      "Batch:\t59 /100\t:  0.2811143398284912 \t17.306025981903076 \t54.9898567199707\n",
      "Batch:\t60 /100\t:  0.2811732292175293 \t17.58821725845337 \t64.54544830322266\n",
      "Batch:\t61 /100\t:  0.280120849609375 \t17.86982226371765 \t78.76152801513672\n",
      "Batch:\t62 /100\t:  0.2859189510345459 \t18.157308101654053 \t70.38256072998047\n",
      "Batch:\t63 /100\t:  0.28452610969543457 \t18.443095684051514 \t48.44091033935547\n",
      "Batch:\t64 /100\t:  0.28279685974121094 \t18.72710347175598 \t49.426979064941406\n",
      "Batch:\t65 /100\t:  0.2898902893066406 \t19.01804757118225 \t81.5459213256836\n",
      "Batch:\t66 /100\t:  0.28180646896362305 \t19.300875902175903 \t37.66220474243164\n",
      "Batch:\t67 /100\t:  0.28036928176879883 \t19.582754373550415 \t60.21539306640625\n",
      "Batch:\t68 /100\t:  0.28414082527160645 \t19.867899417877197 \t83.28884887695312\n",
      "Batch:\t69 /100\t:  0.281583309173584 \t20.150946140289307 \t72.4998779296875\n",
      "Batch:\t70 /100\t:  0.2765321731567383 \t20.428523302078247 \t63.039222717285156\n",
      "Batch:\t71 /100\t:  0.28275418281555176 \t20.712398529052734 \t76.54476165771484\n",
      "Batch:\t72 /100\t:  0.27927684783935547 \t20.993536949157715 \t62.24545669555664\n",
      "Batch:\t73 /100\t:  0.30792951583862305 \t21.302956342697144 \t72.46160888671875\n",
      "Batch:\t74 /100\t:  0.2879974842071533 \t21.592134714126587 \t61.163230895996094\n",
      "Batch:\t75 /100\t:  0.2833902835845947 \t21.87692618370056 \t54.85672378540039\n",
      "Batch:\t76 /100\t:  0.284071683883667 \t22.16201090812683 \t68.23588562011719\n",
      "Batch:\t77 /100\t:  0.2804882526397705 \t22.444042444229126 \t73.78206634521484\n",
      "Batch:\t78 /100\t:  0.28804969787597656 \t22.73311734199524 \t49.26985168457031\n",
      "Batch:\t79 /100\t:  0.285250186920166 \t23.021307468414307 \t62.255889892578125\n",
      "Batch:\t80 /100\t:  0.287290096282959 \t23.30960178375244 \t44.66439437866211\n",
      "Batch:\t81 /100\t:  0.28241729736328125 \t23.593116760253906 \t54.66380310058594\n",
      "Batch:\t82 /100\t:  0.27868127822875977 \t23.872847080230713 \t59.58718490600586\n",
      "Batch:\t83 /100\t:  0.2842535972595215 \t24.158429384231567 \t67.78894805908203\n",
      "Batch:\t84 /100\t:  0.2878589630126953 \t24.44764471054077 \t57.33501434326172\n",
      "Batch:\t85 /100\t:  0.2827575206756592 \t24.731690406799316 \t50.73951721191406\n",
      "Batch:\t86 /100\t:  0.28527021408081055 \t25.018136501312256 \t91.80279541015625\n",
      "Batch:\t87 /100\t:  0.28580236434936523 \t25.305134773254395 \t47.07910919189453\n",
      "Batch:\t88 /100\t:  0.2868070602416992 \t25.59326410293579 \t78.6433334350586\n",
      "Batch:\t89 /100\t:  0.2795989513397217 \t25.87385654449463 \t63.87391662597656\n",
      "Batch:\t90 /100\t:  0.2847328186035156 \t26.159749746322632 \t52.0732307434082\n",
      "Batch:\t91 /100\t:  0.28107595443725586 \t26.441800355911255 \t50.28765869140625\n",
      "Batch:\t92 /100\t:  0.2772238254547119 \t26.720652103424072 \t58.95173645019531\n",
      "Batch:\t93 /100\t:  0.2879500389099121 \t27.010045766830444 \t102.74044799804688\n",
      "Batch:\t94 /100\t:  0.2826251983642578 \t27.294018983840942 \t49.661739349365234\n",
      "Batch:\t95 /100\t:  0.28721189498901367 \t27.5824978351593 \t79.00627899169922\n",
      "Batch:\t96 /100\t:  0.28244781494140625 \t27.86614489555359 \t34.11817169189453\n",
      "Batch:\t97 /100\t:  0.28640007972717285 \t28.153557538986206 \t33.311561584472656\n",
      "Batch:\t98 /100\t:  0.28859782218933105 \t28.44365406036377 \t91.95040893554688\n",
      "Batch:\t99 /100\t:  0.2898242473602295 \t28.73454976081848 \t98.82278442382812\n",
      "Batch:\t100 /100\t:  0.2895545959472656 \t29.02557945251465 \t137.12567138671875\n",
      "Batch:\t101 /100\t:  0.2852306365966797 \t29.312278032302856 \t88.68616485595703\n",
      "Batch:\t102 /100\t:  0.2906966209411621 \t29.60430645942688 \t80.52556610107422\n",
      "Batch:\t103 /100\t:  0.28971171379089355 \t29.895031929016113 \t54.22919845581055\n",
      "Batch:\t104 /100\t:  0.2797431945800781 \t30.17627787590027 \t72.51622772216797\n",
      "Batch:\t105 /100\t:  0.28637027740478516 \t30.463642120361328 \t115.22469329833984\n",
      "Batch:\t106 /100\t:  0.28321099281311035 \t30.74829626083374 \t49.14332962036133\n",
      "Batch:\t107 /100\t:  0.27567028999328613 \t31.025114059448242 \t73.09322357177734\n",
      "Batch:\t108 /100\t:  0.2833127975463867 \t31.309598684310913 \t65.71040344238281\n",
      "Batch:\t109 /100\t:  0.28287315368652344 \t31.59368896484375 \t53.576927185058594\n",
      "Batch:\t110 /100\t:  0.2828257083892822 \t31.87764859199524 \t86.44324493408203\n",
      "Batch:\t111 /100\t:  0.28833818435668945 \t32.16712760925293 \t62.951839447021484\n",
      "Batch:\t112 /100\t:  0.28196167945861816 \t32.450793504714966 \t59.1165885925293\n",
      "Batch:\t113 /100\t:  0.2850914001464844 \t32.73734211921692 \t52.655357360839844\n",
      "Batch:\t114 /100\t:  0.2881808280944824 \t33.02698588371277 \t93.99925231933594\n",
      "Batch:\t115 /100\t:  0.28461766242980957 \t33.31305265426636 \t75.00714874267578\n",
      "Batch:\t116 /100\t:  0.280259370803833 \t33.59481143951416 \t48.42275619506836\n",
      "Batch:\t117 /100\t:  0.28444600105285645 \t33.88072085380554 \t39.384437561035156\n",
      "Batch:\t118 /100\t:  0.27744531631469727 \t34.159149408340454 \t99.04029083251953\n",
      "Batch:\t119 /100\t:  0.28554224967956543 \t34.446104526519775 \t59.458126068115234\n",
      "Batch:\t120 /100\t:  0.28192639350891113 \t34.7291316986084 \t55.4012336730957\n",
      "Batch:\t121 /100\t:  0.2920877933502197 \t35.0222589969635 \t76.94679260253906\n",
      "Batch:\t122 /100\t:  0.28964710235595703 \t35.31291580200195 \t57.899375915527344\n",
      "Batch:\t123 /100\t:  0.2852764129638672 \t35.599294662475586 \t53.44673538208008\n",
      "Batch:\t124 /100\t:  0.2815062999725342 \t35.882081747055054 \t85.34754180908203\n",
      "Batch:\t125 /100\t:  0.2841475009918213 \t36.167938232421875 \t55.7140007019043\n",
      "Batch:\t126 /100\t:  0.2809181213378906 \t36.45007801055908 \t71.91454315185547\n",
      "Batch:\t127 /100\t:  0.2797729969024658 \t36.73095226287842 \t51.073238372802734\n",
      "Batch:\t128 /100\t:  0.2900509834289551 \t37.0331346988678 \t44.16965103149414\n",
      "Batch:\t129 /100\t:  0.287290096282959 \t37.321741819381714 \t41.94226837158203\n",
      "Batch:\t130 /100\t:  0.28725147247314453 \t37.61116647720337 \t101.19454193115234\n",
      "Batch:\t131 /100\t:  0.2800331115722656 \t37.89262533187866 \t57.37064743041992\n",
      "Batch:\t132 /100\t:  0.2867908477783203 \t38.18124961853027 \t63.49378967285156\n",
      "Batch:\t133 /100\t:  0.2851400375366211 \t38.46785545349121 \t72.39251708984375\n",
      "Batch:\t134 /100\t:  0.28572583198547363 \t38.75489020347595 \t87.1701889038086\n",
      "Batch:\t135 /100\t:  0.28356218338012695 \t39.039628982543945 \t56.45319747924805\n",
      "Batch:\t136 /100\t:  0.27194881439208984 \t39.31361269950867 \t49.900760650634766\n",
      "Batch:\t137 /100\t:  0.2857666015625 \t39.60042667388916 \t52.0731086730957\n",
      "Batch:\t138 /100\t:  0.28096580505371094 \t39.88299322128296 \t78.12294006347656\n",
      "Batch:\t139 /100\t:  0.2827725410461426 \t40.16716933250427 \t34.043479919433594\n",
      "Batch:\t140 /100\t:  0.28169965744018555 \t40.450586557388306 \t42.957618713378906\n",
      "Batch:\t141 /100\t:  0.28346681594848633 \t40.73529672622681 \t26.61931037902832\n",
      "Batch:\t142 /100\t:  0.2866671085357666 \t41.023130655288696 \t37.628135681152344\n",
      "Batch:\t143 /100\t:  0.28296661376953125 \t41.30709433555603 \t67.06459045410156\n",
      "Batch:\t144 /100\t:  0.2853100299835205 \t41.59375333786011 \t39.840423583984375\n",
      "Batch:\t145 /100\t:  0.2892322540283203 \t41.8839385509491 \t81.15884399414062\n",
      "Batch:\t146 /100\t:  0.29123687744140625 \t42.17650628089905 \t93.34712982177734\n",
      "Batch:\t147 /100\t:  0.2783210277557373 \t42.45585036277771 \t60.43547058105469\n",
      "Batch:\t148 /100\t:  0.2841227054595947 \t42.741291999816895 \t60.64808654785156\n",
      "Batch:\t149 /100\t:  0.28761792182922363 \t43.030006647109985 \t44.021583557128906\n",
      "Batch:\t150 /100\t:  0.283038854598999 \t43.31481170654297 \t48.61415100097656\n",
      "Batch:\t151 /100\t:  0.28198957443237305 \t43.59776067733765 \t61.90040969848633\n",
      "Batch:\t152 /100\t:  0.28862476348876953 \t43.887922048568726 \t73.31484985351562\n",
      "Batch:\t153 /100\t:  0.2843129634857178 \t44.17416954040527 \t92.77262878417969\n",
      "Batch:\t154 /100\t:  0.28406238555908203 \t44.45974683761597 \t100.92063903808594\n",
      "Batch:\t155 /100\t:  0.28070759773254395 \t44.74169635772705 \t28.08717155456543\n"
     ]
    }
   ],
   "source": [
    "if not pointwise:\n",
    "    loss_func = nn.MarginRankingLoss(margin=1, size_average=False)\n",
    "else:\n",
    "    loss_func = nn.BCEWithLogitsLoss()\n",
    "    training_model += '_pointwise'\n",
    "# db = training_loop(training_model = training_model,\n",
    "#                    parameter_dict = parameter_dict,\n",
    "#                    modeler = modeler,\n",
    "#                    train_loader = train_loader,\n",
    "#                    optimizer=optimizer,\n",
    "#                    loss_func=loss_func,\n",
    "#                    data=data,\n",
    "#                    dataset=parameter_dict['dataset'],\n",
    "#                    device=device,\n",
    "#                    test_every=1,\n",
    "#                    validate_every=1,\n",
    "#                     pointwise=pointwise,\n",
    "#                    problem='core_chain')\n",
    "train_loss, modeler, valid_accuracy, test_accuracy = cc.training_loop(training_model = training_model,\n",
    "                                                                           parameter_dict = parameter_dict,\n",
    "                                                                           modeler = modeler,\n",
    "                                                                           train_loader = train_loader,\n",
    "                                                                           optimizer=optimizer,\n",
    "                                                                           loss_func=loss_func,\n",
    "                                                                           data=data,\n",
    "                                                                           dataset=parameter_dict['dataset'],\n",
    "                                                                           device=device,\n",
    "                                                                           test_every=1,\n",
    "                                                                           validate_every=1,\n",
    "                                                                            pointwise=pointwise,\n",
    "                                                                           problem='core_chain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h[0][0].shape, h[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.show()\n",
    "plt.plot(valid_accuracy)\n",
    "plt.show()\n",
    "plt.plot(test_accuracy)\n",
    "plt.show()\n",
    "print(valid_accuracy)\n",
    "print(test_accuracy)\n",
    "print(\"validation accuracy is , \", max(valid_accuracy))\n",
    "print(\"maximum test accuracy is , \", max(test_accuracy))\n",
    "print(\"correct test accuracy i.e test accuracy where validation is highest is \", test_accuracy[valid_accuracy.index(max(valid_accuracy))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merity_model = merity.RNNModel(rnn_type='LSTM', \n",
    "                ntoken=238462,\n",
    "                ninp=400,\n",
    "                nhid=1150,\n",
    "                nlayers=3,\n",
    "                dropout=0.0,\n",
    "                dropouth=0.5,\n",
    "                dropouti=0.5,\n",
    "                dropoute=0.5,\n",
    "                wdrop=0,\n",
    "                tie_weights=True)\n",
    "\n",
    "pretrained_weights = torch.load('./ulmfit/wt103/fwd_wt103_enc.h5', map_location= lambda storage, loc: storage)\n",
    "\n",
    "fastai_model = lm_rnn.RNN_Encoder(ntoken=238462, emb_sz=400, n_hid=1150, n_layers=3, pad_token=0,qrnn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# print(pretrained_weights.keys())\n",
    "\n",
    "# pprint(list(merity_model.state_dict().keys()))\n",
    "# pprint(list(fastai_model.state_dict().keys()))\n",
    "\n",
    "\n",
    "key_mapping = {\n",
    "    'encoder.weight' : 'encoder.weight',\n",
    "    'rnns.0.module.weight_ih_l0' : 'rnns.0.weight_ih_l0', \n",
    "    'rnns.0.module.bias_ih_l0' : 'rnns.0.bias_ih_l0', \n",
    "    'rnns.0.module.bias_hh_l0' : 'rnns.0.bias_hh_l0', \n",
    "    'rnns.0.module.weight_hh_l0_raw' : 'rnns.0.weight_hh_l0', \n",
    "    'rnns.1.module.weight_ih_l0' : 'rnns.1.weight_ih_l0', \n",
    "    'rnns.1.module.bias_ih_l0' : 'rnns.1.bias_ih_l0', \n",
    "    'rnns.1.module.bias_hh_l0': 'rnns.1.bias_hh_l0', \n",
    "    'rnns.1.module.weight_hh_l0_raw' : 'rnns.1.weight_hh_l0', \n",
    "    'rnns.2.module.weight_ih_l0' : 'rnns.2.weight_ih_l0', \n",
    "    'rnns.2.module.bias_ih_l0' : 'rnns.2.bias_ih_l0', \n",
    "    'rnns.2.module.bias_hh_l0': 'rnns.2.bias_hh_l0', \n",
    "    'rnns.2.module.weight_hh_l0_raw' : 'rnns.2.weight_hh_l0',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ew = pretrained_weights.pop('encoder.weight')\n",
    "pretrained_weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights.pop('encoder_with_dropout.embed.weight')\n",
    "pretrained_weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in key_mapping.items():\n",
    "    pretrained_weights[v] = pretrained_weights.pop(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merity_model, fastai_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k, v.shape) for k, v in merity_model.state_dict().items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k, v.shape) for k, v in fastai_model.state_dict().items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k,x.shape) for k, x in pretrained_weights.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merity_model.load_state_dict(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
