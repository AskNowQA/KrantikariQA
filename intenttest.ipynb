{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc4fc781fb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "from __future__ import print_function\n",
    "\n",
    "# In-repo files\n",
    "from utils import prepare_vocab_continous as vocab_master\n",
    "from utils import embeddings_interface as ei\n",
    "import data_loader as dl\n",
    "import components as com\n",
    "import auxiliary as aux\n",
    "import network as net\n",
    "\n",
    "# Torch files\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import  DataLoader\n",
    "\n",
    "# Other libs\n",
    "from qelos_core.scripts.lcquad.corerank import FlatEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import ConfigParser\n",
    "import numpy as np\n",
    "import pylab\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important Macros\n",
    "\n",
    "# Reading and setting up config parser\n",
    "config = ConfigParser.ConfigParser()\n",
    "config.readfp(open('configs/intent.cfg'))\n",
    "\n",
    "# Setting up device,model name and loss types.\n",
    "training_model = 'bilstm_dense'\n",
    "_dataset = 'lcquad'\n",
    "pointwise = False\n",
    "\n",
    "#Loading relations file.\n",
    "COMMON_DATA_DIR = 'data/data/common'\n",
    "_dataset_specific_data_dir = 'data/data/%(dataset)s/' % {'dataset': _dataset}\n",
    "_word_to_id = aux.load_word_list(COMMON_DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing parameters\n",
    "\n",
    "parameter_dict = {}\n",
    "parameter_dict['max_length'] =  int(config.get(training_model,'max_length'))\n",
    "parameter_dict['hidden_size'] = int(config.get(training_model,'hidden_size'))\n",
    "parameter_dict['number_of_layer'] = int(config.get(training_model,'number_of_layer'))\n",
    "parameter_dict['embedding_dim'] = int(config.get(training_model,'embedding_dim'))\n",
    "parameter_dict['vocab_size'] = int(config.get(training_model,'vocab_size'))\n",
    "parameter_dict['batch_size'] = int(config.get(training_model,'batch_size'))\n",
    "parameter_dict['bidirectional'] = bool(config.get(training_model,'bidirectional'))\n",
    "parameter_dict['_neg_paths_per_epoch_train'] = int(config.get(training_model,'_neg_paths_per_epoch_train'))\n",
    "parameter_dict['_neg_paths_per_epoch_validation'] = int(config.get(training_model,'_neg_paths_per_epoch_validation'))\n",
    "parameter_dict['total_negative_samples'] = int(config.get(training_model,'total_negative_samples'))\n",
    "parameter_dict['epochs'] = int(config.get(training_model,'epochs'))\n",
    "parameter_dict['dropout'] = float(config.get(training_model,'dropout'))\n",
    "\n",
    "_dataset_specific_data_dir,\\\n",
    "    _model_specific_data_dir,\\\n",
    "    _file,\\\n",
    "    _max_sequence_length,\\\n",
    "    _neg_paths_per_epoch_train,\\\n",
    "    _neg_paths_per_epoch_validation,\\\n",
    "    _training_split,\\\n",
    "    _validation_split,\\\n",
    "    _index = aux.data_loading_parameters(_dataset,parameter_dict)\n",
    "\n",
    "parameter_dict['index'] = _index\n",
    "parameter_dict['training_split'] = _training_split\n",
    "parameter_dict['validation_split'] = _validation_split\n",
    "parameter_dict['dataset'] = _dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull raw data from disk\n",
    "\n",
    "dataset = json.load(open(os.path.join(_dataset_specific_data_dir, _file)))\n",
    "vocab, _ = vocab_master.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x(_datum):\n",
    "    return np.asarray(_datum['uri']['question-id'])\n",
    "\n",
    "def get_y(_datum):\n",
    "    \"\"\"\n",
    "        Legend: 010: ask\n",
    "                100: count\n",
    "                001: list\n",
    "    \"\"\"\n",
    "\n",
    "    data = _datum['parsed-data']['sparql_query'][:_datum['parsed-data']['sparql_query'].lower().find('{')]\n",
    "    # Check for ask\n",
    "    if u'ask' in data.lower():\n",
    "        return np.asarray([0, 1, 0])\n",
    "\n",
    "    if u'count' in data.lower():\n",
    "        return np.asarray([1, 0, 0])\n",
    "\n",
    "    return np.asarray([0, 0, 1])\n",
    "\n",
    "def convert_to_continous_ids(X, vocab):\n",
    "    \"\"\"\n",
    "        Maps the IDs in X to their values in vocab. \n",
    "        The vectors and vocab are expected to come via vocab_master file\n",
    "    \"\"\"\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            X[i][j] = vocab[X[i][j]]\n",
    "    \n",
    "    return X\n",
    "\n",
    "def preprocess_data(dataset, vocab, parameter_dict):\n",
    "    \"\"\"\n",
    "        Process the raw data to make X, Y\n",
    "        Convert them to the new (continous) ID space\n",
    "        Pad BUT DO NOT SHUFFLE!\n",
    "        \n",
    "        Challenge: how do we keep this synced with released splits?\n",
    "    \"\"\"\n",
    "    \n",
    "    X = np.zeros((len(dataset), parameter_dict['max_length']), dtype=np.int64)\n",
    "    Y = np.zeros((len(dataset), 3), dtype=np.int64)\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        \n",
    "        datum = dataset[i]\n",
    "        \n",
    "        # Call fns to parse it\n",
    "        x, y = get_x(datum), get_y(datum)\n",
    "\n",
    "        # Append ze data into their lists\n",
    "        X[i, :min(x.shape[0], parameter_dict['max_length'])] = x[:min(x.shape[0], parameter_dict['max_length'])]\n",
    "        Y[i] = y\n",
    "        \n",
    "    # Convert them to new IDs\n",
    "    X = convert_to_continous_ids(X, vocab)\n",
    "\n",
    "    # Split\n",
    "    if parameter_dict['index'] == None:\n",
    "        # We don't have a specific index, so we split based on split points.\n",
    "\n",
    "        train_X = X[:int(X.shape[0]*parameter_dict['training_split'])]\n",
    "        train_Y = Y[:int(Y.shape[0]*parameter_dict['training_split'])]\n",
    "\n",
    "        valid_X = X[int(X.shape[0]*parameter_dict['training_split']): int(X.shape[0]*parameter_dict['validation_split'])]\n",
    "        valid_Y = Y[int(Y.shape[0]*parameter_dict['training_split']): int(Y.shape[0]*parameter_dict['validation_split'])]\n",
    "        \n",
    "        test_X = X[int(X.shape[0]*parameter_dict['validation_split']): ]\n",
    "        test_Y = Y[int(Y.shape[0]*parameter_dict['validation_split']): ]\n",
    "    \n",
    "        return {'train_X':train_X, 'train_Y':train_Y, \\\n",
    "                'valid_X':valid_X, 'valid_Y':valid_Y, \\\n",
    "                'test_X':test_X, 'test_Y':test_Y }\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # We have an index to split with. We first get the test split out by index, \n",
    "        # and then we split the rest based on training_split\n",
    "\n",
    "#         test_X = X[int(X.shape[0]*parameter_dict['index']): ]\n",
    "#         test_Y = Y[int(Y.shape[0]*parameter_dict['index']): ]\n",
    "        \n",
    "#         rest_X = X[ :int(X.shape[0]*parameter_dict['index'])]\n",
    "#         rest_Y = Y[ :int(Y.shape[0]*parameter_dict['index']): ]\n",
    "        \n",
    "        train_X = X[:parameter_dict['index']]\n",
    "        train_Y = Y[:parameter_dict['index']]\n",
    "        \n",
    "        valid_X = X[parameter_dict['index']:]\n",
    "        valid_Y = Y[parameter_dict['index']:]\n",
    "        \n",
    "        return {'train_X':train_X, 'train_Y':train_Y, \\\n",
    "                'valid_X':valid_X, 'valid_Y':valid_Y, \\\n",
    "                'test_X':None, 'test_Y':None }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentClassifier:\n",
    "    \n",
    "    def __init__(self, _parameter_dict, _word_to_id,  _device, _pointwise=False, _debug=False):\n",
    "        self.debug = _debug\n",
    "        self.parameter_dict = _parameter_dict\n",
    "        self.device = _device\n",
    "        self.pointwise = _pointwise\n",
    "        self.word_to_id = _word_to_id\n",
    "        self.hiddendim = self.parameter_dict['hidden_size'] * (2 * int(self.parameter_dict['bidirectional']))\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"Init Models\")\n",
    "\n",
    "        self.encoder = FlatEncoder(embdim=self.parameter_dict['embedding_dim'],\n",
    "                                   dims=[self.parameter_dict['hidden_size']],\n",
    "                                   word_dic=self.word_to_id,\n",
    "                                   bidir=True).to(self.device)\n",
    "\n",
    "        self.dense = com.DenseClf(inputdim=self.hiddendim,        # *2 because we have two things concatinated here\n",
    "                                  hiddendim=self.hiddendim/2,\n",
    "                                  outputdim=3).to(self.device)\n",
    "        \n",
    "    def train_batch(self, data, optimizer, loss_fn, device):\n",
    "        '''\n",
    "            Given data, passes it through model, inited in constructor, returns loss and updates the weight\n",
    "            :params data: {batch of question, pos paths, neg paths and dummy y labels}\n",
    "            :params optimizer: torch.optim object\n",
    "            :params loss fn: torch.nn loss object\n",
    "            :params device: torch.device object\n",
    "\n",
    "            returns loss\n",
    "        '''\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Unpacking the data and model from args\n",
    "        ques_batch, y_label = data['ques_batch'], data['y_label']\n",
    "        \n",
    "        ques_batch = torch.tensor(ques_batch, dtype=torch.long, device=device)\n",
    "        y_label = torch.tensor(y_label, dtype=torch.float, device=device)\n",
    "        \n",
    "        # Encoding all the data\n",
    "        ques_batch = self.encoder(ques_batch)\n",
    "\n",
    "        # Calculating dot score\n",
    "        out = self.dense(ques_batch)\n",
    "\n",
    "        '''\n",
    "            If `y == 1` then it assumed the first input should be ranked higher\n",
    "            (have a larger value) than the second input, and vice-versa for `y == -1`\n",
    "        '''\n",
    "        loss = loss_fn(out, y_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return out, loss\n",
    "    \n",
    "    \n",
    "    def predict(self, data, loss_fn, device):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            ques_batch, y_label = data['ques_batch'], data['y_label']\n",
    "            \n",
    "            ques_batch = torch.tensor(ques_batch, dtype=torch.long, device=device)\n",
    "            y_label = torch.tensor(y_label, dtype=torch.float, device=device)\n",
    "            \n",
    "            # Encoding all the data\n",
    "            ques_batch = self.encoder(ques_batch)\n",
    "\n",
    "            # Calculating dot score\n",
    "            out = self.dense(ques_batch)\n",
    "\n",
    "            '''\n",
    "                If `y == 1` then it assumed the first input should be ranked higher\n",
    "                (have a larger value) than the second input, and vice-versa for `y == -1`\n",
    "            '''\n",
    "            loss = loss_fn(out, y_label)\n",
    "            \n",
    "            return out, loss\n",
    "        \n",
    "    def eval(self, y_true, y_pred):\n",
    "        \n",
    "        return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred.detach().cpu().numpy(), axis=1))\n",
    "    \n",
    "    def prepare_save(self):\n",
    "        \n",
    "        return [('encoder', self.encoder), ('dense', self.dense)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(classifier, optimizer, loss_fn, dataset, parameter_dict, device):\n",
    "    \n",
    "    training_acc = []\n",
    "    valid_acc = []\n",
    "    training_loss = []\n",
    "    best_valid_acc = 0.0\n",
    "    loc = aux.save_location('intent', 'bilstm_dense', 'lcquad')\n",
    "\n",
    "    for epoch in range(parameter_dict['epochs']):\n",
    "        \n",
    "        epoch_time = time.time()\n",
    "                \n",
    "        epoch_loss = []\n",
    "        epoch_train_acc = []\n",
    "        \n",
    "        for b in range(dataset['len']/int(parameter_dict['batch_size'])):\n",
    "            \n",
    "            batch_time = time.time()\n",
    "            \n",
    "            # Sample new data\n",
    "            data = {}\n",
    "            index = np.random.randint(0, dataset['len'], parameter_dict['batch_size'])\n",
    "            \n",
    "            data['ques_batch'] = dataset['train_X'][index]\n",
    "            data['y_label'] = dataset['train_Y'][index]\n",
    "            \n",
    "            y_pred, loss = classifier.train_batch(data, optimizer, loss_fn, device)\n",
    "            \n",
    "            acc = classifier.eval(y_true = data['y_label'], y_pred = y_pred)\n",
    "            \n",
    "            # Book-keeping\n",
    "            epoch_loss.append(loss.item())\n",
    "            epoch_train_acc.append(acc)\n",
    "            \n",
    "#             print(\"Batch:\\t%d\" % b, \"/%d\\t: \" % (dataset['len']/int(parameter_dict['batch_size'])),\n",
    "#                   \"%s\" % (time.time() - batch_time),\n",
    "#                   \"\\t%s\" % (time.time() - epoch_time),\n",
    "#                   \"\\tloss: %s\" % loss.item(),\n",
    "#                   end=None if b + 1 == dataset['len']/int(parameter_dict['batch_size']) else \"\\r\")                 \n",
    "        # Epoch level\n",
    "        training_acc.append(epoch_train_acc)\n",
    "        training_loss.append(epoch_loss)\n",
    "    \n",
    "        # Evaluate on validation data\n",
    "        data = {}\n",
    "        data['ques_batch'] = dataset['valid_X']\n",
    "        data['y_label'] = dataset['valid_Y']\n",
    "        y_pred, _ = classifier.predict(data, loss_fn, device)\n",
    "        valid_acc.append(classifier.eval(y_true=data['y_label'], y_pred=y_pred))\n",
    "        \n",
    "        # IF it outperformed, save model.\n",
    "        if best_valid_acc < valid_acc[-1]:\n",
    "            aux_save_information = {\n",
    "                'epoch' : epoch,\n",
    "                'validation_accuracy':valid_acc[-1],\n",
    "                'parameter_dict':parameter_dict }\n",
    "            \n",
    "            aux.save_model(loc, classifier, model_name='model.torch', epochs=epoch, optimizer=optimizer, \\\n",
    "                       accuracy=valid_acc[-1], aux_save_information={})\n",
    "            best_valid_acc = valid_acc[-1]\n",
    "                         \n",
    "        print(\"Epoch: \", epoch, \"/\", parameter_dict['epochs'],\n",
    "              \"\\t\\bTime: %s\\t\" % (time.time() - epoch_time),\n",
    "              \"Loss: %s\\t\" % (sum(epoch_loss)),\n",
    "              \"Valdacc: %s\\t\" % (valid_acc[-1]))\n",
    "                         \n",
    "    return classifier, training_loss, training_acc, valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss(loss, loss2=None, _label=\"Some label\", _label2=\"Some other label\", _name=\"Generic Name\", _only_epoch=True):\n",
    "    \"\"\"\n",
    "        Fn to visualize loss.\n",
    "        Expects either\n",
    "            - [int, int] for epoch level stuff\n",
    "            - [ [int, int], [int, int] ] for batch level data. \n",
    "    \"\"\"\n",
    "    \n",
    "    plt.rcParams['figure.figsize'] = [15, 8] \n",
    "    \n",
    "    # Detect input format\n",
    "    if type(loss[0]) is not list: #in [int, float, long]:\n",
    "        \n",
    "#         print(\"here\")\n",
    "        plt.plot(loss, '-b', label=_label)\n",
    "        if loss2: plt.plot(loss2, '-r', label=_label2)\n",
    "        plt.ylabel(_name)\n",
    "        pylab.legend(loc='upper left')\n",
    "        plt.show()\n",
    "        \n",
    "    elif type(loss[0]) == list:\n",
    "        \n",
    "        if _only_epoch:\n",
    "            loss = [ np.mean(x) for x in loss ]\n",
    "            if loss2 is not None: \n",
    "                loss2 = [ np.mean(x) for x in loss2 ]\n",
    "            \n",
    "        else:\n",
    "            loss = [ y for x in loss for y in x ]\n",
    "            if loss2 is not None: loss2 = [ y for x in loss2 for y in x ]\n",
    "            \n",
    "        plt.plot(loss, '-b', label=_label)\n",
    "        if loss2 is not None: plt.plot(loss2, '-r', label=_label2)\n",
    "        plt.ylabel(_name)\n",
    "        pylab.legend(loc='upper left')\n",
    "        plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PartiallyPretrainedWordEmb: vectors loaded in 0.014 second\n",
      "PartiallyPretrainedWordEmb: words loaded in 0.071 second\n",
      "PartiallyPretrainedWordEmb: dictionary created in 0.051 second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "components.py:104: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(self.output(_x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('model with accuracy ', 0.976, 'stored at', 'data/models/intent/bilstm_dense/lcquad/4/model.torch')\n",
      "in model save, no vectors were found.\n",
      "Epoch:  0 / 300 Time: 1.3372490406\t Loss: 4.46850211883\t Valdacc: 0.976\t\n",
      "('model with accuracy ', 0.986, 'stored at', 'data/models/intent/bilstm_dense/lcquad/4/model.torch')\n",
      "in model save, no vectors were found.\n",
      "Epoch:  1 / 300 Time: 0.857127904892\t Loss: 0.624742302743\t Valdacc: 0.986\t\n",
      "('model with accuracy ', 0.998, 'stored at', 'data/models/intent/bilstm_dense/lcquad/4/model.torch')\n",
      "in model save, no vectors were found.\n",
      "Epoch:  2 / 300 Time: 0.837203979492\t Loss: 0.560418187149\t Valdacc: 0.998\t\n",
      "Epoch:  3 / 300 Time: 0.77201795578\t Loss: 0.260042877268\t Valdacc: 0.994\t\n",
      "Epoch:  4 / 300 Time: 0.722867012024\t Loss: 0.161336295576\t Valdacc: 0.996\t\n",
      "Epoch:  5 / 300 Time: 0.718774795532\t Loss: 0.185221418746\t Valdacc: 0.996\t\n",
      "Epoch:  6 / 300 Time: 0.719738006592\t Loss: 0.18199166454\t Valdacc: 0.998\t\n",
      "Epoch:  7 / 300 Time: 0.7206158638\t Loss: 0.153400089964\t Valdacc: 0.998\t\n",
      "Epoch:  8 / 300 Time: 0.791898965836\t Loss: 0.100722893535\t Valdacc: 0.998\t\n",
      "Epoch:  9 / 300 Time: 0.708168983459\t Loss: 0.16107807326\t Valdacc: 0.998\t\n",
      "Epoch:  10 / 300 Time: 0.706105947495\t Loss: 0.116961117918\t Valdacc: 0.998\t\n",
      "Epoch:  11 / 300 Time: 0.705619096756\t Loss: 0.174755707072\t Valdacc: 0.998\t\n",
      "Epoch:  12 / 300 Time: 0.714562892914\t Loss: 0.148969895694\t Valdacc: 0.998\t\n",
      "Epoch:  13 / 300 Time: 0.731925964355\t Loss: 0.13379598233\t Valdacc: 0.998\t\n",
      "Epoch:  14 / 300 Time: 0.765402078629\t Loss: 0.114331889102\t Valdacc: 0.996\t\n",
      "Epoch:  15 / 300 Time: 0.708201169968\t Loss: 0.0839359186067\t Valdacc: 0.998\t\n",
      "Epoch:  16 / 300 Time: 0.710485935211\t Loss: 0.100102470626\t Valdacc: 0.998\t\n",
      "Epoch:  17 / 300 Time: 0.708595991135\t Loss: 0.0501411481555\t Valdacc: 0.998\t\n",
      "Epoch:  18 / 300 Time: 0.712491035461\t Loss: 0.0834187485521\t Valdacc: 0.998\t\n",
      "Epoch:  19 / 300 Time: 0.709365129471\t Loss: 0.0500824215027\t Valdacc: 0.998\t\n",
      "Epoch:  20 / 300 Time: 0.712556123734\t Loss: 0.165971943207\t Valdacc: 0.998\t\n",
      "Epoch:  21 / 300 Time: 0.708258867264\t Loss: 0.100133803707\t Valdacc: 0.998\t\n",
      "Epoch:  22 / 300 Time: 0.705429077148\t Loss: 0.122896770534\t Valdacc: 0.99\t\n",
      "Epoch:  23 / 300 Time: 0.712931871414\t Loss: 0.0576611867196\t Valdacc: 0.998\t\n",
      "Epoch:  24 / 300 Time: 0.710320949554\t Loss: 0.19606619658\t Valdacc: 0.99\t\n",
      "Epoch:  25 / 300 Time: 0.70701789856\t Loss: 0.294343937346\t Valdacc: 0.992\t\n",
      "Epoch:  26 / 300 Time: 0.709716081619\t Loss: 0.10288124708\t Valdacc: 0.994\t\n",
      "Epoch:  27 / 300 Time: 0.759537935257\t Loss: 0.0391340463894\t Valdacc: 0.996\t\n",
      "Epoch:  28 / 300 Time: 0.767737865448\t Loss: 0.157485565175\t Valdacc: 0.998\t\n",
      "Epoch:  29 / 300 Time: 0.716719865799\t Loss: 0.107684095137\t Valdacc: 0.998\t\n",
      "Epoch:  30 / 300 Time: 0.709471940994\t Loss: 0.0836339444634\t Valdacc: 0.998\t\n",
      "Epoch:  31 / 300 Time: 0.711773872375\t Loss: 0.183374109736\t Valdacc: 0.998\t\n",
      "Epoch:  32 / 300 Time: 0.708606004715\t Loss: 0.133426499929\t Valdacc: 0.998\t\n",
      "Epoch:  33 / 300 Time: 0.71263384819\t Loss: 0.0833682622092\t Valdacc: 0.998\t\n",
      "Epoch:  34 / 300 Time: 0.71336388588\t Loss: 0.083377116491\t Valdacc: 0.998\t\n",
      "Epoch:  35 / 300 Time: 0.707385063171\t Loss: 0.0833485295082\t Valdacc: 0.998\t\n",
      "Epoch:  36 / 300 Time: 0.713112831116\t Loss: 0.0500164448435\t Valdacc: 0.998\t\n",
      "Epoch:  37 / 300 Time: 0.710812091827\t Loss: 0.100015581157\t Valdacc: 0.998\t\n",
      "Epoch:  38 / 300 Time: 0.832907915115\t Loss: 0.050017094092\t Valdacc: 0.998\t\n",
      "Epoch:  39 / 300 Time: 0.714565992355\t Loss: 0.0667089134775\t Valdacc: 0.998\t\n",
      "Epoch:  40 / 300 Time: 0.756243944168\t Loss: 0.166587722075\t Valdacc: 0.998\t\n",
      "Epoch:  41 / 300 Time: 0.742521047592\t Loss: 0.0830278522474\t Valdacc: 0.998\t\n",
      "Epoch:  42 / 300 Time: 0.74015212059\t Loss: 0.0511597927309\t Valdacc: 0.996\t\n",
      "Epoch:  43 / 300 Time: 0.764731168747\t Loss: 0.101410325562\t Valdacc: 0.998\t\n",
      "Epoch:  44 / 300 Time: 0.816051006317\t Loss: 0.0334116024747\t Valdacc: 0.998\t\n",
      "Epoch:  45 / 300 Time: 0.73140501976\t Loss: 0.100033683576\t Valdacc: 0.998\t\n",
      "Epoch:  46 / 300 Time: 0.716881990433\t Loss: 0.0500313046074\t Valdacc: 0.998\t\n",
      "Epoch:  47 / 300 Time: 0.718106031418\t Loss: 0.0666874548929\t Valdacc: 0.998\t\n",
      "Epoch:  48 / 300 Time: 0.720666885376\t Loss: 0.0500203638263\t Valdacc: 0.998\t\n",
      "Epoch:  49 / 300 Time: 0.717707872391\t Loss: 0.0666820005786\t Valdacc: 0.998\t\n",
      "Epoch:  50 / 300 Time: 0.718713998795\t Loss: 0.133343981331\t Valdacc: 0.998\t\n",
      "Epoch:  51 / 300 Time: 0.72373509407\t Loss: 0.0333454187005\t Valdacc: 0.998\t\n",
      "Epoch:  52 / 300 Time: 0.715769052505\t Loss: 0.150008850256\t Valdacc: 0.998\t\n",
      "Epoch:  53 / 300 Time: 0.753168106079\t Loss: 0.0666778067352\t Valdacc: 0.998\t\n",
      "Epoch:  54 / 300 Time: 0.742985963821\t Loss: 0.0333405275694\t Valdacc: 0.998\t\n",
      "Epoch:  55 / 300 Time: 0.719253063202\t Loss: 0.0833464820729\t Valdacc: 0.998\t\n",
      "Epoch:  56 / 300 Time: 0.715857028961\t Loss: 0.100008471576\t Valdacc: 0.998\t\n",
      "Epoch:  57 / 300 Time: 0.755984067917\t Loss: 0.100005997575\t Valdacc: 0.998\t\n",
      "Epoch:  58 / 300 Time: 0.752754926682\t Loss: 0.0666748285336\t Valdacc: 0.998\t\n",
      "Epoch:  59 / 300 Time: 0.715559005737\t Loss: 0.133341298128\t Valdacc: 0.998\t\n",
      "Epoch:  60 / 300 Time: 0.763135910034\t Loss: 0.0666727961198\t Valdacc: 0.998\t\n",
      "Epoch:  61 / 300 Time: 0.731690168381\t Loss: 0.133341776969\t Valdacc: 0.998\t\n",
      "Epoch:  62 / 300 Time: 0.726516008377\t Loss: 0.100004122248\t Valdacc: 0.998\t\n",
      "Epoch:  63 / 300 Time: 0.725899934769\t Loss: 0.0166723204327\t Valdacc: 0.998\t\n",
      "Epoch:  64 / 300 Time: 0.770445823669\t Loss: 0.016671478884\t Valdacc: 0.998\t\n",
      "Epoch:  65 / 300 Time: 0.738355875015\t Loss: 0.133338480041\t Valdacc: 0.998\t\n",
      "Epoch:  66 / 300 Time: 0.726896047592\t Loss: 0.0500048177944\t Valdacc: 0.998\t\n",
      "Epoch:  67 / 300 Time: 0.717053890228\t Loss: 0.0500039123623\t Valdacc: 0.998\t\n",
      "Epoch:  68 / 300 Time: 0.716231107712\t Loss: 0.0333373323286\t Valdacc: 0.998\t\n",
      "Epoch:  69 / 300 Time: 0.722989082336\t Loss: 0.0333368245181\t Valdacc: 0.998\t\n",
      "Epoch:  70 / 300 Time: 0.720474004745\t Loss: 0.100003589303\t Valdacc: 0.998\t\n",
      "Epoch:  71 / 300 Time: 0.756883144379\t Loss: 0.0833360138803\t Valdacc: 0.998\t\n",
      "Epoch:  72 / 300 Time: 0.737522125244\t Loss: 0.0833364063661\t Valdacc: 0.998\t\n",
      "Epoch:  73 / 300 Time: 0.715117931366\t Loss: 0.0833367517367\t Valdacc: 0.998\t\n",
      "Epoch:  74 / 300 Time: 0.71667098999\t Loss: 0.0666682528945\t Valdacc: 0.998\t\n",
      "Epoch:  75 / 300 Time: 0.719764947891\t Loss: 0.0833371963721\t Valdacc: 0.998\t\n",
      "Epoch:  76 / 300 Time: 0.71986913681\t Loss: 0.0833357635537\t Valdacc: 0.998\t\n",
      "Epoch:  77 / 300 Time: 0.717376947403\t Loss: 0.0833362349841\t Valdacc: 0.998\t\n",
      "Epoch:  78 / 300 Time: 0.713166952133\t Loss: 0.0666695552362\t Valdacc: 0.998\t\n",
      "Epoch:  79 / 300 Time: 0.715164899826\t Loss: 0.200003084392\t Valdacc: 0.998\t\n",
      "Epoch:  80 / 300 Time: 0.716953039169\t Loss: 0.0333360466664\t Valdacc: 0.998\t\n",
      "Epoch:  81 / 300 Time: 0.718564987183\t Loss: 0.100002031353\t Valdacc: 0.998\t\n",
      "Epoch:  82 / 300 Time: 0.725070953369\t Loss: 0.100002235726\t Valdacc: 0.998\t\n",
      "Epoch:  83 / 300 Time: 0.722578048706\t Loss: 0.0666688908248\t Valdacc: 0.998\t\n",
      "Epoch:  84 / 300 Time: 0.722240924835\t Loss: 0.20000150886\t Valdacc: 0.998\t\n",
      "Epoch:  85 / 300 Time: 0.721625089645\t Loss: 0.0666685900458\t Valdacc: 0.998\t\n",
      "Epoch:  86 / 300 Time: 0.718648195267\t Loss: 0.0500018976593\t Valdacc: 0.998\t\n",
      "Epoch:  87 / 300 Time: 0.724597930908\t Loss: 0.0333346445915\t Valdacc: 0.998\t\n",
      "Epoch:  88 / 300 Time: 0.721886873245\t Loss: 0.116668255133\t Valdacc: 0.998\t\n",
      "Epoch:  89 / 300 Time: 0.72091794014\t Loss: 0.100001938187\t Valdacc: 0.998\t\n",
      "Epoch:  90 / 300 Time: 0.718849897385\t Loss: 0.066668127109\t Valdacc: 0.998\t\n",
      "Epoch:  91 / 300 Time: 0.721183776855\t Loss: 0.0333346075254\t Valdacc: 0.998\t\n",
      "Epoch:  92 / 300 Time: 0.71528506279\t Loss: 0.0166679059586\t Valdacc: 0.998\t\n",
      "Epoch:  93 / 300 Time: 0.717234134674\t Loss: 0.100001637465\t Valdacc: 0.998\t\n",
      "Epoch:  94 / 300 Time: 0.7141289711\t Loss: 0.10000117438\t Valdacc: 0.998\t\n",
      "Epoch:  95 / 300 Time: 0.716024875641\t Loss: 0.100000975567\t Valdacc: 0.998\t\n",
      "Epoch:  96 / 300 Time: 0.715451002121\t Loss: 0.0833346699326\t Valdacc: 0.998\t\n",
      "Epoch:  97 / 300 Time: 0.717279911041\t Loss: 0.100000761078\t Valdacc: 0.998\t\n",
      "Epoch:  98 / 300 Time: 0.718842983246\t Loss: 0.033334826222\t Valdacc: 0.998\t\n",
      "Epoch:  99 / 300 Time: 0.720663070679\t Loss: 0.0833346501162\t Valdacc: 0.998\t\n",
      "Epoch:  100 / 300 Time: 0.723302841187\t Loss: 0.0500011377551\t Valdacc: 0.998\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  101 / 300 Time: 0.726129055023\t Loss: 0.0833346065163\t Valdacc: 0.998\t\n",
      "Epoch:  102 / 300 Time: 0.715323925018\t Loss: 0.0833340202504\t Valdacc: 0.998\t\n",
      "Epoch:  103 / 300 Time: 0.718441963196\t Loss: 0.0500008128571\t Valdacc: 0.998\t\n",
      "Epoch:  104 / 300 Time: 0.725732088089\t Loss: 0.0500009151357\t Valdacc: 0.998\t\n",
      "Epoch:  105 / 300 Time: 0.717015028\t Loss: 0.0666675393065\t Valdacc: 0.998\t\n",
      "Epoch:  106 / 300 Time: 0.725769996643\t Loss: 0.0833340411799\t Valdacc: 0.998\t\n",
      "Epoch:  107 / 300 Time: 0.714101076126\t Loss: 0.0333339491395\t Valdacc: 0.998\t\n",
      "Epoch:  108 / 300 Time: 0.726042032242\t Loss: 0.116667386891\t Valdacc: 0.998\t\n",
      "Epoch:  109 / 300 Time: 0.722506046295\t Loss: 0.083333956676\t Valdacc: 0.998\t\n",
      "Epoch:  110 / 300 Time: 0.724575996399\t Loss: 0.100000619619\t Valdacc: 0.998\t\n",
      "Epoch:  111 / 300 Time: 0.718425035477\t Loss: 0.083333941348\t Valdacc: 0.998\t\n",
      "Epoch:  112 / 300 Time: 0.720066070557\t Loss: 0.100000802547\t Valdacc: 0.998\t\n",
      "Epoch:  113 / 300 Time: 0.717023849487\t Loss: 0.0333338364309\t Valdacc: 0.998\t\n",
      "Epoch:  114 / 300 Time: 0.717415094376\t Loss: 0.0666672806963\t Valdacc: 0.998\t\n",
      "Epoch:  115 / 300 Time: 0.72435092926\t Loss: 0.0833338539356\t Valdacc: 0.998\t\n",
      "Epoch:  116 / 300 Time: 0.718715906143\t Loss: 0.116667209327\t Valdacc: 0.998\t\n",
      "Epoch:  117 / 300 Time: 0.724185943604\t Loss: 0.0500004080047\t Valdacc: 0.998\t\n",
      "Epoch:  118 / 300 Time: 0.725097894669\t Loss: 0.0500003673446\t Valdacc: 0.998\t\n",
      "Epoch:  119 / 300 Time: 0.717801809311\t Loss: 0.0166671324697\t Valdacc: 0.998\t\n",
      "Epoch:  120 / 300 Time: 0.724829912186\t Loss: 0.0500004781561\t Valdacc: 0.998\t\n",
      "Epoch:  121 / 300 Time: 0.723029136658\t Loss: 0.0666670370871\t Valdacc: 0.998\t\n",
      "Epoch:  122 / 300 Time: 0.720092058182\t Loss: 0.100000314889\t Valdacc: 0.998\t\n",
      "Epoch:  123 / 300 Time: 0.759991884232\t Loss: 0.1000003897\t Valdacc: 0.998\t\n",
      "Epoch:  124 / 300 Time: 0.738038778305\t Loss: 0.0833336155834\t Valdacc: 0.998\t\n",
      "Epoch:  125 / 300 Time: 0.762679815292\t Loss: 0.0500003879606\t Valdacc: 0.998\t\n",
      "Epoch:  126 / 300 Time: 0.738624811172\t Loss: 0.116666894963\t Valdacc: 0.998\t\n",
      "Epoch:  127 / 300 Time: 0.721596002579\t Loss: 0.0833336120834\t Valdacc: 0.998\t\n",
      "Epoch:  128 / 300 Time: 0.724192857742\t Loss: 0.0500002315536\t Valdacc: 0.998\t\n",
      "Epoch:  129 / 300 Time: 0.716439008713\t Loss: 0.0666670229295\t Valdacc: 0.998\t\n",
      "Epoch:  130 / 300 Time: 0.719561100006\t Loss: 0.0500002827882\t Valdacc: 0.998\t\n",
      "Epoch:  131 / 300 Time: 0.788454055786\t Loss: 0.100000328882\t Valdacc: 0.998\t\n",
      "Epoch:  132 / 300 Time: 0.726109981537\t Loss: 0.0666669595479\t Valdacc: 0.998\t\n",
      "Epoch:  133 / 300 Time: 0.721900939941\t Loss: 0.0333335319671\t Valdacc: 0.998\t\n",
      "Epoch:  134 / 300 Time: 0.719196081161\t Loss: 0.0833335501197\t Valdacc: 0.998\t\n",
      "Epoch:  135 / 300 Time: 0.713768959045\t Loss: 0.0500001713094\t Valdacc: 0.998\t\n",
      "Epoch:  136 / 300 Time: 0.718724012375\t Loss: 0.0833335348294\t Valdacc: 0.998\t\n",
      "Epoch:  137 / 300 Time: 0.762784957886\t Loss: 0.116666819312\t Valdacc: 0.998\t\n",
      "Epoch:  138 / 300 Time: 0.779651880264\t Loss: 0.133333506698\t Valdacc: 0.998\t\n",
      "Epoch:  139 / 300 Time: 0.780767917633\t Loss: 0.0166668972413\t Valdacc: 0.998\t\n",
      "Epoch:  140 / 300 Time: 0.7380027771\t Loss: 0.08333349116\t Valdacc: 0.998\t\n",
      "Epoch:  141 / 300 Time: 0.722184896469\t Loss: 0.100000191929\t Valdacc: 0.998\t\n",
      "Epoch:  142 / 300 Time: 0.728478908539\t Loss: 0.116666880328\t Valdacc: 0.998\t\n",
      "Epoch:  143 / 300 Time: 0.785306930542\t Loss: 0.133333423863\t Valdacc: 0.998\t\n",
      "Epoch:  144 / 300 Time: 0.751973867416\t Loss: 0.0833334337162\t Valdacc: 0.998\t\n",
      "Epoch:  145 / 300 Time: 0.726773023605\t Loss: 0.050000103104\t Valdacc: 0.998\t\n",
      "Epoch:  146 / 300 Time: 0.724586009979\t Loss: 0.0333334752932\t Valdacc: 0.998\t\n",
      "Epoch:  147 / 300 Time: 0.720282077789\t Loss: 0.0833334742574\t Valdacc: 0.998\t\n",
      "Epoch:  148 / 300 Time: 0.834005832672\t Loss: 0.0666668278149\t Valdacc: 0.998\t\n",
      "Epoch:  149 / 300 Time: 0.734565973282\t Loss: 0.0666667859526\t Valdacc: 0.998\t\n",
      "Epoch:  150 / 300 Time: 0.818671941757\t Loss: 0.0833334623017\t Valdacc: 0.998\t\n",
      "Epoch:  151 / 300 Time: 0.754691123962\t Loss: 0.0500001195477\t Valdacc: 0.998\t\n",
      "Epoch:  152 / 300 Time: 0.725140094757\t Loss: 0.0500000849012\t Valdacc: 0.998\t\n",
      "Epoch:  153 / 300 Time: 0.773329973221\t Loss: 0.0666667892452\t Valdacc: 0.998\t\n",
      "Epoch:  154 / 300 Time: 0.722145080566\t Loss: 0.0833334311766\t Valdacc: 0.998\t\n",
      "Epoch:  155 / 300 Time: 0.721165895462\t Loss: 0.116666795431\t Valdacc: 0.998\t\n",
      "Epoch:  156 / 300 Time: 0.721086025238\t Loss: 0.150000075055\t Valdacc: 0.998\t\n",
      "Epoch:  157 / 300 Time: 0.723707199097\t Loss: 0.0333334151965\t Valdacc: 0.998\t\n",
      "Epoch:  158 / 300 Time: 0.779091835022\t Loss: 0.166666746249\t Valdacc: 0.998\t\n",
      "Epoch:  159 / 300 Time: 0.72677898407\t Loss: 0.0500000653963\t Valdacc: 0.998\t\n",
      "Epoch:  160 / 300 Time: 0.765964031219\t Loss: 0.0500000932618\t Valdacc: 0.998\t\n",
      "Epoch:  161 / 300 Time: 0.734502792358\t Loss: 0.16666674456\t Valdacc: 0.998\t\n",
      "Epoch:  162 / 300 Time: 0.72701716423\t Loss: 0.050000072332\t Valdacc: 0.998\t\n",
      "Epoch:  163 / 300 Time: 0.72573018074\t Loss: 0.133333389127\t Valdacc: 0.998\t\n",
      "Epoch:  164 / 300 Time: 0.74761390686\t Loss: 0.0666667290738\t Valdacc: 0.998\t\n",
      "Epoch:  165 / 300 Time: 0.745830059052\t Loss: 0.0666667438218\t Valdacc: 0.998\t\n",
      "Epoch:  166 / 300 Time: 0.714087963104\t Loss: 0.100000059339\t Valdacc: 0.998\t\n",
      "Epoch:  167 / 300 Time: 0.722194910049\t Loss: 0.100000066051\t Valdacc: 0.998\t\n",
      "Epoch:  168 / 300 Time: 0.719249010086\t Loss: 0.0333333997992\t Valdacc: 0.998\t\n",
      "Epoch:  169 / 300 Time: 0.719073057175\t Loss: 0.0333333869824\t Valdacc: 0.998\t\n",
      "Epoch:  170 / 300 Time: 0.7180788517\t Loss: 0.133333389546\t Valdacc: 0.998\t\n",
      "Epoch:  171 / 300 Time: 0.717000961304\t Loss: 0.0333333796885\t Valdacc: 0.998\t\n",
      "Epoch:  172 / 300 Time: 0.732544898987\t Loss: 0.150000049589\t Valdacc: 0.998\t\n",
      "Epoch:  173 / 300 Time: 0.722203969955\t Loss: 0.100000037791\t Valdacc: 0.998\t\n",
      "Epoch:  174 / 300 Time: 0.715217113495\t Loss: 0.0666667152918\t Valdacc: 0.998\t\n",
      "Epoch:  175 / 300 Time: 0.7188808918\t Loss: 0.0833333682238\t Valdacc: 0.998\t\n",
      "Epoch:  176 / 300 Time: 0.718858957291\t Loss: 0.116666722748\t Valdacc: 0.998\t\n",
      "Epoch:  177 / 300 Time: 0.71456193924\t Loss: 0.100000045148\t Valdacc: 0.998\t\n",
      "Epoch:  178 / 300 Time: 0.720407962799\t Loss: 0.100000045695\t Valdacc: 0.998\t\n",
      "Epoch:  179 / 300 Time: 0.741391897202\t Loss: 0.200000041643\t Valdacc: 0.998\t\n",
      "Epoch:  180 / 300 Time: 0.723466873169\t Loss: 0.133333376534\t Valdacc: 0.998\t\n",
      "Epoch:  181 / 300 Time: 0.716434955597\t Loss: 0.0833333687031\t Valdacc: 0.998\t\n",
      "Epoch:  182 / 300 Time: 0.718712091446\t Loss: 0.100000033964\t Valdacc: 0.998\t\n",
      "Epoch:  183 / 300 Time: 0.720860958099\t Loss: 0.0833333633073\t Valdacc: 0.998\t\n",
      "Epoch:  184 / 300 Time: 0.716711044312\t Loss: 0.116666710086\t Valdacc: 0.998\t\n",
      "Epoch:  185 / 300 Time: 0.716516017914\t Loss: 0.100000030761\t Valdacc: 0.998\t\n",
      "Epoch:  186 / 300 Time: 0.728056192398\t Loss: 0.10000002689\t Valdacc: 0.998\t\n",
      "Epoch:  187 / 300 Time: 0.718486070633\t Loss: 0.116666702994\t Valdacc: 0.998\t\n",
      "Epoch:  188 / 300 Time: 0.720075130463\t Loss: 0.083333358694\t Valdacc: 0.998\t\n",
      "Epoch:  189 / 300 Time: 0.72262597084\t Loss: 0.0833333581856\t Valdacc: 0.998\t\n",
      "Epoch:  190 / 300 Time: 0.720923900604\t Loss: 0.0833333552037\t Valdacc: 0.998\t\n",
      "Epoch:  191 / 300 Time: 0.727288961411\t Loss: 0.0500000257686\t Valdacc: 0.998\t\n",
      "Epoch:  192 / 300 Time: 0.726386070251\t Loss: 0.133333358908\t Valdacc: 0.998\t\n",
      "Epoch:  193 / 300 Time: 0.727118968964\t Loss: 0.0666666914781\t Valdacc: 0.998\t\n",
      "Epoch:  194 / 300 Time: 0.725746870041\t Loss: 0.100000018255\t Valdacc: 0.998\t\n",
      "Epoch:  195 / 300 Time: 0.721978187561\t Loss: 0.0500000173079\t Valdacc: 0.998\t\n",
      "Epoch:  196 / 300 Time: 0.719053983688\t Loss: 0.0666666889291\t Valdacc: 0.998\t\n",
      "Epoch:  197 / 300 Time: 0.722852945328\t Loss: 0.100000021563\t Valdacc: 0.998\t\n",
      "Epoch:  198 / 300 Time: 0.768126010895\t Loss: 0.100000021448\t Valdacc: 0.998\t\n",
      "Epoch:  199 / 300 Time: 0.736775159836\t Loss: 0.0666666865781\t Valdacc: 0.998\t\n",
      "Epoch:  200 / 300 Time: 0.721241950989\t Loss: 0.133333349819\t Valdacc: 0.998\t\n",
      "Epoch:  201 / 300 Time: 0.719928026199\t Loss: 0.0500000159833\t Valdacc: 0.998\t\n",
      "Epoch:  202 / 300 Time: 0.720508098602\t Loss: 0.0833333498862\t Valdacc: 0.998\t\n",
      "Epoch:  203 / 300 Time: 0.719697952271\t Loss: 0.116666684812\t Valdacc: 0.998\t\n",
      "Epoch:  204 / 300 Time: 0.724492073059\t Loss: 0.0833333509417\t Valdacc: 0.998\t\n",
      "Epoch:  205 / 300 Time: 0.722631931305\t Loss: 0.100000016604\t Valdacc: 0.998\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  206 / 300 Time: 0.76819396019\t Loss: 0.100000015972\t Valdacc: 0.998\t\n",
      "Epoch:  207 / 300 Time: 0.730901002884\t Loss: 0.116666680629\t Valdacc: 0.998\t\n",
      "Epoch:  208 / 300 Time: 0.722949981689\t Loss: 0.1000000149\t Valdacc: 0.998\t\n",
      "Epoch:  209 / 300 Time: 0.723106145859\t Loss: 0.100000017911\t Valdacc: 0.998\t\n",
      "Epoch:  210 / 300 Time: 0.719661951065\t Loss: 0.0333333446126\t Valdacc: 0.998\t\n",
      "Epoch:  211 / 300 Time: 0.72007393837\t Loss: 0.0500000111409\t Valdacc: 0.998\t\n",
      "Epoch:  212 / 300 Time: 0.724190950394\t Loss: 0.0166666757172\t Valdacc: 0.998\t\n",
      "Epoch:  213 / 300 Time: 0.715774059296\t Loss: 0.0666666797051\t Valdacc: 0.998\t\n",
      "Epoch:  214 / 300 Time: 0.759819030762\t Loss: 0.0500000110153\t Valdacc: 0.998\t\n",
      "Epoch:  215 / 300 Time: 0.738842010498\t Loss: 0.0666666793541\t Valdacc: 0.998\t\n",
      "Epoch:  216 / 300 Time: 0.71585392952\t Loss: 0.0500000125383\t Valdacc: 0.998\t\n",
      "Epoch:  217 / 300 Time: 0.718030929565\t Loss: 0.0666666798999\t Valdacc: 0.998\t\n",
      "Epoch:  218 / 300 Time: 0.725868940353\t Loss: 0.0666666770917\t Valdacc: 0.998\t\n",
      "Epoch:  219 / 300 Time: 0.72434592247\t Loss: 0.0500000118571\t Valdacc: 0.998\t\n",
      "Epoch:  220 / 300 Time: 0.71830201149\t Loss: 0.0500000080637\t Valdacc: 0.998\t\n",
      "Epoch:  221 / 300 Time: 0.71865105629\t Loss: 0.0666666746819\t Valdacc: 0.998\t\n",
      "Epoch:  222 / 300 Time: 0.765336990356\t Loss: 0.100000011073\t Valdacc: 0.998\t\n",
      "Epoch:  223 / 300 Time: 0.739953041077\t Loss: 0.0500000086215\t Valdacc: 0.998\t\n",
      "Epoch:  224 / 300 Time: 0.751674890518\t Loss: 0.0333333421726\t Valdacc: 0.998\t\n",
      "Epoch:  225 / 300 Time: 0.75634598732\t Loss: 0.0500000070145\t Valdacc: 0.998\t\n",
      "Epoch:  226 / 300 Time: 0.725919961929\t Loss: 0.0833333434815\t Valdacc: 0.998\t\n",
      "Epoch:  227 / 300 Time: 0.723508834839\t Loss: 0.100000012233\t Valdacc: 0.998\t\n",
      "Epoch:  228 / 300 Time: 0.745979070663\t Loss: 0.0833333428146\t Valdacc: 0.998\t\n",
      "Epoch:  229 / 300 Time: 0.774538040161\t Loss: 0.0166666730111\t Valdacc: 0.998\t\n",
      "Epoch:  230 / 300 Time: 0.727643966675\t Loss: 0.0666666752655\t Valdacc: 0.998\t\n",
      "Epoch:  231 / 300 Time: 0.72459602356\t Loss: 0.0833333445338\t Valdacc: 0.998\t\n",
      "Epoch:  232 / 300 Time: 0.821949005127\t Loss: 0.0666666732886\t Valdacc: 0.998\t\n",
      "Epoch:  233 / 300 Time: 0.768389940262\t Loss: 0.11666667812\t Valdacc: 0.998\t\n",
      "Epoch:  234 / 300 Time: 0.738870859146\t Loss: 0.133333345038\t Valdacc: 0.998\t\n",
      "Epoch:  235 / 300 Time: 0.719465970993\t Loss: 0.0333333400902\t Valdacc: 0.998\t\n",
      "Epoch:  236 / 300 Time: 0.718513011932\t Loss: 0.0666666747559\t Valdacc: 0.998\t\n",
      "Epoch:  237 / 300 Time: 0.721102952957\t Loss: 0.06666667524\t Valdacc: 0.998\t\n",
      "Epoch:  238 / 300 Time: 0.721086978912\t Loss: 0.0833333413835\t Valdacc: 0.998\t\n",
      "Epoch:  239 / 300 Time: 0.728716135025\t Loss: 0.050000007228\t Valdacc: 0.998\t\n",
      "Epoch:  240 / 300 Time: 0.720805883408\t Loss: 0.116666677555\t Valdacc: 0.998\t\n",
      "Epoch:  241 / 300 Time: 0.716761112213\t Loss: 0.0666666739964\t Valdacc: 0.998\t\n",
      "Epoch:  242 / 300 Time: 0.720428943634\t Loss: 0.0833333406903\t Valdacc: 0.998\t\n",
      "Epoch:  243 / 300 Time: 0.780378818512\t Loss: 0.0166666707773\t Valdacc: 0.998\t\n",
      "Epoch:  244 / 300 Time: 0.776763916016\t Loss: 0.0833333411875\t Valdacc: 0.998\t\n",
      "Epoch:  245 / 300 Time: 0.725862979889\t Loss: 0.100000008713\t Valdacc: 0.998\t\n",
      "Epoch:  246 / 300 Time: 0.72255897522\t Loss: 0.0333333380412\t Valdacc: 0.998\t\n",
      "Epoch:  247 / 300 Time: 0.719244003296\t Loss: 0.116666675533\t Valdacc: 0.998\t\n",
      "Epoch:  248 / 300 Time: 0.849608182907\t Loss: 0.0833333404686\t Valdacc: 0.998\t\n",
      "Epoch:  249 / 300 Time: 0.763180017471\t Loss: 0.0333333386132\t Valdacc: 0.998\t\n",
      "Epoch:  250 / 300 Time: 0.725517034531\t Loss: 0.0666666729362\t Valdacc: 0.998\t\n",
      "Epoch:  251 / 300 Time: 0.724151134491\t Loss: 0.0666666724643\t Valdacc: 0.998\t\n",
      "Epoch:  252 / 300 Time: 0.721935987473\t Loss: 0.0666666725453\t Valdacc: 0.998\t\n",
      "Epoch:  253 / 300 Time: 0.720905065536\t Loss: 0.0500000054432\t Valdacc: 0.998\t\n",
      "Epoch:  254 / 300 Time: 0.767802000046\t Loss: 0.100000007459\t Valdacc: 0.998\t\n",
      "Epoch:  255 / 300 Time: 0.785547971725\t Loss: 0.0666666728464\t Valdacc: 0.998\t\n",
      "Epoch:  256 / 300 Time: 0.717796087265\t Loss: 0.0666666720493\t Valdacc: 0.998\t\n",
      "Epoch:  257 / 300 Time: 0.716054916382\t Loss: 0.0666666726719\t Valdacc: 0.998\t\n",
      "Epoch:  258 / 300 Time: 0.719556808472\t Loss: 0.0833333395173\t Valdacc: 0.998\t\n",
      "Epoch:  259 / 300 Time: 0.884869098663\t Loss: 0.0833333403605\t Valdacc: 0.998\t\n",
      "Epoch:  260 / 300 Time: 0.720726966858\t Loss: 0.0666666721766\t Valdacc: 0.998\t\n",
      "Epoch:  261 / 300 Time: 0.726945161819\t Loss: 0.0833333400113\t Valdacc: 0.998\t\n",
      "Epoch:  262 / 300 Time: 0.715167999268\t Loss: 0.100000007278\t Valdacc: 0.998\t\n",
      "Epoch:  263 / 300 Time: 0.772051811218\t Loss: 0.100000007491\t Valdacc: 0.998\t\n",
      "Epoch:  264 / 300 Time: 0.738492012024\t Loss: 0.050000004548\t Valdacc: 0.998\t\n",
      "Epoch:  265 / 300 Time: 0.7188808918\t Loss: 0.0500000043962\t Valdacc: 0.998\t\n",
      "Epoch:  266 / 300 Time: 0.717782020569\t Loss: 0.0833333391919\t Valdacc: 0.998\t\n",
      "Epoch:  267 / 300 Time: 0.723372936249\t Loss: 0.0666666718602\t Valdacc: 0.998\t\n",
      "Epoch:  268 / 300 Time: 0.725325107574\t Loss: 0.0333333373989\t Valdacc: 0.998\t\n",
      "Epoch:  269 / 300 Time: 0.922585964203\t Loss: 0.0500000042179\t Valdacc: 0.998\t\n",
      "Epoch:  270 / 300 Time: 0.880469083786\t Loss: 0.100000006814\t Valdacc: 0.998\t\n",
      "Epoch:  271 / 300 Time: 0.783374786377\t Loss: 0.0500000041451\t Valdacc: 0.998\t\n",
      "Epoch:  272 / 300 Time: 0.722369909286\t Loss: 0.0833333393171\t Valdacc: 0.998\t\n",
      "Epoch:  273 / 300 Time: 0.824553012848\t Loss: 0.0333333364638\t Valdacc: 0.998\t\n",
      "Epoch:  274 / 300 Time: 0.737798929214\t Loss: 0.0500000038734\t Valdacc: 0.998\t\n",
      "Epoch:  275 / 300 Time: 0.841727018356\t Loss: 0.066666671543\t Valdacc: 0.998\t\n",
      "Epoch:  276 / 300 Time: 0.808680057526\t Loss: 0.133333341612\t Valdacc: 0.998\t\n",
      "Epoch:  277 / 300 Time: 0.832855939865\t Loss: 0.116666673713\t Valdacc: 0.998\t\n",
      "Epoch:  278 / 300 Time: 0.807028055191\t Loss: 0.0500000038447\t Valdacc: 0.998\t\n",
      "Epoch:  279 / 300 Time: 0.791743993759\t Loss: 0.0500000041893\t Valdacc: 0.998\t\n",
      "Epoch:  280 / 300 Time: 0.833832979202\t Loss: 0.0833333389919\t Valdacc: 0.998\t\n",
      "Epoch:  281 / 300 Time: 0.72953915596\t Loss: 0.0500000039654\t Valdacc: 0.998\t\n",
      "Epoch:  282 / 300 Time: 0.799628973007\t Loss: 0.133333341859\t Valdacc: 0.998\t\n",
      "Epoch:  283 / 300 Time: 0.830518007278\t Loss: 0.116666673785\t Valdacc: 0.998\t\n",
      "Epoch:  284 / 300 Time: 0.93284702301\t Loss: 0.133333341556\t Valdacc: 0.998\t\n",
      "Epoch:  285 / 300 Time: 0.820704936981\t Loss: 0.0833333386496\t Valdacc: 0.998\t\n",
      "Epoch:  286 / 300 Time: 0.736888170242\t Loss: 0.100000006575\t Valdacc: 0.998\t\n",
      "Epoch:  287 / 300 Time: 0.718662023544\t Loss: 0.10000000609\t Valdacc: 0.998\t\n",
      "Epoch:  288 / 300 Time: 0.737938165665\t Loss: 0.0666666711409\t Valdacc: 0.998\t\n",
      "Epoch:  289 / 300 Time: 0.966350078583\t Loss: 0.0666666714039\t Valdacc: 0.998\t\n",
      "Epoch:  290 / 300 Time: 0.971291065216\t Loss: 0.0666666711515\t Valdacc: 0.998\t\n",
      "Epoch:  291 / 300 Time: 0.817620038986\t Loss: 0.0333333362651\t Valdacc: 0.998\t\n",
      "Epoch:  292 / 300 Time: 0.728209972382\t Loss: 0.0333333360459\t Valdacc: 0.998\t\n",
      "Epoch:  293 / 300 Time: 0.725239992142\t Loss: 0.150000008581\t Valdacc: 0.998\t\n",
      "Epoch:  294 / 300 Time: 0.719994068146\t Loss: 0.0833333383884\t Valdacc: 0.998\t\n",
      "Epoch:  295 / 300 Time: 0.71918296814\t Loss: 0.0833333386534\t Valdacc: 0.998\t\n",
      "Epoch:  296 / 300 Time: 0.843810081482\t Loss: 0.100000006205\t Valdacc: 0.998\t\n",
      "Epoch:  297 / 300 Time: 0.780302047729\t Loss: 0.0833333387735\t Valdacc: 0.998\t\n",
      "Epoch:  298 / 300 Time: 0.746392011642\t Loss: 0.150000008586\t Valdacc: 0.998\t\n",
      "Epoch:  299 / 300 Time: 0.735999822617\t Loss: 0.0833333382848\t Valdacc: 0.998\t\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "\n",
    "_dataset = preprocess_data(dataset, vocab, parameter_dict)\n",
    "_dataset['len'] = len(_dataset['train_X'])\n",
    "classifier = IntentClassifier(_parameter_dict=parameter_dict, _word_to_id=_word_to_id, _device=device)\n",
    "\n",
    "optimizer = optim.Adam(list(filter(lambda p: p.requires_grad, classifier.encoder.parameters())) +\n",
    "                           list(filter(lambda p: p.requires_grad, classifier.dense.parameters())))\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "op = training_loop(classifier, optimizer, loss_fn, _dataset, parameter_dict, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4IAAAHVCAYAAACkHUUHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzs3Xl8FdX9//H3SULCFnYUCiqoWMQFUMAF3Esr7nWp28/aRa1LrdaltdVqtdraqq1rbbXVql/3raJ1p2qrYk1AQAERFdmRNSEBAknu5/fHJ8O9WQnI3Azh9Xw87iO5c2fuPXfuLOc958xMMDMBAAAAALYeOS1dAAAAAABAdhEEAQAAAGArQxAEAAAAgK0MQRAAAAAAtjIEQQAAAADYyhAEAQAAAGArQxAEAAAAgK0MQRAAAAAAtjIEQQAAAADYyuS1dAE2lx49eli/fv1auhgAAAAA0CImTJiw1Mx6NmfcVhME+/Xrp+Li4pYuBgAAAAC0iBDC7OaOS9dQAAAAANjKEAQBAAAAYCtDEAQAAACArUyrOUewIZWVlZo3b54qKipauihoRNu2bdW3b1+1adOmpYsCAAAAbDVadRCcN2+eCgsL1a9fP4UQWro4qMPMtGzZMs2bN0/9+/dv6eIAAAAAW41W3TW0oqJC3bt3JwQmVAhB3bt3p8UWAAAAyLJWHQQlEQITjt8HAAAAyL5WHwQBAAAAALURBGNUUlKiP//5z5s07RFHHKGSkpImx7n66qv1+uuvb9L7AwAAANh6EQRj1FQQrKqqanLaF198UV26dGlynOuuu07f+MY3Nrl8AAAAALZOrfqqoZkuvliaNGnzvueQIdKttzb++hVXXKHPPvtMQ4YM0ejRo3XkkUfqV7/6lbp27aqPP/5Yn3zyiY477jjNnTtXFRUVuuiii3TOOedIkvr166fi4mKVl5drzJgxGjVqlN5991316dNHzz33nNq1a6fvfe97Ouqoo3TiiSeqX79+OvPMM/X888+rsrJSTz75pAYOHKglS5botNNO04IFC7Tffvvptdde04QJE9SjR49aZT3vvPNUVFSkNWvW6MQTT9S1114rSSoqKtJFF12kVatWqaCgQOPGjVP79u3185//XC+//LJycnJ09tln68ILL9y8MxcAAABAbGgRjNGNN96onXbaSZMmTdJNN90kSZo4caJuu+02ffLJJ5Kk++67TxMmTFBxcbFuv/12LVu2rN77zJw5UxdccIGmTp2qLl266Omnn27w83r06KGJEyfqvPPO08033yxJuvbaa3XooYdq6tSpOvHEEzVnzpwGp73hhhtUXFysKVOm6K233tKUKVO0bt06nXzyybrttts0efJkvf7662rXrp3uueceffHFF5o0aZKmTJmi008/fXPMLgAAAABZstW0CDbVcpdNI0aMqHXPvNtvv13PPvusJGnu3LmaOXOmunfvXmua/v37a8iQIZKkvffeW1988UWD73388cevH+eZZ56RJL399tvr3//www9X165dG5z2iSee0D333KOqqiotXLhQ06ZNUwhBvXv31vDhwyVJnTp1kiS9/vrrOvfcc5WX54tPt27dNno+AAAAAGg5sbUIhhDuCyEsDiF81MjrIYRwewjh0xDClBDCXhmvnRlCmFnzODOuMraEDh06rP//zTff1Ouvv67x48dr8uTJGjp0aIP31CsoKFj/f25ubqPnF0bjNTVOQ2bNmqWbb75Z48aN05QpU3TkkUdybz8AAACgFYuza+g/JB3exOtjJA2oeZwj6W5JCiF0k3SNpH0kjZB0TQih4WashCssLFRZWVmjr5eWlqpr165q3769Pv74Y7333nubvQwjR47UE088IUl69dVXtWLFinrjrFy5Uh06dFDnzp315Zdf6qWXXpIkff3rX9fChQtVVFQkSSorK1NVVZVGjx6tv/71r+vD5vLlyzd7uQEAAADEJ7YgaGb/kdRUQjhW0oPm3pPUJYTQW9K3JL1mZsvNbIWk19R0oEys7t27a+TIkdp99911+eWX13v98MMPV1VVlXbddVddccUV2nfffTd7Ga655hq9+uqr2n333fXkk0+qV69eKiwsrDXO4MGDNXToUA0cOFCnnXaaRo4cKUnKz8/X448/rgsvvFCDBw/W6NGjVVFRobPOOkvbb7+99txzTw0ePFiPPPLIZi83AAAAgPgEM4vvzUPoJ+kFM9u9gddekHSjmb1d83ycpJ9LOlhSWzO7vmb4ryStMbObG3iPc+Stidp+++33nj17dq3Xp0+frl133XUzfqMtz9q1a5Wbm6u8vDyNHz9e5513niZt7sunfkX8TvGprJRWrfL/27b1B+ozk0Jo6VIkS3W1lJvb0qXYPFIpKYdLo6EVM/PHlrKcJ2mdTFJZgM0hhDDBzIY1Z9wtetE3s3vMbJiZDevZs2dLFyeR5syZo+HDh2vw4MH6yU9+onvvvbeli4QsqayUdt1V6trVHz17SosWtXSpkmX6dOkHP/CAXHNNpa1eKiVdconUvbv06qstXZqv7k9/knbYQSotbemSAPH48ktpxAjpG9+QNuLyAC3m1lt9nWzkIuZZVVUl7befdMEFLV0SoGW0ZBCcL2m7jOd9a4Y1NhybYMCAAfrggw80efJkFRUVrb8CKDZs2jTpW9+SZs36au/z8svSmDEezOoqL5dOPVX6zW++2mc05Omnpc8+80r9Ndf4Zz3/fOPjl5ZKv/+9tNtuUv/+/jj/fD/KnA233y7V3EZzk6RS0gsvSKNHb/gqwdXVvuMfNEh67DGpsFC68caN/0wz6eyzpVtu2bQyN2XNGumEE6Snntq06X/7W+mMM9Itws1RVSX98IcenvLzpaOO8uWoJa1ZI515pvTLX27a9A89JM2bJ9XcwWeTvPOO9O1v+/KZrfXhd7+TfvGL7HyWJF1/fXq9HzhQmjw5/dqqVdJhh0lvv5298qBhZtJZZ0knnSQVFXmYOuAAacoU6Y03/HfMlmifsc8+vo9ZunTD0yxZIl19ta+TZ5zh2+KNMX++dOih0muvbVqZ67r/fun996U//7nhe01//rm0//7Sxx83PP2dd0rHHCM1cOevLcJ//iMNGyZdd53U3Ms9PPigNHKk9H//13C9JokqK32fH23jvv516YMPWrpUCWFmsT0k9ZP0USOvHSnpJUlB0r6S3q8Z3k3SLEldax6zJHXb0GftvffeVte0adPqDUPDUimzefPMSktrD5s/32zlys33OaWl/p6ZNtfv9MgjZjffXHvYuHFmF15otqGPmDjR7OKLzcrK0sO+9S3vbHPKKelhy5aZnXuu2dNPm1VX+7B33jE77zyzhx4yW7eu/nsfcoi/z7//XXv4smVm++wTdegxe/HF5n/X5thvP7Odd/ZyplJm/fqZHXVU+vUFC8xOPdXs2982O/ZYs06dvBwHHWT23e+ajR7tz195ZfOWqzF77umf99prGzfdunVmDz5otttuPn1urlnnzrV/y0wVFWYnnujjXnih2eLFZnfc4c/fe2/jPvv559O/3+uv+7BUyuxPfzL785/9/7oWLzY76yyf79/+tpfhk0/qj/fjH/v79u9vVlW1ceX65BOfD5LZyJFmK1Y0Pu6ECWYnneRlGT7cp7n2WrPly832398sJ8fsuuvMSkqa99lffmn2k5+YzZrV8Osvv2x2/vlmU6b487Vrze6/3+yii8zWrKk9bmmpL4/RPH7qqfRr995rdtNNDc/jyJw5Pl1hoVn79mYLFzbvO0Q++8zsgAP8Pdq397933928aVevNrvkkvTv/IMfmL3/fvOmraw069bNP3Pt2vTwF19Mv99JJ5lNnZp+LZUyu/pqs//9r/nfL/Lhh2Yh+Dbju981a9fO7Ec/Sr/+0EP+3f/f/9v4906aefN8WdvQslBZaXb55WZ/+Uv95bI5Hn3U7K67ml4+N8Urr/hv0aaN/+3Qwbd377zjv11Ojtnbb/v6+9vfmv3852ZffOHTrlzp26af/tRs+vT67z19utkFF6SXsTPP9PdtyF13pfcZu++eXkeuuiq9b2zIxRd7GX/xC5/mt7/duO9/3XXp75+5PairrMzs0kvNJk9ufJxVq8y+9jWzvfc269rVbMyY+uN85zv+eT/8Yf3X3n03vZ3dbbf6dZvIunVmDzzg2/rVq5v+fpvLhx963SX6LR99tP44//qXWdu2Zt27p5elP/yh9jgvvWR2zTW+PpiZffSRTxNtD3fYwfefq1Y1XZ4HHkiX5YwzfNsaWbTIl4u5c7/KN27cmjVmxxzj5T3uOF9PCgp8O9BaSSq25ma15o64sQ9Jj0paKKlS0jxJP5R0rqRza14Pku6S9JmkDyUNy5j2B5I+rXl8vzmfRxD8ahYsMCsqMisu9oBSXW02c6YPa2iHUVd5eeMV78jatWYffODvmVm52Ry/0x/+kK4sPvaYD5s3zytT0fDjjmt4Q/PWW15RlLyCbuahTTIbMMD/TpjgO/QTTki/3y67eCVbMsvL87/bb+8bvMi8eV7BknxDF1m82Hee+fm+gd5jD7Ntt/VKdGMmTPBgm2n2bN9Q11Vc7J95663pYRdd5Bvw8nJ/fumlvhPbYw9/nHKKTxepqPDwuNdeTe/YI6tWeSB7882mKz+VlWbPPFO7UlxW5pUDyXfKdT+vutrn09KltYc//LDP86gy8n//55/fWIW9vDwd8G+5JT185UpfBk4/fcPfM1JV5Z+5005mX/+6VygWL/aKVLSM/OxntefF3LlmAwf6Tiia7wUFvoycdFJ6/v/rXz79Xnv53+eea365zMxOPtl31Hfd5RWmwYMbPhhSVWU2aJBXJPfYw8fLnG/l5WbHH+9l6NTJK29NVYpnz/b1QvIdfl2PPJJeVyQ/2NC3b/r5T3+aHnfJEl8W8vJ8uRo+3Ctrc+eaXXllepqzz248KN95p4/zwgv+Puef37z5Z+YVqd69/TNvv92X0W9+00PStGn+u776qofma681u/56376Z+fJ08MH+u+6+u8/bqNJ82GE+XVPrSLQMS/5/ZPjw9G/Vpk3t7zN9uo9/wAG13+uVV9JlvPPO2tveyDHHePmi9eu003zbGY17+OH+3l27piuEmb74wgNTUwcckuDTT32bVnd7nEr5tmTBgvSwa69N/wbbbuvbi8a2g6+8UjtwRNsTyQ+KNGf72RzV1WZDh/p3WLrUD3wecojZpEn+emmp2Y47mm2zjVnHjukDY7m5fhCwa9f0/ioEX0ejZeO443xY27bpbVOXLullavz4dDnefNPH/cY3fL9k5gclTjnFxz/jjMaXk/x8PyiSSvl2Ki/Pt3fR+lBR4b/FxIn1p0+lfFs7YkT6INXFF3v5b7ghfWAk8yDrsGG117Wnn/Z9fipl9vvf+zhvvZWuQ2Sub9F+tGtXny+Z+5/SUj9I16+f2bPP+vzecUezX/86PU+vvdbsiivMttsuvSxdeGH6PZYt88/NHP/aa81uvNHs888b/v7jxvl+rqntcFmZ1106dfLfMdpHRvu8ykpfX/PyfB+zeLFv7446Kr29NPP1JVqOjjvOv/Oee5r17OnryvPPm40a5a/36OHfpaHt2syZ/ll9+3p5Cgt92/rhh77PiOpZmQefNsWSJf6b1p2fURnvvDM97lFH+e+3uQ/UJEUigmC2HwTBjVNenq48lZf7Bm/mTK9MFBX5UZ+iIt+wFhU13NIVSaV8J/jhh02P8/HH/l5FRbU3qJm/04wZfiSzsfd4802zsWNrP376U1+Sv/Mds3339YrSrFlmhx7qleF33/UjWu3a+RHOTNERsYEDzc45x9a3OowY4RutRYu8QvTNb5r97W/++u9+Z/b4415J3WknD1srV/pGccQI30FGlYKbbkqHlMyNThTColakDz/0QHDkkQ3P67Fj/fW2bdNhNpXynVwI6daVyJln+tG9zFacceO8LM8+6795ly4+z5ry4IO1w/XChfXn/9ixvrHt0SO9s9tnHw/Edce74w6fZ5IHkEhU8T35ZP/7+OPp1yor/Qhe3aBQWuo7l6FDfccVzdtUyndugwbV3sivWOHBPSfH7O9/r/9df/ITr1w31lIQLedRWM+cNxMn+rQ9e/qwSy7xVmLJKz1jx5o9+aQfPS0s9IpHZOFCD1hRUBg92itye+7pO/Q+fbzCFZkxo/4BjU8/TbfARZWXq67y5y+/nN6ZH3ts7bB///0+/MknG/7OkYkT07/NYYc1fNDno4+8wtO5c3rczMrj3Xf7snrggV7W667z73bQQX4w4/zzbX0L9Ny5Zrvu6st7VCn55BNfpqN5/MMfplsVTj654YDzzW96MDXz3yMvz7dDG/L++77e9+7t3yuyYIEv54MG+XKXvkRH+vGtb/l6mZvrFbZIaalvD3r3Tof8p55quCJyySVeYc7NTf+O8+entz9mHtD79k1Pf+ON6TJEgfTjj9MtFtHj8MNrH71/+20ffv316WHRgYjnnvPlPTc33eqTWVE28/kTfafCQrPLLvOK5aZYsqR2K6eZf37mb9Acn31Wf9vz2GNmvXr57zpqlK9v0XL8xBNe/n79fF2KWnpOP93sjTfSvSO++936AeeWW2x9y0hFhQ+Lehgcd5z/PfPM+tMVFW18b5tHH/X3e/DBxscZP95beE4/3bdXc+b4drNnTy/P+PE+T6+6Kh0MJZ8vV11V+2BkebnZbbf5clZQ4Pu45ct9PR8woP52IJUy+81v0tuazNdTKW9RLijwMpn5NnnnndP7jF/9yg+oSb4NrHtgNNq23XOPl+3oo+uvf0cfnT7I+r3v1d6+Zfbg2Hdf3wcecYS/tnq1f8999kn/VqNH+7x85x2f5sYb02XJbH0185b4aD2o+zjwQF+nLrrIn//rX74+R71YGnrk5voBmWee8eX3/vu9zpF5cOK3v2344MsPf+jb2mhdzewFc9pp6YMhBx9cu46wZk066M2b5/OiSxf/XaT0tvf552t/3n//mz7A2tBBy+jAZLRvjbYZ3br5PO/c2cvSvn3j9b/I1KkNb1+ig6wNzcv27b1XQ6Z77vHXMuutH36Y3l7873+1t80lJfVbl8vLfbuyuQ70bE4EwRqtMQhG3fw2VdTds6jIK3dz53qImDzZN35VVV7hisJaebn/v2RJ42VYvjwd8Bo7Mr9wob++eLFXUjKPdkW/07RpHtZOO63h94h2MA09olaB6AhWtMH629/S0//gB15RibpmlJT486FDvVxr13oFLj/fp43Cws03+/P8fA+XTa30y5fX3rkMGeJH8f/61/RGJwphJ59ce9qo8rD99t4CMXu2/z733+87hSFDvAxRF5Wo8pKTU7vL55df+ngXXFD7/det8w3u97/vRwOl9E6sMVVVfgRvp538aF1BQeO/wVFHeaXp7rv9yGhj440Yke5uM2+ef050ZPbLL33nuPPO/v1nz05Xprp18x185LnnfPgbb9Qv9z/+4a9FQfvLL33+tWnTeOiZMcOnueIKn++Zj3/+07vNST4PzjvPK36ZraXRcnL99b5+pFLpoBI9evSoHcQylZT4fOjVy9eDqPJ7ww0+7dSp/p0LCjzQnHmmlyvq8hK1Ku6/v1deMnfwS5Z4t8GuXdPzYM0ar9ANH978bcoDD/iyuO++vp2YO9d3mKec4sthz56+XSkr84rcQQf5e//ud17GI49svGvU6tUe/nr1ajgwm5ndd5+/z6WXpsscHckfM6Z2wCkt9e962WX+fMEC3za0a+fdbhvrurpwoX+P/v1rd1+KRMvd17/u24g1a3wZWL7cv+c22/hvNHZsw+9fUeHbpajl9JJLas//VMrXt8MP9/m8774+PFpno6AULeNRi8z++3uZ2rf3bZ2ZV/46dPDvVF3t3WlD8CD06af++40a5fM86ilg5tuKHj18Pb39dv+c997z7coll6THywzMzzzjXc1zcnz9z5x3q1en16W6rfqRNWt8WyP5gY+nn/b1rKDA3/Mf/6g9j+bPT79n5n7nlVfS3dbqPqJgP368P7/rLv+uAwb4PO/e3edFv37+iNahVCrdJfG449Lb5quusvUhRvKDgtXV6VarVCrdsvjtb/tvn0qlp+vSxf+fOdPfb968+utiWZm/Nnu2l3GPPTa+q3hjUikv74bqFUuW+L4xLy/9t6luztEy07Wrh4iHHvLeBpL3ksi0enXtfcahh/r6ER0YzSzXT3/qy2BmWIjKv2SJt8Z16+bL/Ouvp3s87LKLz8OePb0cd97p63deXro11Sy9fenXL32A+Y9/9NcOOcT3zZWV/h0kP8Dc2PzMfESiZXybbfz7duzovY/qjj9vnm+3ogN40WPAAF+HX3893UpfWOhdmD/+2L9jdIDyF7+oXbaqKq87SL4ve+65husyUdfPqA4VHZT9xz98Pfzxjxv+zSsrfT7vtlvt5TMK71deWXv8zz7z36BnT68TTprk49U9vSfT++/7bxZtwz/6yL9zcbHvMzp18n1GU79BZMGC9P7azOunUe+tzLrKo4969+pOnfz1zP13tG8uKmq8zC2FIFhjSwyCHTp0MDOz+fPn2wknnFDrtepqDxL77HOQFW1gyfvTn/5kqzJqRWPGjLHly1fYnDm+0H72mVcEogCXeWQylUq3SqVSvoLOnJl+Pm2aV5qjDXRmS19DLQWVlekWx1TKP3fSpPT006ZNs4oKr6hLvpLXPVL63nteAY26z2U+pk6tvbOIKkjHH197eNQiFm3Ybr3Vn2eu2DNmeCVi4MD0UcE1a/yoVdQtbUOiI/NRxe3WW9NH82+4ofEQlkp560fU3TTzcfDBXrG96CLfGE+Z4juF3XbzDZnkR+XWrfN5JDXcpffUU33DG7VoNCcAREdR8/M9DL79dv3foG6lurLSN+51x4u61H3wgb9n1I32+OO9kmOWrmhnPm6/PT1fo6OKF17ov1V0FD7TmjVekf3mN70i1r27L1cvv9z0dx0zpuEKZFQ5+NOfvPtwdH5O3fMnG2pNnDEj/f03dLTTzL9P5vssXuyVoqiVafhwb72MKrvdunmFJLNVMbPba6bly9OtotF3rdvdeEOefTZ9sCR6FBZ65Siz3NGBjai70amnNt2zwMyXi/x8/70aC8wLF9Zfbu+5x3fSBxyQrrxHB0r+85/0eDNm+IGQNm18Xv6//1e7Nb262itYbdvWb5nKNGdO4weE1qyp3cWwMVVVvgxLXkGLKk/Tpvmwu+/2SnROjh/1P+IIX0ei7754sb92zTV+oCMErwife66X/8UX/X2uvrr25z7+eHr5jR5//nP98l1wgb/Pnnt65dnMl5moDJ9/7geW+vf3bXrkf/9Lh8M33jD75S99vOizQmi44nTxxf76+ed7GIu2OWed5a3Qkq9/Dz+cDozRo39//w4PP5zuCv3uu/W3P9E58KmUr0cDB/p8lny7M3Wqt0hltvRkigJO5iP67Q491Lc5Tz7pwzNbIG67zdYH3Kjl+/TTfbtXt/K5++4+7aef+vLRrl3t1+u2xmRL5vm6zTmv77330gfxogMn993XcJdRMx8eHRg0S8/rqDtfVZUvF8cd1/TnrlrlPXki//ynrT8IkLleV1bWP6cvlfLlYN99fZrttkt3wXzmGVsfoiQ/SLEpLUFR759u3TZ8znBJSXrZnTSp/gGADz5IH4TLXEaGDWu4h0Qq5fvqDe33o2133R5UixY1PW207N9/f3rYN79Z/8BkpLy8dovmgQf6frahAx1Rd9fttvOW3rrbsB490gfFmmvECH+Y+TamRw9fbouLfXsSHZzIyfGDYt27eyuxmR/Q6tTJD8Qm0cYEwVjvI5hNw4YNs+Li4lrDEnl/uqoqv/zUjjtK7dvXe7ljx44qLy/3ZXvaNKmiYv1rKUky6dAfnaObLr5YwwcNkslHVah9Cdh+Rx+t4oceUo8uXWQ5OVrTZ4DmlXTUypVSr57V6rNymsK6dYp+/aZuoZaqGSm6z5qZtEi9FPr0UefOXswB7eerYnW1wvbba5ttak9fXu5feeedpS5d/Kphc2anNLRgunLWVWj60qXa+ajjVFUl5eX5LGrTRsqruYeZSVq71v8vKGi6rNH40X2BQp3hayukkONXRFy71r9U25uu90tr1lg5+gQV/vfFWveVi+ZBTjPuNWeq9bOpbVsvR8Xa2uM19V1SqfRnSn4/t9DAe+fn+/esqKgpW/Bp8/KkNnn137eqOn2Vr8x5vKHv09D8/Cqi75GbK+W3qSl/zf/R51nGcpeb4/Nj7dp0uSvW+msF+Q1/RmVV+lLqOTk+3YZ+PzOpOlV/eAi1v39UxtwsXXd5XaVfXS+nZtmNloW6v0tzfiuTtG5derzG5l9TUubTR6Lls+7nrF3r8zQ31+d/c5afjVnXMkXLdgi+/KdSPs+i9a/uZ1RX+TSSz4e8PC9rZWXz142vyuTLaFWVL0tt8tPP2xZ4Odet8/JUVnqZ2rRJT792nZc5L89fLyioGZ6xrWns+0e/X91le/04KX9/Kb09ieZxQYH/TaX8/7q/VbSuRnJy0vekrKxMr/eR6pR/z9w624CoXJnLbFTmvFytL3RVVXp7kbmONCVzW1h3vbIm7itXXWfbFJWx7neuO98zPy8vzx/RdJnrUuZ3kXyeRGVp7LfKlk3ZF6RqquobW+7M3zwvz797ZaUvHxtzf9PM99mYfZ6l5PWqkB4Wbc8yf79NsanbuKbeb0Pb442xqfv8zLpafs22rLratx15DdRH6qqu9n1dmzb+ZlXV6W1z9F75+b6trLuvzsnZ+PkZ1RGi7Wvd5SOaD6FmOaiq8mny8314VVXN9m/yJL8MaYJszH0Em/HTtBIXX9zwtYG/iiFDmrxO/RVXXKHttttOF9TcoObXv/61Oubn69xRo3TseedpxapVqqys1PXXX69jjz221rRfTJ2qo449Vh+99prWVFfre5deqg8+nK4B/XfSyrUprWzTTdp2W519+S9UNHmyKirW6jvHHKHfXHapbr/vPi1YulSHXHCBunXppjfuvFMDhwzUAw9P0qBBPfR/9/5O999/v5SXp7NOPVUXn3WWvpg7V2O++12NGj5c706YoD7bbqvn/v53tWvXTpVrpeUrpK6dpSdfeE13/O2PWltVrbadv6Y//vED8o0VAAAgAElEQVRhtWu3rXJWLdRl112vopmfKT8/6JprrtEJJ5ygl19+WT/72S9VUVGtPn166I03xqlTJ6mjypWzdo3UrZuqVq3VH6t+oiFDpG+Olv7+V6lbN+k73/F58dKL0kcf+W0Wttuu7lyuL0hqaFsfJI1/QyoulsYcJv3rX9K5vceq7b33poPgokXqNO5ZvyHT0KHrp92Y+n6Q9Mlk6eVXpP790t9j4rvSf2suv37EGGmPPRp/j5xGPjNImviO9PY7Up+vSaef7hupGZOkV16VgvktLwYPbvh9qyqkO++QCtpK55+nZm0BGpufX0WQ9NpYae5c6bvflf58t3TYQX4Z60Z/P5PuvUPaaWfpwAN8mkMO8vtnNaS6Qpo4QdplF7+HYnPL1ZyNYhzzpClry6Xp06S99pJCXuNlaE65gqS8amnCBN93FXTe+PI0tnzW/ZySRX6Psz33VK0DKxt6702RJ2neF9K4cdLSmku57767dOQRDX9GjqSqNX4J8eJiaU2Fl3nHnaQTjldWattBUhtJk4qkf78h9e/lt8uQ/JYZqpbuvl1q304qrZROPUnafvv09JPel958U+rVXVpVLp13ns/n5x6XZs+WDjvU16m6mvX7mfTgPX6LgPPOltoUSmvKfL3r2l5asUI6+mhpUAPHWnMkrS3x7fagQb49j/zrOWn2HOmC873CumaN9Pe/S+0Kpe+eKSmv/nIcJLVJSUXFUreuflAxc3nKNWnuPGnuHGn4CClkhMxGVUt/u1tavVo6/RSpb9/0ZzX10ze2fuVIeuWf0oxPpJH7S6NG1X49T9LcL6TyMl8uo/LX/S1yTfrsc2nJYr+VT7tOzfguWbIp271NXZ+DpOrVvh+dOdOHFeRLP/6xNqrmGiStLpG+mOVVtuas1w0tA0HS0nnSyrKGl/mNsbmPHzZnfd4Ym7p/C5K+nC099riktR6e9thT+sbo5r1hSEn3/0UqK/fnvXr5LUmqasLlvvtKBx2Y/qyvGmBKlkj33S/lVksdO0lnnV27nPXmQ5X0j3s9CK4okQbtLh1xhPxGzVuy5jYdJv2xwa6hF13k/Ro252MD156dOHGiHXjggeuf77rrrjZn6lSrHD/eSmv6Iy1ZssR22mknS9W0t0ddQ2e9847ttuOOZlVVdsstt9h3vvN9Kyoye++9yZabm2sPPFBkixaZjRu3zKZPN3vvvSrbb7+DbHLN2azbbbeD/ec/S6yoyKxkwkzboXdvW7RosRUXF9vuu+xi5e++a2UrV9qgQYNs4sSJNmvWLMvNzbUPaq4ycNJJJ9lDNX1bqqu9yf2DD8zGjVtua6d9YqkPP7JrrrnXTjvtEps1y+xn3/++XXjqaeu7XSxfvtwWL15sffv2tXff/bzmPMNl6+fFkomzrbqo2CorquzVV6fZwIHp83uuvNKb4hcu9G5MUvqCCV9V1CWxQwfvKlL5x5r+JzNm+Aj33uvPm7rmdDNUVnoXtMwLK0yenO7CsCmXI4+Ulfk5QJlFXLfOu1A1dTntyHXX1T7fpqVEF9+Jzv3MvLBIY77zHb/ASHSRk8zzOwAz3149/7x3u2use2ld5eXeFe3445u+cm+c/v73dBeva69NDz/iCB/WtWv9rrUff5zuGpV5BdHx473rYUPdpjfGo4/W3/ZGF6zY1FtJRF2/o4sAXXaZd49s6CqRcXvoofrnUn0Vn3/uV5ZuqWWotZo61S/mlnnVRyTX5Zf7ucTRRYE2xhNP+LYrqg8sXuxd5L///Ya7u34V0W21Mk9T2ZCo7pGfn74tSxKJcwRdEs4RHDhwoM2fP98mTZpk+++/v9mKFbZu/Hi74LTTbI899rDBgwdb27Zt7bPPFtq6dTVBMJWyWS+9ZLsNGGBmZkcffazdffe49SffDx061B5+uMiKisyuvPJuGzJkqO2yyx7WtWsPe+SRR23NGrPevT0ILl1qllq8xHbo3duWfPGF3fqnP9mvzj57/UldV111ld122202a9Ys23nnndeX+8Ybb7Tf/OY3659H5xOOHTvFRo8aZbvvvLPtvPMuNnLkt2z1arO9Bg60GU8/bcXFqfX95seOHWunnXaaffZZnataplK2bsJkKyn6xGbONHvppWm1KgHROTKXXuon0++zz4bPLWquVMrPj1tf2friC39y000+wlFH+VnHMVxTOJXy80iik8+3dtGs797d+/s3JxxHV/oaNsxPuE/i1bqATfXkk759is7JNvPtRXROWUOii85s6PzXzeWvf/X1r7n3laxr7Vo/P+rUU/2c64ICvwIjAGTb73/v5wc29wJMVVV+Hvnvfx9vub6qjQmCWTrDZet10kkn6amnntLjjz+uk08+Waqu1sMvvaQlS5dqwoQJmjRpkrbZZlt99FGFPvzQj+2uWVEhrVsnCzmaPVtaudLfq0+f9Ptus420YMEsPfLIzfr3v8fpf/+bov33P1LLllVo1iwfZ6edpO7dpdClpu/XypXegTuVkjrX7w9WEJ1gIik3N1dV0QlW8pbvEKQ//OFC/fj739eHjz2me+/5i/LyKtSuwDtqB0k5Vr2+a1OkoiJ97ookac0atUmt0wp1UUmJv3dGL0ztuqs/v+UW74P98MO1z4v5KkKQzjpL6thROuccSTvs4P0ox46VVq2SXn9dOvbY5vdj28jPHjdO+ulPN/tbb5F22EEaMEBatsx/grZtNzzN6NH+t7hYOuywxs/jAbZEJ54oTZ3qXR8jRx7p58icemrD05x6qrTtttLBB2eliDrnHKmoqMFdSLPk53t3+X/+U7r8ct/nXXfd5i0jADTHz37m1b7mnneamyu99JJP11pQjYrZySefrMcee0xPPfWUTjrpJKm6WqXl5dqmSxe1CUFvvPGG5syZLUnq1Ml3iss+L5Ekra3K1dKl0qhRB+p//3tEBQXSRx99pClTpqhLF6lv35UqLOygzp07q6LiS40f/5KWLvU806VLodauLfNCtGnjKaS0VAfsvrv++dZbWp2Xp1WrVunZZ5/VAQccsMHv0bWrV9bLy0vVp08fyUwPPPCAv1hdrdEjRuiuJ59Unqq0erW0YsUK7bvvvvrPf/6jzz+fpYICafny5T5+iX+/laGLCgulwsL6n3fGGf73jjs80G5OF13k56b16lUz4JhjpHfekR591FPrMcds3g9Eo77xDf/b2Hl+dfXrl14eommB1myXXfwCW0ce2fDrv/qVn0NV62Bbwp1+up8b+Nhj0vnn+0EhAED2EQRjtttuu6msrEx9+vRR7969pVRKp48Zo+Lp07XH4MF68MEHtdNOA5WX5xXcnBxp24ISVeW3U5v8oD32kK688jytWVOuXXfdVVdffbX23ntvSdJeew3W0KFDNXDgQJ1++mnaZ5+RMpN69JDOO+8cHX744TrkkEO8IDWXltyrTx9978QTNWK//bTPPvvorLPO0tDM5rhGRFfi+/Wvf62TfvQj7X3GGeoRXQGgqkpX/fCHWlFWphNO2UsHHjhYb7zxhnr27Km7775Hl156vI44YrC3iEoeBDt00E4D29Q76T9ywQXSW2/VXDBhM8vJ8auXrnfssd5KesUVfpj7wAM3/4eiQVGY22efjZ+GIIitRa3tVR25uQ0fTEuy/ff38FdYKP3yly1dGgDYenH7iGybP19auND/79dP1r2HJk/2/NG/v/watpMnS1/7mj82Qirl3ey6dWugmbuiwi/hJvkeuLmXUWxISYn06afeh7NDB6msTJoxQ5I0r93OKsvpomi21711hNatk6ZM8X6uvXtLSsDvZOaXI50/3/tZPfJIy5VlK1NZKf3lL95dt1275k0zZ44fJIhajQFsef77X98tRd29AQCbB7ePSLLoZmBmUkWFVq+WqqpMvSvnSp+uS9/4rKlDwI3IyWki37Vt64+Kik1671qiG8JEZc04l7BdmyotrrkNYgi17/8naX230K9chs0pBO8Oevfd3jqIrGnTRrrwwo2bZvvtCYHAlq4ZZyQAAGJGEMy26O6oNV01V670e+q1Xbm45s6UOX5CXnObRzZGr5qbVH3VK680EQTb5lUplfK82a5d+ubn64Ngaak/ac6VQbLp3HO9qamxE3EAAACAVqTVB0EzU4jhCpCbLGoRrGmdK0tJPfJKpOrgd95t7qWLNkWPHpvnfZoIggW5/n9ZmQfBtWv9KnE5OfLvvnKlX/K05jdJTNfkPfeUXnihpUsBAAAAZEWrvlhM27ZttWzZsuSEDcnDUG6u1LatbO1alZeZOluJnzUfZwjcnKJyZgbBnBypTRvlqVrt2vm5ipIHwfWNf6Wl3me0pluomWnZsmVqm7TWQQAAAKCVa9Utgn379tW8efO0ZMmSli5K2qJF3hpWXi4tW6YyTdSnWuJXeJk+vaVL13zLl3u/z5UrpaVL/f+cHKm0VGX5q1VS4hcCWbRIat/e86+WLvWuqXPnrm8RbNu2rfr27duy3wUAAADYyrTqINimTRv179+/pYtR28kn+30ifvYzacwYLQ8jtau9I82bV/uO8Ul3/PHSHntITzzh59UtWuStmmaa89Bb2m8/6eKLpVtv9RvDX/KTKmnUKOmoo6To/oMAAAAAWkSr7hqaSGVlHpi+/nVJ0kh7Rxo2bMsKgZLUvXu6/+eyZf68Ztj220sHHeS3BZCkAQPkN2xfvpybtQMAAAAJQBDMtigIduumFXk1F2/ZEsNRjx7e1VPyvz161Bp2+unpK4buvLOk557zq8Z861stU14AAAAA6xEEsy0KgpJm5gz0YVviveu6d284CC5bJpnpxBM994Ug7djfPAgedpjUsWPLlhsAAAAAQTCr1q3zR2GhUinprcr9tXCbPf1cuy1NFPoqK/1qoFHX0KoqaeVKde3q+XbgQKngs2nS559vmYEXAAAAaIUIgtlUVuZ/Cwu1ZIn0c/ud/nll8foraG5RevTwe0PMmZN+Ht2nsKal8O9/l/79b3lroCQdfXT2ywkAAACgHoJgNmUEwQULJFOOem3XpmXLtKm6d/e/M2b43waCYGGh1KuXPAgOHy597WvZLycAAACAegiC2bRypf/t1Enz5/u/W2w2ikJfFASjrqFS+mqikrRwofT++3QLBQAAABKkVd9HMHEyWwS/8H+3+CD48cfp5zUXwVl/ERlJev55/7slXhkVAAAAaKUIgtlUp2uoVNN1ckvUUNfQhoLg2LFS//7S7rtnt3wAAAAAGkXX0GyqEwS32UZqs4WeIthg19BOnaS8vHTX0PJy6fXXvVvolnhBHAAAAKCVokUwm+oEwS22W6gkdeki5eRIixZJ7dtL7dr58Mz7C776ql9ZlG6hAAAAQKLQIphNrSkI5uZKXbv6/1E30ej/KAiOHevjjBqV/fIBAAAAaBRBMJtaUxCU0t1Do7/R/8uW+Y3lX3hBOuKILbj/KwAAANA6EQSzqaxMKihQpdpo8WKpT5+WLtBX1FgQXLpUGj/eAyG3jQAAAAAShyCYTWVlUmGhvvxSMmsFLYJRl9C6XUOXLfObyOfnS4cf3jJlAwAAANAogmA21QTB6NYRW3wQbKpF8LnnpEMOSd9SAgAAAEBiEASzaeVKqVOn1h8Eq6qkTz+lWygAAACQUATBbGptLYKNdQ2NHH10dssDAAAAoFkIgtmUEQRzc6WePVu6QF9RYy2CkrT33lLfvtkvEwAAAIANIghmU0YQ7NXLw+AWLQp9ma2A0TC6hQIAAACJRRDMpowguMV3C5Wkgw+WLrpIGjkyPWzIEOknP5HOPrvFigUAAACgaXktXYCtSkYQ3HHHli7MZtCpk3TrrbWHFRRIt93WMuUBAAAA0Cy0CGaLmVRe3rpaBAEAAABskQiC2bJqlWSmqnaFWraMIAgAAACg5RAEs6WsTJL030l+g3WCIAAAAICWQhDMktee8SD4tycKtdde3GIPAAAAQMshCGbJ2Ic9CF5+baGKi1vBPQQBAAAAbLG4amiWtKtcKUkacmAnKbRwYQAAAABs1WgRzJK2ld4iqMLCli0IAAAAgK0eQTBLCIIAAAAAkoIgmCUEQQAAAABJQRDMknZVBEEAAAAAyUAQzJJ2VWVKKUgdOrR0UQAAAABs5QiCWdKuqkxrcjtKgUuGAgAAAGhZBMEsaVddpjV5dAsFAAAA0PIIglnSvoogCAAAACAZCIJZQosgAAAAgKQgCGZJx6oSrWrTpaWLAQAAAAAEwWwprC7RaoIgAAAAgAQgCGZJx6pSrconCAIAAABoeQTBLPEWwc4tXQwAAAAAIAhmxbp1amertZoWQQAAAAAJQBDMhtJSSdIagiAAAACABCAIZsP6IEjXUAAAAAAtjyCYDSUlkqQ1BbQIAgAAAGh5BMFsIAgCAAAASBCCYDYQBAEAAAAkCEEwG2rOEawo4BxBAAAAAC2PIJgNtAgCAAAASBCCYDaUlKhaOaos6NjSJQEAAAAAgmBWlJaqLHRSyGV2AwAAAGh5JJNsKCnRypwuymFuAwAAAEgAokk2lJSoNBAEAQAAACQD0SQbCIIAAAAAEoRokg2lpVqpzgRBAAAAAIlANMmGmhbBEFq6IAAAAABAEMyOkhKViK6hAAAAAJKBaBK36mpp5UqV0jUUAAAAQELEGk1CCIeHEGaEED4NIVzRwOs7hBDGhRCmhBDeDCH0zXjtDyGEqSGE6SGE20PYQjtWlpVJEi2CAAAAABIjtmgSQsiVdJekMZIGSTo1hDCozmg3S3rQzPaUdJ2k39VMu7+kkZL2lLS7pOGSDoqrrLEqKfE/BEEAAAAACRFnNBkh6VMz+9zM1kl6TNKxdcYZJOnfNf+/kfG6SWorKV9SgaQ2kr6MsazxqQmCK4wgCAAAACAZ4owmfSTNzXg+r2ZYpsmSjq/5/9uSCkMI3c1svDwYLqx5vGJm0+t+QAjhnBBCcQiheMmSJZv9C2wWUYugcY4gAAAAgGRo6WhymaSDQggfyLt+zpdUHULYWdKukvrKw+OhIYQD6k5sZveY2TAzG9azZ89slrv5SksleYvgFnqWIwAAAIBWJi/G954vabuM531rhq1nZgtU0yIYQugo6QQzKwkhnC3pPTMrr3ntJUn7SfpvjOWNB11DAQAAACRMnNGkSNKAEEL/EEK+pFMkjc0cIYTQI4QQleEXku6r+X+OvKUwL4TQRt5aWK9r6BYhCoIpuoYCAAAASIbYoomZVUn6saRX5CHuCTObGkK4LoRwTM1oB0uaEUL4RNK2km6oGf6UpM8kfSg/j3CymT0fV1ljtf6qoQRBAAAAAMkQZ9dQmdmLkl6sM+zqjP+fkoe+utNVS/pRnGXLmtJSWceOqi7PIwgCAAAASASiSdxKSqQuXSSJIAgAAAAgEYgmcSspkXXqLIkgCAAAACAZiCZxKymROtMiCAAAACA5iCZxKy2V1QRB7iMIAAAAIAkIgnGjaygAAACAhCGaxK2kRKlOdA0FAAAAkBxEkziZSaWlSnGOIAAAAIAEIZrEadUqqbpaqUKCIAAAAIDkIJrEqaREkpQq5BxBAAAAAMlBNIlTFAQ5RxAAAABAghBN4lRaKkmqLuT2EQAAAACSgyAYpw4dpCOPVFWvvpJoEQQAAACQDHktXYBWbcgQ6YUXVLnAnxIEAQAAACQB0SQLUin/SxAEAAAAkAREkywgCAIAAABIEqJJFhAEAQAAACQJ0SQLCIIAAAAAkoRokgUEQQAAAABJQjTJAjP/y30EAQAAACQBQTALaBEEAAAAkCREkywgCAIAAABIEqJJFhAEAQAAACQJ0SQLCIIAAAAAkoRokgUEQQAAAABJQjTJAoIgAAAAgCQhmmRBFAS5fQQAAACAJCAIZkF0H0FaBAEAAAAkAdEkC+gaCgAAACBJiCZZQBAEAAAAkCREkywgCAIAAABIEqJJFhAEAQAAACQJ0SQLCIIAAAAAkoRokgUEQQAAAABJQjTJguj2EdxHEAAAAEASEASzgBZBAAAAAElCNMkCgiAAAACAJCGaZAFBEAAAAECSEE2ygCAIAAAAIEmIJllAEAQAAACQJESTLCAIAgAAAEgSokkWREGQ20cAAAAASAKCYBZE9xGkRRAAAABAEhBNsoCuoQAAAACShGiSBQRBAAAAAElCNMkCgiAAAACAJCGaZAFBEAAAAECSEE2ygCAIAAAAIEmIJllAEAQAAACQJESTLIhuH8F9BAEAAAAkAUEwC2gRBAAAAJAkRJMsIAgCAAAASBKiSRYQBAEAAAAkCdEkCwiCAAAAAJKEaJIFBEEAAAAASUI0yQKCIAAAAIAkIZpkQRQEuX0EAAAAgCQgCGZBdB9BWgQBAAAAJAHRJAvoGgoAAAAgSYgmWUAQBAAAAJAkRJMsIAgCAAAASBKiSRYQBAEAAAAkCdEkCwiCAAAAAJKEaJIFBEEAAAAASUI0yYLo9hHcRxAAAABAEhAEs4AWQQAAAABJQjTJAoIgAAAAgCQhmmQBQRAAAABAkhBNsiAKgpwjCAAAACAJCIJZkErRGggAAAAgOYgnWUAQBAAAAJAkxJMsSKXoFgoAAAAgOQiCWWBGiyAAAACA5CCeZAFdQwEAAAAkCfEkCwiCAAAAAJIk1ngSQjg8hDAjhPBpCOGKBl7fIYQwLoQwJYTwZgihb8Zr24cQXg0hTA8hTAsh9IuzrHEiCAIAAABIktjiSQghV9JdksZIGiTp1BDCoDqj3SzpQTPbU9J1kn6X8dqDkm4ys10ljZC0OK6yxo0gCAAAACBJ4ownIyR9amafm9k6SY9JOrbOOIMk/bvm/zei12sCY56ZvSZJZlZuZqtjLGusCIIAAAAAkiTOeNJH0tyM5/NqhmWaLOn4mv+/LakwhNBd0i6SSkIIz4QQPggh3FTTwlhLCOGcEEJxCKF4yZIlMXyFzYMgCAAAACBJWjqeXCbpoBDCB5IOkjRfUrWkPEkH1Lw+XNKOkr5Xd2Izu8fMhpnZsJ49e2at0BuL+wgCAAAASJI4g+B8SdtlPO9bM2w9M1tgZseb2VBJV9YMK5G3Hk6q6VZaJemfkvaKsayx4j6CAAAAAJIkznhSJGlACKF/CCFf0imSxmaOEELoEUKIyvALSfdlTNslhBA18x0qaVqMZY0VXUMBAAAAJEls8aSmJe/Hkl6RNF3SE2Y2NYRwXQjhmJrRDpY0I4TwiaRtJd1QM221vFvouBDCh5KCpHvjKmvcCIIAAAAAkiQvzjc3sxclvVhn2NUZ/z8l6alGpn1N0p5xli9bCIIAAAAAkoR4kgUEQQAAAABJQjzJAoIgAAAAgCQhnmQBQRAAAABAkhBPssCM+wgCAAAASA6CYBbQIggAAAAgSYgnWUAQBAAAAJAkxJMsIAgCAAAASBLiSRYQBAEAAAAkCfEkCwiCAAAAAJKEeJIFBEEAAAAASUI8yYJUittHAAAAAEgOgmAWmNEiCAAAACA5iCdZQNdQAAAAAElCPMkCgiAAAACAJCGeZAFBEAAAAECSNCuehBBGNmcYGkYQBAAAAJAkzY0ndzRzGBpAEAQAAACQJHlNvRhC2E/S/pJ6hhAuyXipk6TcOAvWmhAEAQAAACRJk0FQUr6kjjXjFWYMXynpxLgK1dqYcR9BAAAAAMnRZBA0s7ckvRVC+IeZzc5SmVqdVErK21DkBgAAAIAsaW48KQgh3COpX+Y0ZnZoHIVqbegaCgAAACBJmhsEn5T0F0l/k1QdX3FaJ4IgAAAAgCRpbhCsMrO7Yy1JK0YQBAAAAJAkzY0nz4cQzg8h9A4hdIsesZasFSEIAgAAAEiS5rYInlnz9/KMYSZpx81bnNaJIAgAAAAgSZoVBM2sf9wFac1SKW4fAQAAACA5mtVOFUJoH0K4qubKoQohDAghHBVv0VoPM1oEAQAAACRHc+PJ/ZLWSdq/5vl8SdfHUqJWiK6hAAAAAJKkufFkJzP7g6RKSTKz1ZLo7NhMBEEAAAAASdLceLIuhNBOfoEYhRB2krQ2tlK1MgRBAAAAAEnS3KuG/lrSy5K2CyE8LGmkpO/HVajWhiAIAAAAIEmae9XQV0MIEyTtK+8SepGZLY21ZK0IQRAAAABAkjT3qqHjzGyZmf3LzF4ws6UhhHFxF661IAgCAAAASJImWwRDCG0ltZfUI4TQVekLxHSS1CfmsrUaZtxHEAAAAEBybKhr6I8kXSzpa5ImKB0EV0q6M8ZytSq0CAIAAABIkiaDoJndJum2EMKFZnZHlsrU6hAEAQAAACRJcy8Wc0cIYX9J/TKnMbMHYypXq0IQBAAAAJAkzQqCIYSHJO0kaZKk6prBJokg2AwEQQAAAABJ0tz7CA6TNMjMLM7CtFYEQQAAAABJ0tx48pGkXnEWpDUjCAIAAABIkua2CPaQNC2E8L6ktdFAMzsmllK1MqkUt48AAAAAkBzNDYK/jrMQrZ0ZLYIAAAAAkqO5Vw19K+6CtGZ0DQUAAACQJE0GwRDC22Y2KoRQJr9K6PqXJJmZdYq1dK0EQRAAAABAkmzohvKjav4WZqc4rRNBEAAAAECSEE+ygCAIAAAAIEmIJ1lAEAQAAACQJMSTLCAIAgAAAEgS4kkWmHEfQQAAAADJQRDMAloEAQAAACQJ8SQLCIIAAAAAkoR4kgUEQQAAAABJQjyJmZk/CIIAAAAAkoJ4EjMz/0sQBAAAAJAUxJOYpVL+lyAIAAAAICmIJzGLgiC3jwAAAACQFATBmNE1FAAAAEDSEE9iRtdQAAAAAElDPIkZQRAAAABA0hBPYkYQBAAAAJA0xJOYEQQBAAAAJA3xJGYEQQAAAABJQzyJGUEQAAAAQNIQT2LGfQQBAAAAJA1BMGbcRxAAAABA0hBPYkbXUAAAAABJQzyJGUEQAAAAQNIQT2JGEAQAAACQNMSTmBEEAQAAACQN8SRmBEEAAAAASUM8iRm3jwAAAACQNATBmHH7CAAAAABJQzyJGV1DAQAAACQN8SRmBEEAAAAASRNrPAkhHB5CmBFC+DSEcEoANg4AABCHSURBVEUDr+8QQhgXQpgSQngzhNC3zuudQgjzQgh3xlnOOBEEAQAAACRNbPEkhJAr6S5JYyQNknRqCGFQndFulvSgme0p6TpJv6vz+m8k/SeuMmYDQRAAAABA0sQZT0ZI+tTMPjezdZIek3RsnXEGSfp3zf9vZL4eQthb0raSXo2xjLEjCAIAAABImjjjSR9JczOez6sZlmmypONr/v+2pMIQQvcQQo6kWyRd1tQHhBDOCSEUhxCKlyxZspmKvXkRBAEAAAAkTUvHk8skHRRC+EDSQZLmS6qWdL6kF81sXlMTm9k9ZjbMzIb17Nkz/tJuAu4jCAAAACBp8mJ87/mStst43rdm2HpmtkA1LYIhhI6STjCzkhDCfpIOCCGcL6mjpPwQQrmZ1bvgTNJxH0EAAAAASRNnECySNCCE0F8eAE+RdFrmCCGEHpKWm1lK0i8k3SdJZnZ6xjjfkzRsSwyBEl1DAQAAACRPbPHEzKok/VjSK5KmS3rCzKaGEK4LIRxTM9rBkmaEED6RXxjmhrjK01IIggAAAACSJs4WQZnZi5JerDPs6oz/n5L01Abe4x+S/hFD8bKCIAgAAAAgaYgnMSMIAgAAAEga4knMCIIAAAAAkoZ4EjNuHwEAAAAgaQiCMeP2EQAAAACShngSM7qGAgAAAEga4knMCIIAAAAAkoZ4EjOCIAAAAICkIZ7EjCAIAAAAIGmIJzEjCAIAAABIGuJJzAiCAAAAAJKGeBIz7iMIAAAAIGkIgjHjPoIAAAAAkoZ4EjO6hgIAAABIGuJJzAiCAAAAAJKGeBIzgiAAAACApCGexIwgCAAAACBpiCcxIwgCAAAASBriScy4fQQAAACApCEIxozbRwAAAABIGuJJzOgaCgAAACBpiCcxIwgCAAAASBriScwIggAAAACShngSM4IgAAAAgKQhnsSMIAgAAAAgaYgnMSMIAgAAAEga4knMuI8gAAAAgKQhCMaM+wgCAAAASBriSczoGgoAAAAgaYgnMSMIAgAAAEga4knMCIIAAAAAkoZ4EjOCIAAAAICkIZ7EjCAIAAAAIGmIJzEjCAIAAABIGuJJzKLbR3AfQQAAAABJQRCMGS2CAAAAAJKGeBIzgiAAAACApCGexIwgCAAAACBpiCcxIwgCAAAASBriScyiIMjFYgAAAAAkBUEwZqkUrYEAAAAAkoWIErNUitZAAAAAAMlCEIyZGS2CAAAAAJKFiBIzuoYCAAAASBoiSswIggAAAACShogSM4IgAAAAgKQhosSMIAgAAAAgaYgoMSMIAgAAAEgaIkrMCIIAAAAAkoaIEjMz7iMIAAAAIFkIgjGjRRAAAABA0hBRYkYQBAAAAJA0RJSYEQQBAAAAJA0RJWYEQQAAAABJQ0SJGUEQAAAAQNIQUWJGEAQAAACQNESUmKVS3D4CAAAAQLIQBGNmRosgAAAAgGQhosSMrqEAAAAAkoaIEjOCIAAAAICkIaLEjCAIAAAAIGmIKDEjCAIAAABIGiJKzAiCAAAAAJKGiBIzgiAAAACApCGixIz7CAIAAABIGoJgzLiPIAAAAICkIaLEjK6hAAAAwP9v7/5jdq3rOoC/P3GkWmIqnDEHzFixteNmJzoDWk1IVx78QwKdQVuSY6M/ZKst/8DatChnbZatzdxsMX9VxCgWK5Yxfqx/QgHFHwcGHk0nRxMMsZqbDvj0x30dvHvkIarn+9wXz/V6bc/u69f9nM/22fc+532u7319mRsRZTBBEAAAmBsRZTBBEAAAmBsRZTBBEAAAmBsRZTBBEAAAmBsRZTDLRwAAAHMjCA5m+QgAAGBuRJTBTA0FAADmRkQZTBAEAADmZmhEqarDVfVAVR2tqquf5vxLq+rWqvpUVd1RVadPxw9W1T9X1ZHp3C+MrHMkQRAAAJibYRGlqk5I8p4kFyY5kOSyqjqw5bJ3Jflgd788yTVJ3jkd/2aSN3b3y5IcTvJHVfXCUbWOJAgCAABzMzKinJPkaHd/vru/neS6JBdtueZAktum7duPn+/uB7v7s9P2l5M8nGT/wFqHEQQBAIC5GRlRTkvypbX9h6Zj6z6Z5JJp++IkJ1XVyesXVNU5SU5M8rmtf0BVXVlVd1fV3Y888siOFb6TBEEAAGBuNh1R3pLk/Kr6RJLzkxxL8sTxk1X1kiQfSvKm7n5y65u7+33dfai7D+3fP88bhtYRBAAA5mbfwN99LMkZa/unT8eeMk37vCRJqur5SV7X3Y9N+y9I8vdJfrO77xxY51DWEQQAAOZmZES5K8lZVXVmVZ2Y5NIkN61fUFWnVNXxGt6a5Nrp+IlJbszqQTI3DKxxOFNDAQCAuRkWUbr78SRXJflIkvuTXN/dR6rqmqp67XTZBUkeqKoHk5ya5B3T8TckeUWSX66qe6efg6NqHUkQBAAA5mbk1NB0981Jbt5y7G1r2zck+a47ft394SQfHlnbbhEEAQCAuRFRBhMEAQCAuRFRBhMEAQCAuRFRBrN8BAAAMDeC4GCWjwAAAOZGRBnM1FAAAGBuRJTBBEEAAGBuRJTBBEEAAGBuRJTBBEEAAGBuRJTBBEEAAGBuRJTBBEEAAGBuRJTBrCMIAADMjSA4mHUEAQCAuRFRBjM1FAAAmBsRZTBBEAAAmBsRZTBBEAAAmBsRZTBBEAAAmBsRZTBBEAAAmBsRZTDLRwAAAHMjCA5m+QgAAGBuRJTBTA0FAADmRkQZTBAEAADmRkQZTBAEAADmRkQZTBAEAADmRkQZqNvDYgAAgPkRUQbqXr0KggAAwJyIKAM9+eTq1TqCAADAnAiCA7kjCAAAzJGIMtDxO4KCIAAAMCciykCCIAAAMEciykCCIAAAMEciykCCIAAAMEciykCCIAAAMEciykCWjwAAAOZIEBzI8hEAAMAciSgDmRoKAADMkYgykCAIAADMkYgykCAIAADMkYgykCAIAADMkYgykCAIAADMkYgykCAIAADMkYgykHUEAQCAORIEB7KOIAAAMEciykCmhgIAAHMkogwkCAIAAHMkogwkCAIAAHMkogwkCAIAAHMkogwkCAIAAHMkogxk+QgAAGCOBMGB3BEEAADmSEQZyDqCAADAHIkoA7kjCAAAzJGIMpAgCAAAzJGIMpAgCAAAzJGIMpAgCAAAzJGIMpAgCAAAzJGIMpB1BAEAgDkSBAeyfAQAADBHIspApoYCAABzJKIMJAgCAABzJKIMdNJJybnnJi94waYrAQAA+I59my5gLzt0KLnzzk1XAQAA8N+5IwgAALAwgiAAAMDCCIIAAAALIwgCAAAsjCAIAACwMIIgAADAwgiCAAAACyMIAgAALIwgCAAAsDCCIAAAwMIIggAAAAsjCAIAACyMIAgAALAwQ4NgVR2uqgeq6mhVXf00519aVbdW1aeq6o6qOn3t3OVV9dnp5/KRdQIAACzJsCBYVSckeU+SC5McSHJZVR3Yctm7knywu1+e5Jok75ze++Ikb09ybpJzkry9ql40qlYAAIAlGXlH8JwkR7v789397STXJbloyzUHktw2bd++dv7VSW7p7ke7++tJbklyeGCtAAAAizEyCJ6W5Etr+w9Nx9Z9Mskl0/bFSU6qqpOf5XtTVVdW1d1VdfcjjzyyY4UDAADsZZt+WMxbkpxfVZ9Icn6SY0meeLZv7u73dfeh7j60f//+UTUCAADsKfsG/u5jSc5Y2z99OvaU7v5ypjuCVfX8JK/r7seq6liSC7a8945n+sPuueeer1XVF///Ze+4U5J8bdNFsDH6v1x6v1x6v1x6v2z6v1xz6v1Ln+2F1d1DKqiqfUkeTPKqrALgXUl+sbuPrF1zSpJHu/vJqnpHkie6+23Tw2LuSXL2dOnHk/xEdz86pNiBquru7j606TrYDP1fLr1fLr1fLr1fNv1frudq74dNDe3ux5NcleQjSe5Pcn13H6mqa6rqtdNlFyR5oKoeTHJqkndM7300ye9kFR7vSnLNczEEAgAAzNHIqaHp7puT3Lzl2NvWtm9IcsM27702ybUj6wMAAFiiTT8sZgnet+kC2Cj9Xy69Xy69Xy69Xzb9X67nZO+HfUcQAACAeXJHEAAAYGEEQQAAgIURBAeqqsNV9UBVHa2qqzddD2NV1Req6tNVdW9V3T0de3FV3VJVn51eX7TpOtkZVXVtVT1cVZ9ZO/a0/a6VP54+Cz5VVWdv/5uZu216/1tVdWwa//dW1WvWzr116v0DVfXqzVTNTqiqM6rq9qq6r6qOVNWvTseN/T3uGXpv7O9xVfV9VfWxqvrk1Pvfno6fWVUfnXr8V1V14nT8e6f9o9P5H9pk/c9EEBykqk5I8p4kFyY5kOSyqjqw2arYBT/T3QfX1pK5Osmt3X1WklunffaG9yc5vOXYdv2+MMlZ08+VSd67SzUyxvvz3b1PkndP4//g9NTsTJ/7lyZ52fSeP5n+fuC56fEkv97dB5Kcl+TNU4+N/b1vu94nxv5e960kr+zuH0tyMMnhqjovye9n1fsfSfL1JFdM11+R5OvT8XdP182SIDjOOUmOdvfnu/vbSa5LctGGa2L3XZTkA9P2B5L8/AZrYQd19z8l2bq+6Xb9vijJB3vlziQvrKqX7E6l7LRter+di5Jc193f6u5/SXI0q78feA7q7q9098en7f/Iap3k02Ls73nP0PvtGPt7xDR+/3Pafd7000leme8sg7d13B//PLghyauqqnap3P8VQXCc05J8aW3/oTzzBwbPfZ3kH6vqnqq6cjp2and/Zdr+1ySnbqY0dsl2/fZ5sAxXTdP/rl2bBq73e9Q03evHk3w0xv6ibOl9YuzveVV1QlXdm+ThJLck+VySx7r78emS9f4+1fvp/DeSnLy7FT87giDsnJ/u7rOzmgr05qp6xfrJXq3VYr2WhdDvxXlvkh/OatrQV5L8wWbLYaSqen6Sv07ya9397+vnjP297Wl6b+wvQHc/0d0Hk5ye1Z3dH91wSTtCEBznWJIz1vZPn46xR3X3sen14SQ3ZvVB8dXj04Cm14c3VyG7YLt++zzY47r7q9M/FJ5M8qf5zhQwvd9jqup5WQWBP+/uv5kOG/sL8HS9N/aXpbsfS3J7kp/Maqr3vunUen+f6v10/geT/Nsul/qsCILj3JXkrOmJQidm9YXhmzZcE4NU1Q9U1UnHt5P8XJLPZNXzy6fLLk/yt5upkF2yXb9vSvLG6QmC5yX5xto0MvaALd/7ujir8Z+sen/p9BS5M7N6aMjHdrs+dsb0PZ8/S3J/d//h2iljf4/brvfG/t5XVfur6oXT9vcn+dmsviN6e5LXT5dtHffHPw9en+S2aabA7Oz7ny/h/6K7H6+qq5J8JMkJSa7t7iMbLotxTk1y4/Rd4H1J/qK7/6Gq7kpyfVVdkeSLSd6wwRrZQVX1l0kuSHJKVT2U5O1Jfi9P3++bk7wmq4cFfDPJm3a9YHbMNr2/oKoOZjUl8AtJfiVJuvtIVV2f5L6snjr45u5+YhN1syN+KskvJfn09H2hJPmNGPtLsF3vLzP297yXJPnA9NTX70lyfXf/XVXdl+S6qvrdJJ/I6j8KMr1+qKqOZvVgsUs3UfSzUTMNqAAAAAxiaigAAMDCCIIAAAALIwgCAAAsjCAIAACwMIIgAADAwgiCAAAACyMIAgAALMx/AUQuAkBurpm9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_loss(op[2], op[3], _label=\"training acc\", _label2=\"validation acc\", _name=\"intent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.994"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get test accuracy\n",
    "assert('test_X' in _dataset.keys())\n",
    "\n",
    "y_pred, _ = classifier.predict({'ques_batch': _dataset['test_X'], 'y_label':_dataset['test_Y']}, device=device, loss_fn=loss_fn)\n",
    "classifier.eval(y_true=_dataset['test_Y'], y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
